[[{"l":"Home","p":["Last update: July 28, 2025"]},{"i":"section-2","l":"‎"},{"l":"Introduction","p":["This site is a documentation of General AI tools, mostly RVC-related apps. Made by staffers/members of AI HUB.","See simple & convenient guides regarding RVC model training, RVC inference, realtime voice changing, audio isolation, datasets, TensorBoard, & more. Verified by the experts & for all devices."]},{"i":"section-3","l":"‎"},{"l":"Contributions","p":["We'll appreciate any feedback, big or small. You can directly contact us in AI Hub: @eddycrack864","Leave suggestions in the #suggestions channel. To report issues, use #ai-help-forum.","You can also send an issue/pull request to our GitHub Docs Repository."]},{"i":"section-4","l":"‎"},{"l":"Disclaimer","p":["The tools and guides provided on this site are for informational and educational purposes only. Good and ethical fun only. The creators and contributors of this documentation do not condone and are not responsible for any misuse of the information or tools provided.","Illegal Activities: We strictly prohibit the use of any information or tools from this site for illegal activities, including but not limited to scamming, catfishing, and any form of fraud.","Misuse of AI: The use of AI should be done responsibly. We do not support the use of AI for creating harmful, deceptive, or malicious content.","Copyright: Users are responsible for ensuring they have the necessary rights and permissions for any content they use with these tools. We do not condone copyright infringement.","Responsibility: By using the information and tools provided, you agree that the creators and contributors of this documentation are not liable for any damages or issues that may arise from your actions."]},{"i":"section-5","l":"‎"},{"l":"Credits","p":["Lead by: Nick088, Eddy, Julia (Used to)","General Help: Poopmaster Raid, Light, Faze Masta, Alexolotl, Delik, Razer, Nick088","Reviewing: Faze Masta, Alexolotl, SimplCup, Delik, Litsa, Lyery, Razer","OG Guides: Litsa, Faze Masta, MrM0dz, FDG, Eddy, Julia, Nick088","Backend: Eddy & Yui","Branding: Grvyscale & Cthulhu"]},{"i":"section-6","l":"‎"},{"l":"To-Do","p":["Nothing \uD83D\uDE04"]}],[{"l":"How to Make AI Cover","p":["Last update: July 17, 2025"]},{"l":"- Simple AI cover tutorial, using RVC -"},{"i":"section","l":"‎"},{"l":"1. Extract vocals","p":["Have the audio file of your song ready, & let's extract the vocals from it with an audio isolation software.","RVC is designed to work with only voices, so to get the best results the sample must be clean, without undesired noises."]},{"i":"section-1","l":"‎"},{"l":"2. Get voice model","p":["Learn about them & how to search one here. Be sure to leave credits to the model maker.","In case the model doesn't exist, click here."]},{"i":"section-2","l":"‎"},{"l":"3. Convert the vocals","p":["After obtaining the vocals & model, it's time to set up RVC & do inference.","There are plenty of versions of RVC, but these are the best ones for beginners. Pick according to your needs:"]},{"i":"section-3","l":"‎","p":["For local users, first ensure you meet the minimum requirements."]},{"i":"section-4","l":"‎"},{"l":"Tips","p":["Congratulations, you've made it to the final part. Now it's to mix the song.‎","You're free to use any DAW, but we recommend FL Studio or BandLab, as they are beginner-friendly. You can start by searching some of their mixing tutorials on YouTube.‎","Recommendations for the mix:","Match the volume of the vocals to the same level as the original ones.","Add reverb to the vocals (not to the instrumental), to the same level as the original one.","Add delay if the original vocals had it.","Remove the very low frequencies, ranging from 20 to 100.","For presence and clarity, increase the high range a bit.","Normalize the audio.","Use compressor on vocals.‎","Regarding what to do with the backing vocals, you have 4 options:","Simply leave the original ones in.","Convert them using Mangio-Crepe with a higher hop length.","Record yourself singing them & convert the audio with RVC.","Make vocals from scratch using a voice synthesizer (like SynthV) & convert them with RVC."]},{"i":"section-5","l":"‎"}],[{"l":"How to Make Voice Models","p":["Last update: July 17, 2025"]},{"l":"- Simple model training tutorial, using RVC -"},{"i":"section","l":"‎"},{"l":"1. Prepare dataset","p":["In the context of RVC, the dataset is an audio file containing the voice the model will replicate. It can be either speaking or singing.","For the best results, having a clean dataset is crucial, so take the time to remove any undesired noises."]},{"i":"section-1","l":"‎"},{"l":"2. Set up RVC","p":["With your dataset ready, it's time to set up RVC to train the model.","There are plenty of versions of RVC, but these are the best ones for beginners. Pick according to your needs:"]},{"i":"section-2","l":"‎","p":["For local users, first ensure you meet the local minimum requirements."]},{"i":"section-3","l":"‎"},{"l":"3. Train the model","p":["Before you start training, we inform you that the training guides are oriented around using TensorBoard. Read about it & install it after setting up RVC.","Good luck & remember to be patient! As this won't be an instant process.","‎"]}],[{"l":"Voice Models","p":["Last update: July 17, 2025","In the field of AI, is a program that was trained to recognize certain patterns or make certain decisions.","In this case, voice models are models trained to replicate a voice, and with AI they apply it to the input audio.","There are plenty of them uploaded to the internet, made by the public. And the best way to make them is with RVC."]},{"i":"section","l":"‎"},{"l":"Voice Model Files"},{"l":"*They are made up of two files:*","p":["This file is the model itself.","Contains data regarding pitch.","While training, RVC generates other .PTHs named D_ and G_, but these are the checkpoints, not usable models.","Contains data regarding the voice's accent and speech manner.","File is additional, but usually crucial for the quality of the model.","While training, RVC generates two .INDEX file, but the right one will be named added_ by default.","As people sometimes upload them incorrectly."]},{"i":"section-3","l":"‎"},{"i":"section-4","l":"‎","p":["‎‎","‎ ‎ ‎ ‎ ‎ ‎ ‎","\uD83D\uDC40‎ If there are multiple models, click the View other models... bar to see the others.","\uD83D\uDCE4‎ Click View Model to View it, and by being logged in, you can click the 3 dots and Download it.","\uD83D\uDD17‎ Click Use Model to Use it on Weights.com.","Access the website here& login by clicking the icon on the top right corner.","Check the description, likes, comments, & audio sample. Feedback can help you know how great the model is.","Click the Hugging Face link to download the model, or copy it if that's what you need.","Click the model & go to the Files and versions tab.","Download the correct files of the model. Then if you need its link, upload it to HF.","Go to the models page& search the model in the Filter by name bar.","Head over to the #find-models channel.","If it only exists in weights.com, download the .ZIP & upload it to HF.","If you get models from different years, remember, the person's voice changes overtime.","If you haven't already, join AI Hub here.","If you need a link for it, use the other methods.","If you're curious about the epochs, learn more here.","In the upper search bar, search your model & click the post.","It searches the models uploaded on Weights.com/AI Hub Discord server.","Models uploaded in AI Hub get automatically stored here too.","Reminder: This is a General AI Platform, not every model is an RVC one.","Searching here is specially useful if you need the model as a link, as the posts include one.","Select the Weights command","Send the message","Tap the three dots & Download model. It will download a .ZIP file of it.‎‎‎","The sample of the gender & vocal style according to the model gives the most accurate representation.","Then go to the #voice-models forum channel.","There's also its web version.","This a website where people can upload voice models.","This is a Discord bot developed by the Weights.com team.","This is a forum channel in AI Hub where people upload their own voice models.","This is a free & open-source platform for storing Any Type AI models, interactive AI apps, & datasets.","This step is specially useful if you get multiple results of the same model.","To download it, click the download symbol ( ) on the right of the .ZIP file. If you need its link, right-click it and copy the address.","Type /find","Type the model","Type the name of the model in the Search bar & click a result.","Users can read/share feedback about the models through comments & likes.","You can listen to the audio sample to get a preview of the it."]},{"i":"section-17","l":"‎"},{"l":"If you couldn't find one, you have 3 options:","p":["Make the model yourself","Pick a different one.","Request a free model via AI HUB's #request-models forum channel. Be aware that we don't allow paid commissions"]},{"i":"section-18","l":"‎"},{"l":"1. Zip the model","p":["Select the correct .PTH & .INDEX& zip them into a .ZIP file.","Ensure it's .ZIP & not .RAR or .7ZIP."]},{"i":"section-19","l":"‎"},{"l":"2. Log in","p":["If you haven't already, create an account& log in."]},{"i":"section-20","l":"‎"},{"l":"3. Upload the Model","p":["Once logged in, go to Upload a Voice Model in the train models tab.","‎","In Model name you name the model.","Describe the model in Model Description.","Select a Tag, such as English, Anime, etc.","Upload an Image for the model, like the character's image.","Upload the Model Zip containing the .PTH and .INDEX file.","You're done!"]},{"i":"section-21","l":"‎"},{"i":"section-22","l":"‎"},{"i":"1-zip-the-model-1","l":"1. Zip the model","p":["Select the correct .PTH & .INDEX& zip them into a .ZIP file.","Ensure it's .ZIP & not .RAR or .7ZIP."]},{"i":"section-23","l":"‎"},{"i":"2-log-in-1","l":"2. Log in","p":["If you haven't already, create an account& log in."]},{"i":"section-24","l":"‎"},{"l":"3. Make repository","p":["Once logged in, tap your profile on the upper right corner, & then New Model.","‎","In Model name you name the repo as you want.","Make sure License is set as openrail& the repo is set as Public.","Once done, hit Create model."]},{"i":"section-25","l":"‎"},{"l":"4. Upload model","p":["It will redirect you to the repo. Go to the Files and versions tab on the center, click + Add file on the right & then Upload files.","‎‎","Tap the upload box & submit the ZIP. Or just drag & drop.","Tap on Commit changes to main& the model will begin to upload."]},{"i":"section-26","l":"‎"},{"l":"5. Copy link (optional)","p":["Once it's done, it will redirect you to the files list.","So if you need its link, right-click the download button ( ) of the .ZIP file on the right, and click Copy Link."]},{"i":"section-27","l":"‎"}],[{"l":"What's RVC","p":["Last update: July 17, 2025"]},{"l":"Introduction","p":["RVC ( Retrieval-Based Voice Conversion) is an advanced AI voice cloning software, developed by the RVC-Project team. It's considered the best free & open-source one to date.","It was designed for desktop, requiring great specs to run it effectively, specially GPU for training models.","Though it can be executed through the cloud& be used in any device, in case you don't meet the previous requirement.","RVC doesn't have any major quality improvements since 2023, since its original devs are focused on other projects, RVC is hard to optimize, and it has limitations like non speech sounds such as realistic laughing, screaming, etc. Though, there are commmunity driven Forks that try to experiment with it, mostly about adding new features and performance improvements."]},{"l":"Forks","p":["A fork is a copy of a main GitHub Project. It aims to make a modified version of the project, with improvements, new features & modifications.","RVC has quite a few forks made by the community, each one meeting different needs for the user, and with its pros & cons.","These are the main ones, along with their cloud-based counterparts:","‎"]},{"l":"FAQ"},{"l":"Frequently asked questions."},{"l":"*What's the best fork?*","p":["As explained before, it depends on your needs. It's best to try them yourself.","For local users, Applio is a great starting point. For cloud users you can use either the Applio Colab or applio kaggle."]},{"l":"*What are the requirements for RVC locally?*","p":["30 GB","6 GB","6GB","8GB","For inference, the storage requirement varies depending on the fork. It can be around 5 to 9 GB","GPU","If you don't meet the minimum requirements, it's more convenient to use RVC on the cloud.","MINIMUM REQUIREMENT","NVIDIA GTX 900 Series / AMD RX580 (Mac isn't supported)","NVIDIA RTX 20 Series or later / AMD Radeon RX 5xxx or later (Mac isn't supported)","NVIDIA RTX 20 Series or later / AMD Radeon RX 5xxx or later / Apple M3","RAM","SPEC","Storage","SUGGESTED REQUIREMENT","The minimum specs vary depending if it's for training models or inference.","VRAM"]},{"l":"*Can I use it on my AMD GPU?*","p":["You can, but it's going to be slower, as they don't have CUDA cores.","So it's more convenient using RVC through the cloud.","If you're willing to use a slower version you can go ahead and follow this guide on how to get zluda working with Applio Zluda."]},{"l":"*How long does it take to train?*","p":["The total time depends on a lot of factors, like dataset length, batch size, pretrains, specs, etc.","A 10 min dataset with RMVPE may take around 1 to 2 hours."]},{"l":"*Can I run it on a Mac?*","p":["Yes, on Macs of recent generations.","But you can only do inference& it's a little unstable."]},{"l":"*Do I need internet to use it?*","p":["If you're using RVC locally, no (the only exception would be Applio TTS as it uses Microsoft's Edge TTS as a base).","If you're using it through the cloud, then yes."]},{"l":"*Is there a scientific paper for RVC to understand more about it?*","p":["There isn't an official one, but there's an unofficial complex blog to understand how it works."]},{"i":"section-9","l":"‎"}],[{"l":"Applio","p":["Last update: August 3, 2025"]},{"i":"section","l":"‎","p":["Applio is a VITS-based Voice Conversion Tool developed by the IA Hispano team. It's a Fork of Original/Mainline RVC.","It's liked for its great UI, performance improvements and lots of extra features, such as TTS (with RVC models too), plugins, automatic model upload, customizable theme & more.","Because of its user-friendly experience & active development, it's considered to be one of the best forks.","Applio has it's own Applio Docs, which may have more info about the tool."]},{"l":"Are RVC Models Safe?","p":["RVC Models are PyTorch Models, a Python library used for AI. PyTorch uses serialization via Pythons' Pickle Module, converting the model to a file. Since pickle can execute arbitrary code when loading a model, it could be theoretically used for malware, but this fork has a built-in feature to prevent code execution along the model. Also, HuggingFace has a Security Scanner which scans for any unsafe pickle exploits and uses also ClamAV for scanning dangerous files.","‎"]},{"l":"Pros & Cons"},{"l":"*Learn more*","p":["Very complete","Has an active development","Currently stable","Faster interface","Faster Training","Has Crepe for Training","TTS features","Automatic model upload","User-friendly UI","TensorBoard included","Extra features: (plugins, model fusion, etc)","None \uD83D\uDE04"]},{"l":"System & Hardware Requirements","p":["Check if you meet the requirements to run it locally.","If you don't meet the requirements, there are 4 Cloud Versions:","Applio UI Kaggle","Applio UI Google Colab","Applio NO UI Google Colab","Applio UI Lightning.AI"]},{"i":"section-1","l":"‎"},{"i":"section-2","l":"‎","p":["Make sure that you place Applio inside a folder on C drive.","Don't put it in a folder with privileged access.","Don't run the run-install.bat as an administrator.","Make sure the path does not contain any spaces or special characters.","Deactivate your antivirus and firewall to avoid missing dependencies."]},{"l":"Nvidia on Windows (Precompiled)","p":["The easiest way to download Applio is by going to Applio's Hugging Face repo, and clicking the [download] button on the right-hand side.","Unzip the folder. This may take a few minutes.","Open Applio's folder & execute run-applio.bat.","‎‎","A console tab will appear, and after a moment your default browser, with Applio ready to use.‎"]},{"l":"Nvidia RTX 5000 Series on Windows (Precompiled Fix)","p":["The following steps are an unofficial workaround to use Applio with NVIDIA RTX 50 series cards until the next official release includes updated PyTorch.","Follow the standard Nvidia on Windows (Precompiled) download steps first.","After you have extracted the precompiled folder, navigate to it in Windows Explorer.","In the address bar, type CMD and press Enter. This will open a Command Prompt window in the correct directory.","In the CMD window, paste and run the following command to upgrade PyTorch to a compatible version:","If you get an error about requirements being already satisfied, you must first uninstall the existing versions. Run this command first, and then run the command above again.","‎","A console tab will appear, and after a moment your default browser, with Applio ready to use.‎"]},{"l":"Linux & macOS","p":["The easiest way to download Applio is by going to Applio's Hugging Face repo, and clicking the [download] button on the right-hand side.","Unzip the folder. This may take a few minutes.","Make sure you have Python 3.10.12 installed. You can check your version by running python --version.","Open a terminal in the Applio directory you just extracted.","Run the commands corresponding to your Linux distribution:","Run Applio","In the terminal, run the following commands to make the script executable and launch the application:","‎","A console tab will appear, and after a moment your default browser, with Applio ready to use.‎"]},{"l":"Debian/Ubuntu"},{"l":"Arch"},{"l":"Fedora"},{"l":"AMD on Windows (Precompiled Fix)","p":["‎","A console tab will appear, and after a moment your default browser, with Applio ready to use.‎","Add the bin directory of your HIP SDK installation to your system's Path environment variables.","Download a compiled version of Applio(v3.2.5 or higher) and unzip it to your desired folder.","Download and install the VC++ Runtime.","First, check the official System Requirements on the AMD ROCm™ documentation site. In the \"Windows-supported GPUs\" section, determine which steps to follow below.","For AMD GPU users, follow these steps to set up Applio:","For HIP SDK 5.7: C:\\Program Files\\AMD\\ROCm\\5.7\\bin","For HIP SDK 5.7: Run patch_zluda_hip57.bat.","For HIP SDK 6.1: C:\\Program Files\\AMD\\ROCm\\6.1\\bin","For HIP SDK 6.1: Run patch_zluda_hip61.bat.","It's assumed your primary AMD GPU has an index of 0. If you have an iGPU that is listed first in Device Manager (under 'Display Adapters'), you must edit the run-applio-amd.bat file and change the value from 0 to 1.","Move all .bat files from this folder to the main (root) Applio folder.","Navigate to the assets\\zluda folder inside your Applio directory.","Open a Command Prompt in the Applio folder (type CMD in the address bar and press Enter). Run the following commands to install the correct version of PyTorch for Zluda.","Run run-applio-amd.bat to start Applio.","Run the patch file that corresponds to your HIP SDK version:","The very first time you run a task (like inference or training), Applio may appear to freeze for 15-20 minutes. This is normal. Zluda is compiling the necessary kernel code in the background. Subsequent runs will be fast.","This guide is for AMD GPU users on Windows. It uses Zluda to enable CUDA compatibility."]},{"l":"GPU has a green check in the HIP SDK column","p":["Install either v6.1.2 or v5.7.1 HIP SDK from the AMD ROCm Hub."]},{"l":"GPU is RX 6600, 6600XT, 6650XT, 6700, 6700XT, or 6750XT","p":["Install v5.7.1 HIP SDK from the AMD ROCm Hub.","Download the correct archive for your GPU:","For 6700, 6700XT, 6750XT, download the gfx1031 archive.","For 6600, 6600XT, 6650XT, download the gfx1032 archive.","Navigate to C:\\Program Files\\AMD\\ROCm\\5.7\\bin\\rocblas\\ and rename the library folder to library.old.","Create a new, empty folder named library in its place.","Unzip the content of the archive you downloaded into this new library folder."]},{"l":"All other AMD GPUs","p":["Find your GPU's gfxNNNN value. You can do this by searching \"techpowerup your_gpu_name\" (e.g., \"techpowerup RX 7900 XTX\") and finding the \"Shader ISA\" on the specifications page.","Follow the steps for your corresponding gfx value:","Install v5.7.1 HIP SDK from the AMD ROCm Hub.","Download this archive.","Navigate to C:\\Program Files\\AMD\\ROCm\\5.7\\bin\\rocblas\\ and rename the library folder to library.old.","Unzip the content of the archive directly into the C:\\Program Files\\AMD\\ROCm\\5.7\\bin\\rocblas\\ folder.","Visit this repository and follow the specific instructions provided there."]},{"i":"section-3","l":"‎"},{"i":"section-4","l":"‎","p":["If you encounter an issue, be sure to read the Troubleshooting chapter."]},{"i":"section-5","l":"‎"},{"i":"section-6","l":"‎"},{"l":"1. Upload voice model.","p":["Go to the Download tab. You have two ways of uploading it: through its link or manually inputting its files.","Go to the Download tab & paste the link of the model in the Model Link bar. It must be from Hugging Face or Google Drive.‎‎","Press Download Model.","Drag & drop the model's .PTH in the Drop files box below.‎‎","Then drag the .INDEX.","‎"]},{"l":"2. Select voice model.","p":["Return to the Inference tab & click the Refresh button on the right.","‎","Select your model in the Voice Model dropdown."]},{"i":"section-7","l":"‎"},{"l":"3. Input vocals.","p":["With Applio you can convert audios individually or in batches:","Drag & drop the audio or click the upload box to search it.‎‎","Then select it in the dropdown below.‎","Go to the Batch tab.‎","In the Input Folder bar, paste the path folder containing the audios.","In Output Folder you can paste a path folder for the results.","Ensure the paths don't contain spaces/special characters.","‎"]},{"l":"4. Modify settings. (optional)","p":["Unfold Advanced Settings if you wish to modify the inference settings for better results, or to determine the output folder.","‎"]},{"i":"section-8","l":"‎"},{"l":"5. Convert.","p":["Click Convert at the bottom. The audio will begin to process. The processing time will mainly depend on your specs, length of audio & the algorithm picked.","Once it's done, you can hear the results in the Export Audio box below.","By default the output files will be in the \" audios\" folder: \\ApplioV3.0.7\\assets\\audios"]},{"i":"section-9","l":"‎"},{"i":"section-10","l":"‎","p":["The training guide will be centered around using TensorBoard. Read about it first if you haven't already. If you encounter an issue, be sure to read the Troubleshooting chapter."]},{"i":"section-12","l":"‎"},{"l":"1. PREPROCESS","p":["Go to the Train tab. Input a name for your model in Model Name. Don't include spaces/special characters.","‎","Paste the path file of your dataset in the Dataset Path bar. Ensure the path doesn't contain spaces/special characters.","Select your dataset's sample rate. If you don't know the amount, click here.","Ensure RVC Version is set as V2& click Preprocess Dataset.","It'll finish when the output box says preprocessed successfully."]},{"l":"2. EXTRACT","p":["Select the algorithm you want. Use either Crepe or RMVPE, as the rest are outdated.","If you chose Crepe, you can modify its hop length.","Press Extract Features. It'll finish when it says extracted successfully."]},{"l":"3. TRAIN","p":["If you are a newbie, use 8. But in case your dataset is short (around 2 minutes or less), use 4.","Frequency of the saving checkpoints, based on the epochs.‎","If you are a newbie, simply leave it at 15, but if you wish to be percise set it to 1.","‎‎‎","E.g: with a value of 10, they will be saved after the epoch 10, 20, 30, etc.","Input the total amount of epochs(training cycles) for the model.‎","But since we'll use TensorBoard, use an arbitrarily large value like 1000","‎","Sync graphs trains a single epoch and sets the log interval to the amount of steps that epoch trained for.‎","Doing this makes the Tensor Board's graphs accurate.","If you have multiple GPUs, tick GPU Settings to use a specific one for the training.","Click Generate Index. This will create the model's .INDEX file.","Press Start Training to begin the training process.‎","To open TB, execute run-tensorboard in Applio's folder. Remember to monitor it, as well as the console just in case.‎","The latter will show you errors if they happen, and information about the epochs & checkpoints."]},{"l":"4. FINAL STEP","p":["When you're very sure of overtraining, you can stop training by going to the Settings tab & press Restart Applio.","‎","Create a new folder anywhere named as the model.‎","Open Applio's folder, go to logs, and open the folder named as the model.‎","Select the .INDEX named added_& move it to your newly made folder.‎‎‎","In said folder you'll also find all the checkpoints.‎","Select the one closest to before the overtraining point, and move it to the new folder.","The checkpoints will be organized with this format: ModelName_Epoch_Step.pth Example: arianagrande_e60_s120.pth","‎‎","And that's all, have fun with your model. To test it, do a normal inference as usual."]},{"l":"5. RESUMING","p":["In case the training finished but the model still needed training, you don't have to start from scratch.‎","Simply enter the same settings & criteria that you've previously inserted. You don't have to do the preprocess or train the .INDEX again.‎","You can change the save frequency, or increase the Total Epoch amount in case you didn't input enough before.‎","Begin training again & remember to monitor TB & console like before."]},{"i":"section-47","l":"‎"},{"i":"section-48","l":"‎"},{"i":"section-49","l":"‎","p":["+ with any RVC model"]},{"i":"section-50","l":"‎","p":["Applio is also known for having one TTS tool by default, with plenty of voices to choose for.","You can also use it with RVC models& apply the inference settings if you wish.","Aditionally, you can download the Eleven Labs TTS plugin."]},{"i":"section-51","l":"‎"},{"l":"Instructions:","p":["Go to the TTS tab.","‎"]},{"i":"section-52","l":"‎","p":["If you want to use an RVC model, download it, go to TTS, click Refresh& select it in Voice Model& Index File.","‎‎","To modify the inference settings or the output folder for the TTS/RVC audio, unfold Advanced Settings."]},{"i":"section-53","l":"‎","p":["In TTS Voices select the voice of your desired language, accent & gender.","In Text to Synthesize input your text. Then click Convert.","‎‎","If you are using an RVC model, select a voice that matches the model the most, to guarantee great results."]},{"i":"section-54","l":"‎","p":["Once it's done, you'll be able to hear the result in the Export Audio box. By default, the output audio will be in the \" audios\" folder. < \\ApplioV3.0.7\\assets\\audios>"]},{"i":"section-55","l":"‎"},{"i":"section-56","l":"‎"},{"i":"section-57","l":"‎","p":["To Update Applio, you need to firstly Save your audios and models, then Delete the current Applio folder and reinstall the latest version."]},{"i":"section-58","l":"‎"},{"i":"section-59","l":"‎"},{"i":"section-60","l":"‎","p":["Applio has an Extra menu, containing an audio analyzer, originally made by Ilaria.","Making it convenient for determining the sample rate of datasets when training models.","It also contains the model fusion tool, ideal for advanced users."]},{"i":"section-61","l":"‎"},{"l":"Audio Analyzer:","p":["Go to the Extra tab & press the upload box to input your audio. Or simply drag & drop.","‎"]},{"i":"section-62","l":"‎","p":["Once it's done uploading, click Get information about the audio."]},{"i":"section-63","l":"‎","p":["In Sampling rate you'll see the audio's full sample rate. Use said value for training."]},{"i":"section-64","l":"‎","p":["If the frequencies don't reach the top of the spectrogram, see at which number peaks & multiply it by 2."]},{"i":"section-65","l":"‎","p":["‎‎","Here it reached 20 kHz. Doubling it gives 40kHz. Therefore the ideal target sample rate would be 40k"]},{"i":"section-66","l":"‎"},{"i":"section-67","l":"‎","p":["Plugins are components that you can add to Applio, that add new features & enhance your experience.","These are made by the public, and are free & easy to install.","You can find them on their GitHub page. More will be added in the future."]},{"i":"section-68","l":"‎"},{"l":"Installation:","p":["Access their GitHub page & click on the name of the plugin you want.","‎"]},{"i":"section-69","l":"‎","p":["Click on the ZIP file.","‎‎","Click on the download button on the right. This will download the ZIP file of the plugin."]},{"i":"section-70","l":"‎","p":["Open Applio & head over to the Plugins tab. Drag & drop the ZIP file to the upload box.","‎‎","You will be able to see its installation process in the console."]},{"i":"section-71","l":"‎","p":["Go to the settings tab & click Restart Applio at the bottom. Then you'll be able to see the plugin in the Plugins tab."]},{"i":"section-72","l":"‎"},{"i":"section-73","l":"‎"},{"i":"section-74","l":"‎"},{"l":"*There's no option for my sample rate.*","p":["If it's lower than 32k: select 32k.‎","If it's 44.1k: select 40k.‎","If i'ts higher than 48k: select 48k."]},{"l":"*The voice glitches out.*","p":["This a phenomenon called artifacting. To fix it, read here."]},{"l":"*I couldn't find my answer.*","p":["Report your issue here."]},{"i":"section-78","l":"‎"}],[{"l":"Mainline","p":["Last update: August 3, 2025"]},{"i":"section","l":"‎","p":["Mainline RVC is the base, original, & unmodified official version of RVC. Made by the RVC-Project team. It can be called either Original/Mainline RVC.","It has less features compared to other forks, but still has the necessary tools to do a decent job.","It's specially liked because it's a little faster than other forks, as it's less bloated in a way.","Its actual name is not \"Mainline\", but it was given by the public to properly distinguish it from the other versions.‎"]},{"l":"Pros & Cons"},{"l":"***Unfold***","p":["Easy to install.","Simpler to use.","Doesn't have an active development.","Has less features.","Doesn't include Mangio-Crepe.","Manual model upload.","Won't work for RTX 50 Series GPUs."]},{"i":"section-1","l":"‎"},{"i":"section-2","l":"‎","p":["Go to their download page here","Click the Download word. RVC will begin to download.‎‎","Once it's done, unzip the folder.","Open RVC's folder, find the \" go-web.bat\" file and execute it.‎‎ It will then open a console, & after a moment your default web browser with RVC ready to be used.‎‎‎‎","(Optional) To access RVC more easily, make a shortcut of the go-web file.‎"]},{"i":"section-3","l":"‎","p":["If you encounter an issue, be sure to read the Troubleshooting chapter."]},{"i":"section-4","l":"‎"},{"l":"1. Upload voice model.","p":["Open RVC's folder, go to the assets folder and put your model's .PTH file inside the weights folder.‎‎","Return to the previous folder & put the model's .INDEX file in the logs folder."]},{"i":"section-5","l":"‎"},{"l":"2. Select voice model.","p":["In RVC, click the Refresh voice list and index path button.","‎","In its left, click Inferencing voice& select your model."]},{"i":"section-6","l":"‎"},{"l":"3. Select vocals.","p":["In Enter the path of the audio file paste the path file of your audio. Ensure the path doesn't include spaces or special characters."]},{"i":"section-7","l":"‎"},{"l":"4. Modify settings. (optional)","p":["If you wish, modify the inference settings on display accordingly for better results."]},{"i":"section-8","l":"‎"},{"l":"5. Convert.","p":["Click the long Convert button at the bottom & it will begin to convert.","The processing time will mainly depend on your specs, length of audio, & the algorithm picked."]},{"i":"section-9","l":"‎"},{"l":"6. Download output.","p":["Once it's done processing, a playable audio will pop up in the Export audio box. To download, click the three dots on the right & hit Download.","‎"]},{"i":"section-10","l":"‎","p":["The training guide will be centered around using TensorBoard. Read about it first if you haven't already.","If you encounter an issue, be sure to read the Troubleshooting chapter."]},{"i":"section-12","l":"‎"},{"i":"section-13","l":"‎"},{"l":"1. Go to training area.","p":["Open RVC & head over to the Train tab."]},{"i":"section-14","l":"‎"},{"l":"2. Name the model.","p":["In Enter the experiment name you insert a name for your model. Don't include special characters or spaces."]},{"i":"section-15","l":"‎"},{"l":"3. Select Target Sample Rate.","p":["In Target sample rate select the number that matches your datasets' sample rate. Inputting an incorrect one might screw up the final quality."]},{"i":"section-16","l":"‎"},{"i":"section-17","l":"‎"},{"l":"4. Select dataset.","p":["In Enter the path of the training folder paste the path file of your dataset. Ensure the path doesn't include special characters/spaces.","‎","If there's any text in the bar, delete it beforehand."]},{"i":"section-18","l":"‎"},{"l":"5. Process data.","p":["Click the Process Data button on the center.","RVC will process the previous criteria for the training. But also the dataset file, which might take a moment depending on how big it is.","‎","It'll finish when the output box on the right says end preprocess."]},{"i":"section-19","l":"‎"},{"i":"section-20","l":"‎"},{"l":"6. Select GPUs.","p":["In Enter the GPU index(es) determine which GPU(s) you'll use for training, by indicating the index followed by the dash (e.g: 0)."]},{"i":"section-21","l":"‎"},{"l":"7. Select pitch extraction algorithm.","p":["At the right select the Pitch extraction algorithm. Only use RMVPE_GPU or Crepe, as the rest are obsolete.","‎"]},{"i":"section-22","l":"‎","p":["Now click the Feature extraction button on the right.","‎‎‎ It'll finish when the output says all-feature-done."]},{"i":"section-23","l":"‎"},{"l":"8. Create .INDEX.","p":["Press Train feature index at the bottom center. This will create the .INDEX file.","‎‎ It'll finish when the output box says something like this:"]},{"i":"section-24","l":"‎"},{"i":"section-25","l":"‎"},{"l":"9. Select save frequency.","p":["Frequency of the saving checkpoints, based on the epochs.","If you are a newbie, simply leave it at 15.","E.g: with a value of 10, they will be saved after the epoch 10, 20, 30, etc.","‎"]},{"i":"section-26","l":"‎"},{"l":"10. Input epochs amount.","p":["In Total training epochs you determine the total amount of epochs(training cycles) for the model.","But since we'll use TensorBoard, use an arbitrarily large value like 2000."]},{"i":"section-27","l":"‎"},{"l":"11. Select batch size.","p":["Leave Batch size per GPU at 8 if you aren't familiar with it.","If your dataset is short (around 2 minutes or less), use 4 instead."]},{"i":"section-28","l":"‎"},{"l":"12. Launch TensorBoard.","p":["Now before you start training, open TB.","If you haven't already, start reading about it here here."]},{"i":"section-29","l":"‎"},{"l":"13. Begin training.","p":["Start training the model by clicking Train model.","‎‎‎ Remember to monitor TB, & also the console just in case. The latter will show you errors if they happen, and information about the epochs & checkpoints.‎"]},{"i":"section-30","l":"‎"},{"l":"14. Stop training.","p":["When you are very sure of overtraining, you can stop training by pressing the Stop training button where Train model used to be."]},{"i":"section-31","l":"‎"},{"l":"15. Gather model's files.","p":["Create a new folder anywhere named as your model.","Open RVC's folder, go to logs, and open the folder named with the model. Select the .INDEX named added_& move it to your newly made folder.","‎","Now go to the weights folder. Here you'll find the model's checkpoints.","Select the one closest to before the overtraining point, and move it to the new folder","These files will be organized with this format: ModelName_Epoch_Step.pth Example: kalomaze_e60_s120.pth","‎‎","And that's all. Have fun with your model. To test the model, do a normal inference as usual."]},{"i":"section-32","l":"‎"},{"i":"section-33","l":"‎","p":["If the training finished but the model still needed training, you don't have to start from scratch. Follow this procedure:","Simply enter the same settings and criteria that you previously inserted. Model name, sample rate, dataset, batch size, etc. You don't have to press Process Data or train the .INDEX again.","You can change the save frequency, or increase the epochs amount in case you didn't input enough before.","Begin training again & remember to monitor TB & console like before."]},{"i":"section-34","l":"‎"},{"i":"section-35","l":"‎"},{"l":"*There's no option for my sample rate.*","p":["If it's lower than 32k: select 32k.‎","If it's 32k: on the right in Version, press v1& press v2 again. Ensure you leave it as v2. You should be able to see a 32k option now.‎","If it's 44.1k: select 40k.‎","If i'ts higher than 48k: select 48k."]},{"l":"*The voice glitches out.*","p":["This a phenomenon called artifacting. To fix it, read here."]},{"l":"*I don't see the Stop Training button.*","p":["This is a common bug. Close the console to stop RVC entirely."]},{"l":"*I couldn't find my answer.*","p":["Report your issue here."]},{"i":"section-39","l":"‎"}],[{"l":"AICoverMaker","p":["Last update: August 3, 2025"]},{"i":"section","l":"‎","p":["AICoverMaker (or known as RVC-AI-Cover-Maker-WebUI) is an Applio RVC Fork developed by the Eddy, as a better and updated version of the old AICoverGen.","It's liked for its great UI& Automated AI Cover Process, making it the easiest way to make AI Covers, as it automatically separates instrumentals & vocals, and mixes them back with the converted vocals."]},{"l":"Are RVC Models Safe?","p":["RVC Models are PyTorch Models, a Python library used for AI. PyTorch uses serialization via Pythons' Pickle Module, converting the model to a file. Since pickle can execute arbitrary code when loading a model, it could be theoretically used for malware, but this fork has a built-in feature to prevent code execution along the model. Also, HuggingFace has a Security Scanner which scans for any unsafe pickle exploits and uses also ClamAV for scanning dangerous files.","‎"]},{"l":"Pros & Cons"},{"l":"*Learn more*","p":["Automatically separates instrumentals and mixes them with converted ones","Currently stable","Faster interface","Automatic model upload","Has Mangio-Crepe","User-friendly UI","Can't Train models","No Precompiled versions for Non-Windows Users","Doesn't support Mac nor any NON-Nvidia GPUs"]},{"l":"System & Hardware Requirements","p":["SPEC","MINIMUM REQUIREMENT","OS","Windows 10 or later / Any Modern Linux Distro","RAM","6GB","Storage","6 GB","SUGGESTED REQUIREMENT","GPU","NVIDIA RTX 20 Series or later","6GB+","In case you don't meet the requirements to run it locally, it also has a 2 Cloud Versions: Kaggle & Colab"]},{"i":"section-1","l":"‎"},{"i":"section-2","l":"‎","p":["Don't put the folder in a directory with privileged access (like C:\\Program Files).","Make sure the file path does not contain any spaces or special characters.","It's recommended to temporarily deactivate your antivirus and firewall to avoid missing dependencies during installation."]},{"l":"Precompiled (Windows)","p":["The easiest way to download RVC-AI-Cover-Maker-WebUI is by going to Eddy's Latest GitHub Release, and clicking the Precompiled version.","Unzip the folder. It may take a few minutes.","Open the AICoverMaker folder & execute run.bat.","‎‎","A console tab will appear, and after a moment your default browser will open with the WebUI ready to use."]},{"l":"Source / Manual (mainly for Linux)","p":["‎","Download the source code, either .zip (which is the most suggested usually) or .tar.gz, from the latest release link.","‎‎","If you download the .zip from the latest release make sure to rename the folder from rvc-ai-cover-maker-ui-v1.0.5(or whatever version is the latest version) to just rvc-ai-cover-maker-ui otherwise you may run into missing dependencies issues.","Extract the folder. It may take a few minutes.","Open the AICoverMaker folder & execute the script run.sh for Linux, or run.bat for Windows.","A console tab will appear, and after a moment your default browser will open with the WebUI ready to use."]},{"i":"section-3","l":"‎"},{"i":"section-4","l":"‎"},{"l":"Music Download","p":["1. When in the UI look at the top left and look for the tab named Download Music and click it.","‎","2. Then put the song you want to download in the text box and click download."]},{"l":"Model Download","p":["1. In the UI look at the top left and look for the tab named Download Model and click it.","‎","2. Then put the model you want to download in the text box and click download.","3. You can also drag and drop your model in the Drop files box to upload them directly."]},{"i":"section-5","l":"‎"},{"i":"section-6","l":"‎","p":["Please use our Inference Settings guide to find out the inference settings do what.","TTA- results in longer separation time, it gives a little better SDR score but hard to tell if it's really audible in most cases\". it “means \"test time augmentation\", it will do 3 passes on the audio file instead of 1. 1 pass with be with original audio. 1 will be with inverted stereo (L becomes R, R become L). 1 will be with phase inverted and then results are averaged for final output."]},{"i":"section-7","l":"‎"},{"i":"section-8","l":"‎","p":["To Update AICoverMaker, you can either:","Open AICoverMaker's folder & execute the script update.sh for Linux, or update.bat for Windows.","Download the latest precompiled the next time a new version comes out and replace the files."]},{"i":"section-9","l":"‎"}],[{"l":"Weights.com","p":["Last update: August 3, 2025","Weights.com is an easy, freemium AI Cloud Website platform that provides tools for creating AI voice covers, text-to-speech, and more. It serves as an all-in-one web UI, eliminating the need for powerful local hardware or complex setups.","The platform is designed to be user-friendly, catering to both beginners and experienced users. You can train your own custom voice models or use a vast library of high-quality, community-made models, from both the AI Hub Discord& the Weights.com Models Page.","Weights is partnered with AI HUB& DreamTavern.","It's named Weights, as it was originally meant only for RVC with the domain Weights.gg, storing RVC Models, which are PyTorch Weights. Later on, the domain changed to Weights.com and expanded to other things, but for this specific guide, we will focus on the RVC-related side."]},{"l":"Pros & Cons"},{"l":"*Learn more*","p":["All-in-one web cloud platform.","No need for a powerful PC.","Mobile friendly, with even an app.","User-friendly interface suitable for beginners.","Large library of public RVC voice models (anime, singers, etc.).","Free tier available for basic usage.","Integrated TTS and direct recording options.","Active community for sharing and collaboration.","Free plan has limitations on credits and features.","Processing and training times could be slow due to queues in certain, but it's better on Paid Tiers.","Less granular advanced control compared to forks like Applio or Mainline/Original RVC, Because it's meant for everyone and not just tech experts."]},{"l":"Training Disclaimer","p":["Before you train your first model, Weights will present you with a disclaimer that you must accept. This ensures that the tool is used ethically and legally.","Never use the voice of someone without their consent.","Don’t use voices for fraud, scams, or bullying.","Protected intellectual property rights must be respected.","Voices should be used for good, not for harm. Have fun!","You can review the full legal documents here:","Terms of Service","End-User License Agreement (EULA)"]},{"l":"Instructions:","p":["1. Navigate to the Train Model tab from the main menu. You will be presented with the \"New Voice Model\" page.","2. Fill in the Model Details:","Select or drop image: Upload a picture to serve as the thumbnail for your model.","Model Name: Assign a clear and descriptive name.","Model Description: Add optional details about the voice or its intended use.","Private Model: Enable this option if you want the model to be visible only to you. Premium only","3. Upload your Input Audio:","This section is for your dataset, the collection of voice recordings the AI will learn from.","You can drag and drop audio files directly. Minimum 3 minutes, It is recommended to upload at least 5-10+ minutes of clean audio with minimal background noise for good results.","Premium users can train longer datasets, up to 30 minutes.","Weights will automatically clean your dataset.","4. Start the training process by clicking Start Training.","Depending on your subscription, you may see different options, such as \"Train and Publish\" or \"Use Premium Training Item\".","Learn more about Free Premium Items via Streaks.","Once you have a model, you can use it to generate voice covers. This process is also known as inference."]},{"i":"instructions-1","l":"Instructions:","p":["1. Choose a Voice to Use","You can select one of your own trained models or browse the extensive public library of Voice Models.","Use the search bar to find specific characters or artists.","2. Choose your Input Audio","You have three main options for providing the audio to be converted:","Upload an audio file from your device. This is the most common method for creating song covers.","Write text directly into the provided box. Since RVC is Speech-To-Speech Natively, You will need to select a base TTS voice (like \"John\") which will be used as an input for RVC Model.","Record audio directly through your microphone, which is useful for quick tests or generating voice lines."]},{"l":"3. (Optional) Choose Your Settings","p":["Before creating the cover, you can adujst the output with several settings."]},{"l":"Basic Settings","p":["Pre-Stemmed: Turn this on if your uploaded audio file contains only vocals without any instrumental backing.","Pitch: Adjust the pitch of the output voice. A positive value makes it higher, while a negative value makes it lower."]},{"l":"Advanced Settings","p":["De-Echo & Reverb: Removes echo and reverb from the source vocal track for a cleaner conversion.","Isolate Main Singer: Attempts to convert only the lead vocals, ignoring background singers.","Instrumental Pitch: Changes the pitch of the instrumental track separately from the vocals.","Volume Envelope: Lower values make the output volume closer to the original vocal's volume dynamics.","Consonant Protection: Helps reduce audio artifacts and slurring on consonants, especially at lower volumes."]},{"l":"4. Create and Download","p":["Click the Create button to start the conversion process.","Your request will be added to a queue. You can view its status on the My Creations page, where it will show as \"Processing\".","Once finished, you can play the result directly and download it to your device.","Weights.com operates on a freemium model. It offers a Free Plan with core features and daily credits, alongside paid Basic and Pro plans for users who need more and better resources.","Subscriptions increase the number of daily covers, queued items, and weekly voice trainings, while also allowing for longer maximum audio lengths.","Limits and features are subject to change. Always check the Weights.com Official Pricing Page for the most current information.","Weights.com rewards users for daily activity through a Streak system.","To keep your streak going, you must create at least one item (Voice Cover, Image, or Chat) each day before the streak resets.","As a reward for consistency, you will earn a Free Premium Training every 5 days you maintain your streak. This allows you to train more RVC voice models with a bigger dataset without a paid subscription."]}],[{"l":"Applio Kaggle","p":["Last update: August 1, 2025"]},{"i":"section","l":"‎","p":["Kaggle is a cloud platform for using AI apps, powered by virtual machines with powerful GPU's.","It's a great alternative for training RVC voice models through the cloud, since it has the best GPUs.","You only get 30 free GPU hours per week."]},{"l":"Pros & Cons"},{"l":"***Learn more***","p":["Has good GPU's","Has 30 GPU hours","Fast","TensorBoard included","You can leave training unsupervised.","Takes some time to set up."]},{"i":"section-1","l":"‎"},{"l":"Create an Account"},{"i":"section-2","l":"‎"},{"l":"1. Set up account.","p":["Start by making an account here.‎","‎","Verify your acount with a phone number so you can turn on the \"internet\" option."]},{"i":"section-3","l":"‎"},{"l":"Notebook Creation & Setup"},{"i":"section-4","l":"‎"},{"l":"2. Clone Notebook","p":["Go to Kaggle and click \"Create\" then \"New Notebook\" at the top left.","‎","Under your session's name click \"File\" then \"Import Notebook\".","On the new window that appeared on the right click \"Link\" then type in the box this link https://github.com/IAHispano/Applio/blob/main/assets/Applio_Kaggle.ipynb.","Click \"Import\" on the bottom right once you've done this.","When it's done importing it will display this text window.","Under \"Session options\" in the sidebar turn on \"internet\". Make sure persistance is on for both files and varibles.","Turn on T4 X2 GPUs in accelerator.","‎ g: (Optional) Turn on headless mode so you can run so you can run the GPU on all sessions and save your progress. Go to the top right and click \"Save version\" then open the advanced dropdown.","Your runtime will continue draining when you're not running any cells with this option on."]},{"i":"section-5","l":"‎"},{"l":"Ngrok Setup"},{"i":"section-6","l":"‎"},{"l":"3. Ngrok Setup","p":["Scroll down to the fifth cell and you should see a section where you put your ngrok token. If you dont have a ngrok acount sign up here. a2. Once you have an acount you can authenticate your ngrok tunnel agent here: https://dashboard.ngrok.com/get-started/your-authtoken‎","put the Ngrok token in the second cell like so:","Once the Ngrok token is there run the cell.","There is a monthly limit rate with Ngrok so dont be supprised if training is suddenly interrupted."]},{"i":"section-7","l":"‎"},{"l":"Installation"},{"i":"section-8","l":"‎"},{"l":"4. Installation Cells","p":["Starting from the top run all the cells, with the first being:","a2. When it's done it will output Finished.","Now run the last cell which is:"]},{"i":"section-9","l":"‎"},{"l":"Using Applio"},{"i":"section-10","l":"‎"},{"l":"5. Ngrok Links","p":["Click on the Applio URL link to open Applio's UI, click the Tensorboard Url link to open the Tensorboard and click File Url to open the file manager.","‎","Once you've click the Applio Url it will take you to Applio's UI where it operates the same as normal Applio. If you happen to not know how to use Applio you can read about it in the Local Applio Docs, it operates similarly."]},{"i":"---","l":"‎ ‎"}],[{"l":"Applio Colab","p":["Last update: June 15, 2024"]},{"i":"section","l":"‎","p":["Applio is a VITS-based Voice Conversion Tool developed by the IA Hispano team.","It's liked for its great UI& lots of extra features, such as TTS (with RVC models too), plugins, automatic model upload, customizable theme & more.","Because of its user-friendly experience & active development, it's considered to be one of the best forks.","As this cloud version is hosted in Google Colab, remember that you have a runtime of 4 hours.‎"]},{"l":"Pros & Cons"},{"l":"*Learn more*","p":["Very complete","Has an active development","Currently stable","Very fast","TTS features","Automatic model upload","Has Mangio-Crepe","User-friendly UI","TensorBoard included","Extra features: (plugins, model fusion, etc)","Usage limit for free users"]},{"i":"section-1","l":"‎"},{"i":"section-2","l":"‎","p":["Access the Colab space here. Then log in to your Google account."]},{"i":"section-3","l":"‎","p":["Execute the Install Applio cell. This will take around 2 minutes.","‎‎","It'll finish when you see a tick symbol on the left."]},{"i":"section-4","l":"‎","p":["If you are going to train models, upload your dataset to your Google Drive storage & run the Extra cell.","‎‎","To save time, unfold it & cancel the custom pretrain download, if you aren't going to use them."]},{"i":"section-5","l":"‎","p":["Grant the permissions to Google Drive.‎‎"]},{"i":"section-6","l":"‎","p":["Select your sharing method then execute Start Applio.","‎‎","Then open the public URL.","If you select ngrok put your ngrok token, which can be found here in the text box."]},{"i":"section-7","l":"‎","p":["Be sure to read the Troubleshooting chapter if any issue arises."]},{"i":"section-8","l":"‎"},{"l":"1. Upload voice model.","p":["Go to the Download tab. You have two ways of uploading it: through its link or manually inputting its files.","Go to the Download tab & paste the link in the Model Link bar. It must be from Hugging Face or Google Drive.‎‎","Press Download Model.","Below in Drop files, press the upload box & input the model's .PTH.‎‎","Then input the .INDEX.","‎"]},{"l":"2. Select voice model.","p":["Return to the Inference tab & click Refresh on the right.","‎","Select the model in the Voice Model& Index File dropdown."]},{"i":"section-9","l":"‎"},{"l":"3. Input vocals.","p":["With Applio you can convert audios individually or in batches:","Press the upload box & input your audio.‎‎‎‎","Then select it in the dropdown below.‎","Go to the Batch tab.‎‎‎","Go to the file explorer in Colab. Go to drive, right-click the folder containing the audios & click Copy Path.","Paste the path in the Input Folder bar.","In Output Folder you can define the path folder for the results.","Ensure the paths don't contain spaces/special characters.","‎"]},{"l":"4. Modify settings. (optional)","p":["Unfold Advanced Settings if you wish to modify the inference settings for better results.","‎"]},{"i":"section-10","l":"‎"},{"l":"5. Convert.","p":["Click Convert at the bottom to process the audio.","Once it's done, you can hear the results in the Export Audio box below. To download it, press the download symbol on its right.","‎"]},{"i":"section-11","l":"‎"},{"i":"section-12","l":"‎"},{"l":"1. PREPROCESS","p":["Go to the Train tab. Input a name for your model in Model Name. Don't include spaces/special characters.","‎","Upload your dataset to your GD storage if you haven't already.‎","In Colab click the folder on the left ( ) & click the reload button.‎‎‎(For mobile users: tap the three lines on the top left & Show file browser)‎","Open drive, localize your dataset, right-click it & click Copy path.‎‎‎‎","Then paste it on the Dataset Path bar.‎‎","Select your dataset's sample rate. If you don't know the amount, click here.","Ensure RVC Version is set as V2& click Preprocess Dataset.","It'll finish when the output box says preprocessed successfully."]},{"l":"2. EXTRACT","p":["Select the algorithm you want. Use either Crepe or RMVPE, as the rest are outdated.","If you chose Crepe, you can modify its hop length.","Press Extract Features. It'll finish when it says extracted successfully."]},{"l":"3. TRAIN","p":["‎‎‎","‎","But since we'll use TensorBoard, use an arbitrarily large value like 1000","Click Generate Index. This will create the model's .INDEX file.","Disconnect from your Internet.","Don't solve the captchas that (might) pop up occasionally.","E.g: with a value of 10, they will be saved after the epoch 10, 20, 30, etc.","Frequency of the saving checkpoints, based on the epochs.‎","If after around 2:30 hours of training you don't detect OT download the model of the lowest point, in case it's already OT, and the .INDEX.‎","If you are a newbie, simply leave it at 15.","If you are a newbie, use 8. But in case your dataset is short (around 2 minutes or less), use 4.","Input the total amount of epochs(training cycles) for the model.‎","Press Start Training below to begin the training process.‎‎","Run out of GPU runtime.","Stay AFK for a long time.","TB will be available in the Colab. Remember to monitor it, as well as the cell's logs just in case.","The latter will show you errors if they happen, and information about the epochs & checkpoints.‎‎‎‎","Then once your GPU runtime resets, begin the retraining procedure.","Tick Save Only Latest"]},{"l":"4. DOWNLOAD","p":["When you're very sure of overtraining, you can stop training by going to the Settings tab & press Restart Applio.","‎‎","Come back to the Colab & open the new public URL.","Open the file explorer, go to logs, and open the folder named as the model.‎","Download the .INDEX named added_.‎‎‎","In said folder you'll also find all the checkpoints.‎","Select the one closest to before the overtraining point, and move it to the new folder.","The checkpoints will be organized with this format: ModelName_Epoch.pth Example: arianagrande_60e.pth‎‎‎‎","You can determine the Step number of the checkpoints by looking at its epoch number on the logs.","‎‎‎","And that's all, have fun with your model. To test it, do a normal inference as usual."]},{"l":"5. RESUMING","p":["In case the training finished but the model still needed training, you don't have to start from scratch.","Simply enter the same settings & criteria that you had previously inserted. You don't have to do preprocess, extract feature or train the .INDEX again.‎","You can change the save frequency or increase the Total Epoch amount, in case you didn't input enough before.‎","If you're resuming from a new session, unfold the Extra cell in Colab & input the model name you assigned before.‎‎‎","For this, the Auto Backup cell must've ran in the previous session.‎‎","Begin training again & remember to monitor [TB]https://docs.aihub.gg/rvc/resources/training/#tensorboard) as before."]},{"i":"section-47","l":"‎"},{"i":"section-48","l":"‎","p":["+ with any RVC model"]},{"i":"section-49","l":"‎","p":["Applio is also known for having one TTS tool by default, with plenty of voices to choose for.","You can also use it with RVC models& apply the inference settings if you wish.","Additionally, you can download the Eleven Labs TTS plugin."]},{"i":"section-50","l":"‎"},{"l":"Instructions:","p":["Go to the TTS tab.","‎"]},{"i":"section-51","l":"‎","p":["If you want to use an RVC model, download it, go to TTS, click Refresh& select it in Voice Model& Index File.","‎‎","To modify the inference settings or the output folder for the TTS/RVC audio, unfold Advanced Settings."]},{"i":"section-52","l":"‎","p":["In TTS Voices select the voice of your desired language, accent & gender.","In Text to Synthesize input your text. Then click Convert.","‎‎","If you are using an RVC model, select a voice that matches the model the most, to guarantee great results."]},{"i":"section-53","l":"‎","p":["Once it's done, you'll be able to hear the result in the Export Audio box. To download it, click the download button on its right ( )."]},{"i":"section-54","l":"‎"},{"i":"section-55","l":"‎"},{"i":"section-56","l":"‎","p":["Applio has an Extra menu, containing an audio analyzer, F0 Curve and Model Information.","Making it convenient for determining the sample rate of datasets when training models.","It also contains the model fusion tool, ideal for advanced users."]},{"i":"section-57","l":"‎"},{"l":"Audio Analyzer:","p":["Go to the Extra tab & press the upload box to input your audio.","‎"]},{"i":"section-58","l":"‎","p":["Once it's done uploading, click Get information about the audio."]},{"i":"section-59","l":"‎","p":["In Sampling rate you'll see the audio's full sample rate. Use said value for training."]},{"i":"section-60","l":"‎","p":["If the frequencies don't reach the top of the spectrogram, see at which number peaks & multiply it by 2."]},{"i":"section-61","l":"‎","p":["‎‎","Here it reached 20 kHz. Doubling it gives 40kHz. Therefore the ideal target sample rate would be 40k"]},{"i":"section-62","l":"‎"},{"i":"section-63","l":"‎","p":["Plugins are components that you can add to Applio, that add new features & enhance your experience.","These are made by the public, and are free & easy to install.","You can find them on their GitHub page. More will be added in the future."]},{"i":"section-64","l":"‎"},{"l":"Installation:","p":["Access their GitHub page & click on the name of the plugin you want.","‎"]},{"i":"section-65","l":"‎","p":["Click on the ZIP file.","‎‎","Click on the download button on the right. This will download the ZIP file of the plugin."]},{"i":"section-66","l":"‎","p":["Open Applio & head over to the Plugins tab. Press the upload box & upload the ZIP."]},{"i":"section-67","l":"‎","p":["Go to the Settings tab & click Restart Applio at the bottom. Go back to the Colab & open the new public URL.","Then you'll be able to see the plugin in the Plugins tab."]},{"i":"section-68","l":"‎"},{"i":"section-69","l":"‎"},{"i":"section-70","l":"‎"},{"l":"*There's no public URL.*","p":["In case the public URL doesn't show up, there might be a problem with Gradio, you can check if it's down here.","To fix this, instead of waiting until Gradio is back online, just check the share_tunnel* checkbox on the Start Applio cell.","‎","Applio will use localtunnel instead of the Gradio Public Share Link now, copy paste the Password IP(Don't worry, it's the Google PC's IP, not yours).","Then open the Share Link given by the colab and paste the \"Password IP\" in \"Tunnel Password\", finally click Submit."]},{"l":"*There's no option for my sample rate.*","p":["If it's lower than 32k: select 32k.‎","If it's 44.1k: select 40k.‎","If i'ts higher than 48k: select 48k."]},{"l":"*The voice glitches out.*","p":["This a phenomenon called artifacting. To fix it, read here."]},{"l":"*Cannot connect to GPU backend.*","p":["You have exhausted the GPU runtime of Colab."]},{"l":"*I couldn't find my answer.*","p":["Report your issue here."]},{"i":"section-76","l":"‎"}],[{"l":"Applio Lightning Ai","p":["Last update: July 17, 2025"]},{"l":"Introduction","p":["Lightning.Ai is a cloud platform for using AI apps, powered by virtual machines with powerful GPU's.","It's a great alternative for training RVC voice models through the cloud, since it has the best GPUs with tons of VRAM.","Lightning has decent Free GPU hours (in the free plan which can change) but it does have the best GPUs out of all the other cloud options."]},{"l":"Pros & Cons"},{"l":"***Learn more***","p":["Has good GPU's.","Has lots of VRAM","TensorBoard included.","You can leave training unsupervised.","Takes some time to set up.","Needs a phone number.","Low/Decent GPU time depending on what GPU you choose.","2-3 Day verification wait time."]},{"i":"section","l":"‎"},{"l":"How to Setup"},{"l":"1. Set up Account.","p":["First make an account with Lightning Ai","‎","Make sure you verify yourself with a phone number. Once you've done that you will get an email that looks like this:","You will need to wait 2-3 business days to become fully verified","Once you are verified Lightning Ai will send you a email that conatins this:"]},{"l":"2. Notebook and Setup.","p":["Once you have become verified click this link and it should take you to your home page so you can create a notebook.","In the top right click New Studio","‎","Set the studio type to code, teamspace to vision model and select any gpu you want for the machine. Click create when you've selected everything.","Once you are in the notebook go to the left side right under main.py and right click. Select New Notebook.","Select python 3 for your kernal.","In the first cell paste in the code from this code block:","Click the run button once you've pasted the code.","When it's done it will say Finished installing requirements!. When it says that paste in this second code block in another cell.","Once again click the run button when you've pasted this code in another cell."]},{"l":"Code Block 1"},{"l":"Code Block 2"},{"i":"section-2","l":"‎"},{"l":"Using Applio"},{"i":"section-3","l":"‎"},{"l":"1. Open the Gradio link.","p":["To access Applio's UI click on the link next to Running on public URL:, after that it is basically the same as using Applio locally or on other cloud platforms."]},{"l":"2. Accessing Files.","p":["To upload a dataset, upload audio or anything else find the Teamspace Drive button on the right and click it.","‎","The path to Applio is Studio this_studio Applio Applio","Once you're there you can just drag and drop files.","To download files click on the file then click the three dots on the right of it and click download"]},{"l":"3. Opening the Tensor Board.","p":["Find the Tensor board icon on the right side bar and click it.","‎","Once you've done that it will open the Tensor board. To learn how to use it go here"]},{"l":"4. Opening the notebook.","p":["If you want to go back to the notebook simply click on the Jupyter icon on the right.","‎"]},{"i":"section-4","l":"‎"},{"l":"How to use Better GPUs"},{"i":"section-5","l":"‎"},{"l":"1. Swapping GPUs.","p":["To swap GPUs go to the GPU icon the the right and click it.","‎","Then click on GPU and it will show you a list of GPUs you can use by clicking on them and then clicking request.","75 hours monthly of T4 16gb","31 hours monthly of L4 24gb","15 hours monthly of L40 48gb"]},{"i":"---","l":"‎ ‎"}],[{"l":"Applio no UI Colab","p":["Last update: Jan 31, 2025"]},{"i":"section","l":"‎","p":["Applio is a VITS-based Voice Conversion Tool developed by the IA Hispano team.","It's liked for its great UI& lots of extra features, such as TTS (with RVC models too), plugins, automatic model upload, customizable theme & more.","Because of its user-friendly experience & active development, it's considered to be one of the best forks.","As this cloud version is hosted in Google Colab, remember that you have a limited runtime of around 4 hours.‎"]},{"l":"Pros & Cons"},{"l":"*Learn more*","p":["Very complete","Has an active development","Currently stable","Very fast","TTS features","Automatic model upload","Has Mangio-Crepe","User-friendly UI","TensorBoard included","Extra features: (plugins, model fusion, etc)","Usage limit for free users"]},{"i":"section-1","l":"‎"},{"l":"Installation"},{"i":"section-2","l":"‎"},{"l":"1. Running cells.","p":["Start by accessing the colab here.","Then run the Installation cell to install all the requirements."]},{"i":"section-3","l":"‎"},{"l":"Training"},{"l":"2. Preprocess Dataset.","p":["Name your model whatever you want.","Then upload your dataset to your google drive.","Type in the path to your dataset into dataset_path.","Select your sample rate.","Run the cell."]},{"i":"section-4","l":"‎"},{"l":"3. Extract Features.","p":["Choose the f0 method you want, usually RMVPE is the best.","If you chose Crepe set your hop length to 32, 64 or 128. If you chose RMVPE ignore this option.","Run the cell."]},{"i":"section-5","l":"‎"},{"l":"4. Training.","p":["Set the total number of epoch you want to train for.","Choose your batch size, 8 is the best for most cases.","Enable cleanup if this is your first time training a model and you're not resuming.","Set how many epochs you are going to save. If you want to get the best epoch set this to 1 but if you're ok with close enough you can set this to a higher number.","Turn on save_only_latest.","Run the cell to start training!"]},{"i":"section-6","l":"‎"},{"l":"5. Resuming Training.","p":["Set the model names to exactly what you had before.","Run the first cell.","Select your sampe rate and f0 method in the second cell.","Run the final cell.","Then run the training cell again."]},{"i":"---","l":"‎ ‎"}],[{"l":"AICoverMaker","p":["Last update: Mar 8, 2025‎"]},{"i":"section","l":"‎","p":["AICoverMaker (or known as RVC-AI-Cover-Maker-WebUI) is an Applio RVC Fork developed by the Eddy, as a better and updated version of the old AICoverGen.","This are ideal for users who want ' quick & dirty' AI covers, as the whole process of inputting audio, vocal isolation & song mixing is automated."]},{"i":"section-1","l":"‎"},{"i":"section-2","l":"‎"},{"l":"Installation & Setup","p":["1. Go to the AICoverMaker Colab and run the first cell.","‎","Installation may take a couple of minutes, be patient.","2. Next go to the second cell and put your ngrok token in the text box and run it. If you dont have a ngrok acount sign up here and you can authenticate your ngrok tunnel agent here.","There is a monthly limit rate with Ngrok so dont be supprised if training is suddenly interrupted.","2a. You can choose not to use Ngrok if you desire to by running the third cell.","3. Once you've run either the cell with or without ngrok a link under the cell will apear, click it and it will take you the WebUI."]},{"i":"section-3","l":"‎"},{"i":"section-4","l":"‎"},{"i":"installation--setup-1","l":"Installation & Setup","p":["1. Go to the AICoverMaker Kaggle notebook and click Copy Edit on the top right.","‎","2. Once you're in the notebook run the first installation cell. Once it's done it will output Requirements installed.","Installation may take a couple of minutes, be patient.","3. Next go to the second cell and put your Ngrok token in the TOKEN HERE spot and run it. If you dont have a Ngrok acount sign up here and you can authenticate your ngrok tunnel agent here.","There is a monthly limit rate with Ngrok so dont be supprised if training is suddenly interrupted.","3. Once you've run the final cell a ngrok link will apear under the cell, click it and it will take you the WebUI."]},{"i":"section-5","l":"‎"},{"l":"Downloading Music & Models"},{"l":"Music Download","p":["1. When in the UI look at the top left and look for the tab named Download Music and click it.","‎","2. Then put the song you want to download in the text box and click download."]},{"l":"Model Download","p":["1. In the UI look at the top left and look for the tab named Download Model and click it.","‎","2. Then put the model you want to download in the text box and click download.","3. You can also drag and drop your model in the Drop files box to upload them directly."]},{"l":"Inference","p":["Please use our Inference Settings guide to find out the inference settings do what.","TTA- results in longer separation time, it gives a little better SDR score but hard to tell if it's really audible in most cases\". it “means \"test time augmentation\", it will do 3 passes on the audio file instead of 1. 1 pass with be with original audio. 1 will be with inverted stereo (L becomes R, R become L). 1 will be with phase inverted and then results are averaged for final output."]},{"i":"section-6","l":"‎"}],[{"l":"Ilaria RVC Zero","p":["Last update: July 30, 2025"]},{"l":"Introduction","p":["This is a fork of mainline RVC running on Hugging Face Spaces, it’s called this way because it runs on Hugging Face’s ZeroGPU, which currently is an A100 GPU.","Here is the link to the space Ilaria RVC Zero.","For unknwon related ZeroGPU issues, the space is broken. We suggest alternatives till it's fixed or we have updates on the situation.","This space is only for inference which means you can not train a model here."]},{"l":"Model Downloader","p":["Go to any RVC Hugging Face Models and get the Download link by going to the “Files and versions” tab, then right click the Download icon next to the .zip file, and click copy address link, like in the image","‎","Then go to the “Model Loader” tab in the space and click on “Model Downloader”, paste the link into the Model URL give the model a name and click “download model”."]},{"l":"Inference","p":["After you finally uploaded the model, go to the “Inference” tab, and upload the audio which vocals you want to convert to the model’s voice. Click “Refresh Models” and in the “Model” dropdown menu choose yours then click “Convert”.","Then after you get your output, just click on the Download Icon at the top right of the audio file."]},{"l":"Inference Settings","p":["Settings (Inference) While converting, you might have issues of the output pitch may not be the same as the model one, but that’s a thing you can easily fix with the settings!","Go to the “Inference” tab, click on “Settings”, i will explain you the important settings that you may wanna change:","Pitch level: if the output pitch seems different than the model’s pitch, you need to play around with this:","Lower value = male, deeper.","Higher value = female, higher voice tone.","Index influence: How much accent (the one from the .index file) is applied:","Lower value = Less accent of the model’s voice, but helps to reduce artifacting (noises).","Higher value = More accent of the model’s voice, but could have artifacting (depends on the model’s dataset, as the index file depends on it).","For the rest of the settings, you don’t need to change them, leave them as they are."]},{"l":"Ilaria TTS","p":["Go to the “Inference” tab, click on “Ilaria TTS” write all the text you want in “Text”, choose an Edge TTS Model in “Language and Model”, it’s suggested to use one that has the same language and gender of the RVC mode.","b Click on “Speak”, it will generate a TTS audio with the original Edge TTS Model, and then automatically using it as an input"]},{"l":"Errors","p":["ZeroGPU HuggingFace Spaces have a max inference time duration, it’s the time it takes to do an Inference (use the model, not the time of your audio file itself), on default it’s around 1 minute which is what Ilaria RVC uses. You need to retry with a shorter audio, you could also split your audio.","ZeroGPU HuggingFace Spaces have a quota per account, if you aren’t signed in you will get less quota so it’s better to login for more quota. You could get the ‘Sign-up’ part even if you are logged in. The ZeroGPU Quota can’t be seen but it isn’t unlimited. You can either:","Login so you get more quota","Wait","Pay to be an HuggingFace PRO Member to get X5 times more quota","You can find your ZeroGPU quota here","As all ZeroGPU Spaces share this hardware, there might be times where ZeroGPU is busy, if you ever go through this error, you just need to wait a bit and retry.","Ilaria RVC Zero got a limit for the Model Upload Size, if you run into this either use another model."]},{"i":"---","l":"‎ ‎"}],[{"l":"Mainline Kaggle","p":["Last update: August 1, 2025"]},{"l":"Introduction","p":["Kaggle is a cloud platform for using AI apps, powered by virtual machines with powerful GPU's.","It's a great alternative for training RVC voice models through the cloud, since it has the best GPUs.","The Hina Modified Mainline Kaggle has reached EOL (End Of Life), meaning it has had it's last update, it won't be suggested anymore nor fixed for any future issues. We would suggest users to use other alternatives if it breaks, and if it breaks this guide will be removed in the future.","You only get 30 free GPU hours per week."]},{"l":"Pros & Cons"},{"l":"***Learn more***","p":["Has good GPU's","Has 30 GPU hours","Fast","TensorBoard included","You can leave training unsupervised.","Reached End Of Life.","Takes some time to set up.","Doesn't have Mangio-Crepe"]},{"i":"section","l":"‎"},{"l":"How to Setup"},{"i":"section-1","l":"‎"},{"l":"1. Set up account.","p":["Start by making an account here.","Verify your acount with a phone number so you can turn on the \"internet\" option."]},{"l":"2. Clone notebook and setup.","p":["Go to Hina's mainline notebook and click \"Copy and Edit\"","Under \"Session options\" in the sidebar turn on \"internet\". Make sure persistance is on for both files and varibles.","Turn on T4 X2 GPUs in accelerator.","d: (Optional) Turn on headless mode so you can run so you can run the GPU on all sessions and save your progress. Go to the top right and click \"Save version\" then open the advanced dropdown.","‎","Your runtime will continue draining when you're not running any cells with this option on."]},{"i":"section-3","l":"‎"},{"l":"3. Ngrok.","p":["Scroll down to the fifth cell and you should see a section where you put your ngrok token. If you dont have a ngrok acount sign up here.‎ 2. Once you have an acount you can authenticate your ngrok tunnel agent here: https://dashboard.ngrok.com/get-started/your-authtoken‎","put the Ngrok token in the quotation marks like so:","There is a monthly limit rate with Ngrok so dont be supprised if training is suddenly interrupted."]},{"l":"3. Starting the Cells.","p":["From top to bottom execute all the cells. With first being:","The second cell will take ~ 5 minutes to load.","when its finished it will look like this:","If you want to use a pretrain now is the time to download it. Add a new code cell and type this in then run it:"]},{"i":"#","p":["!wget LINK TO PRETRAIN","Run the third cell.","Once you run the final cell it will give you three links.","RVC url: is to open RVC's gui.","File url: is to open Imjoy Elfinder gui.","Tensorboard: is to open the Tensorboard.","The interface should look like this with your D and G files being located here. Here you can manage your files within Kaggle. Whenever you want to download files from the Imjoy GUI just double click. Downloading files uses up the Ngrok bandwith data."]},{"i":"section-6","l":"‎"},{"l":"Starting RVC"},{"i":"section-7","l":"‎"},{"l":"1. Click the RVC link.","p":["It should take you to the GUI where you can then go to the top and click the 'Train' tab."]},{"l":"2. Setup.","p":["Run through the normal RVC setup with setting your model name, sample rate and such. If you are unable to see the 32k sample rate click on V1 then swap back to V2.","Only use V2 Don't use V1 or you will get an error.","In the file manager create a folder named dataset anywhere then drag and drop your dataset in it. Then continue with normal RVC setup and training"]},{"l":"3. Syncing Graphs.","p":["For syncing graphs you need to train however many epochs you have set you save frequency then go into the file manager and find your model which should be in assest/weights, in its name it should have a step count and epoch count like this: model_name_e(number)_s(number).","E means epochs and S means steps.","Once you know how many steps the model trained for stop training the model by stopping the cell.","Then start the same cell and open the file manager and navigate to assest/weights and delete all of your previous models.","Then navigate to the 32k.json file which is located in the V2 folder of /configs and download it by double clicking it, then delete it.","Open the file you have just downloaded in notepad and edit log_interval to the amount of steps your model took, save it then replace the old 32k.json file.","Now go to your /logs/ folder and do the same thing. Modify the log interval of the config.json with your step count. Delete the config.json that's already in the /logs/ folder and replace it with your copy.","Now delete Eval folder, tf-events file, G D_23333333 files and train log file in /logs/ folder."]},{"l":"4. Resuming Training.","p":["Do not process and feature extract again because those files are already in the /logs/ folder. Use the same model name, sample rate, batch size, pretrain, and save frequency to train the model again in the RVC GUI."]},{"i":"---","l":"‎ ‎"}],[{"l":"Mainline Colab","p":["Last update: July 30, 2025","Mainline colab is a port of mainline RVC to Google Colab, for exclusively training.","It's free, includes all the necessary tools for a quality model, the TensorBoard.","The Hina Modified Mainline Google Colab has reached EOL (End Of Life), meaning it has had it's last update, it won't be suggested anymore nor fixed for any future issues. We would suggest users to use other alternatives if it breaks, and if it breaks this guide will be removed in the future.","‎"]},{"l":"Pros & Cons"},{"l":"***Learn more***","p":["Has TensorBoard.","Reached End Of Life.","Inconvenient.","Takes some time to set up.","You can't leave training unsupervised.","Doesn't have Mangio-crepe.","For free users:","It's slower compared to local RVC.","Can't train long datasets without pausing the process."]},{"i":"section","l":"‎"},{"l":"How to Setup"},{"i":"section-1","l":"‎"},{"l":"1. Running cells.","p":["Start by accessing the colab here.","Then run the first two cells to install all the requirements."]},{"l":"2. Installing Pretrains.","p":["If you wish to install a custom pretrain go to the 'Download Custom Pretrains' cell and go into the dropdown menu and find the pretrain you want.","If the pretrain you want isn't there go to the top left and click '+ Code'.","Then in the new cell type in !wget LINK TO PRETRAIN"]},{"i":"section-3","l":"‎"},{"l":"3. Ngrok.","p":["Scroll down to the fifth cell and you should see a section where you put your ngrok token. If you don't have a ngrok acount sign up here.‎ 2. Once you have an acount you can authenticate your ngrok tunnel agent here: https://dashboard.ngrok.com/get-started/your-authtoken‎","put the Ngrok token in like so:","‎","There is a monthly limit rate with Ngrok so don't be supprised if training is suddenly interrupted."]},{"i":"#","p":["The interface should look like this with your D and G files being located here. Here you can manage your files. Whenever you want to download files from the Imjoy GUI just double click. Downloading files uses up the Ngrok bandwith data."]},{"i":"section-5","l":"‎"},{"l":"Starting RVC"},{"i":"section-6","l":"‎"},{"l":"1. Click the RVC link.","p":["It should take you to the GUI where you can then go to the top and click the 'Train' tab."]},{"l":"2. Setup.","p":["Run through the normal RVC setup with setting your model name, sample rate and such. If you are unable to see the 32k sample rate click on V1 then swap back to V2.","Only use V2 Don't use V1 or you will get an error.","In google drive create a folder named training and inside it make another folder named dataset then drag and drop your dataset in it. Then continue with normal RVC setup and training","Make sure you use WAV or Flac files inside. Do not zip the folder."]},{"l":"3. Syncing Graphs.","p":["For syncing graphs you need to train however many epochs you have set you save frequency then go into the file manager and find your model which should be in training/weights, in its name it should have a step count and epoch count like this: model_name_e(number)_s(number).","E means epochs and S means steps.","Once you know how many steps the model trained for stop training the model by stopping the cell.","Then start the same cell and open then navigate to assest/weights in google colab and delete all of your previous models.","Then navigate to the 32k.json file which is located in the V2 folder of /configs and edit log_interval to the amount of steps your model took, save it.","Now go to your /logs/ folder and do the same thing. Modify the log interval of the config.json with your step count.","Now delete Eval folder, tf-events file, G D_23333333 files and train log file in /logs/ folder."]},{"l":"4. Resuming Training.","p":["Do not process and feature extract again because those files are already in the /logs/ folder. Use the same model name, sample rate, batch size, pretrain, and save frequency to train the model again in the RVC GUI."]},{"i":"---","l":"‎ ‎"}],[{"l":"Dataset & Isolation","p":["Last update: July 30, 2025","‎","In this massive guide it will be explained how to properly prepare a dataset to make a RVC model.","In the field of AI, it's the collection of data used to train an AI model. It contains examples of the inputs the model is expected to handle, along with the correct outputs.","In the context of RVC, it's an audio file that's given to RVC, containing the voice the model is going to replicate. It can be a speaking, singing voice drums, sound effects or noise.","The quality, variety& length of the dataset are the biggest determining factors for the final quality of the model. Let's explain Length and Variety.","For beginners we recommend sticking with a dataset length of 15 minutes of pure data not counting silence, or if you desire a natural sounding model go for 40+ minutes of dataset. Just remember quality over quantity.","Variety in your dataset is also important because without it RVC lacks the ability to generate diverse audio.","Some things to increase the generalization abilities of RVC and increase the diversity in your dataset include:","Removing repeated words. ( If you want you can be extreme you can do this and remove every single repeated word that's fine, but generally there is no need to do this. )","Include speech in many ranges and pitches.","Longer datasets.","Expressive speech.","A quality dataset is super important for RVC since without one RVC will struggle to make anything good or believable."]},{"i":"section","l":"‎"},{"l":"Clean vocals.","p":["Ensure there isn't much background noise, reverb, overlapping voices, music, distortion, or small silences. Some quiet natural background noise is fine and won't ruin your model since the original pretrains for RVC were made with a noisy dataset, so RVC knows how to deal with noise. You'll learn more on cleaning vocals in the Vocal Isolation & Cleaning section below.‎"]},{"l":"Audio quality.","p":["The higher the audio quality, the better. If possible have it in a lossless format like WAV or FLAC, not a lossy one like MP3. No converting a MP3 to a FLAC or WAV won't remove the compression.‎"]},{"l":"No harsh sibilance/popping.","p":["Additionally, don't include harsh sibilance (loud \"S\" & \"SH\" pronunciation) or popping sounds (loud \"P\" sound)","Robotic sibilances are due to your dataset being short or they are overfitted. You can fix this by making your dataset larger or by choosing an epoch where the sibilants aren't overfitted.","Harsh sibilances are due to your dataset having harsh sibilants. You can fix this by de-essing or making your dataset larger.‎"]},{"l":"No Audio Damage.","p":["The most inportant part of a clean dataset, if your audio is damaged RVC will struggle with it causing it to overall sound worse because RVC will create synthetic data and try to learn from it, so make sure your audio isn't damged.‎","In RVC, artifacting refers to an anomaly where the output voice sounds \"robotic\" & glitchy. This occurs after the inference or model training process."]},{"l":"Causes","p":["It usually occurs when the dataset/vocal sample meets any of these criteria:","• Audio is low-quality• Voice model was overfitted, undertrained or overtrained• There are overlapping voices• There is reverb• There is noise","As you noticed, most of the issues boil down to the audio sample not being properly clean. RVC is built for purely working with voices, not other sounds.","Remember that the cleaner your input audio is, the better the results."]},{"l":"Solutions"},{"l":"1. Use a lossless format:","p":["If possible, it's best if your audio is in a lossless format like WAV or FLAC, preserving its original quality.","Avoid using lossy ones like MP3 or OGG.‎"]},{"l":"2. If doing inference:","p":["Remove undesired noises with an vocal isolation software.","Lowering the search feature ratio can also minimize this issue.","If breathing sounds produce it, lower the Protection value.‎"]},{"l":"3. If training models:","p":["‎","‎‎‎","*A model isn't there.*","*Cleaning Vocals \uD83D\uDDE3️*","*Cleaning Vocals* \uD83D\uDDE3️","*Extracting Vocals From Songs \uD83C\uDFB6*","*Extracting Vocals From Songs* \uD83C\uDFB6","*Extrating Vocals From Songs* \uD83C\uDFB6","*I can't remove some of the backing vocals.*","*I couldn't find my answer.*","*MVSEP extracted too much/too little.*","*UVR extracted too little/too much.*","A higher value will deepen the extraction, and a lower one will soften it.","A vocal isolation app is a software designed to extract a person's vocals from an audio file, usually through the use of AI models.","Access the space here, you don't need an account to use this.","Aggresive","Anvuew mel dereverb v2","At the right you can select the output format. We recommend picking FLAC. Learn why here.‎","BS Roformer","Check the best models list & in CHOOSE MODEL pick the one according to what you need to remove.‎ If you need to remove multiple noises, follow this pipeline for the best results: Remove instrumental - Remove reverb - Extract main vocals - Remove noise‎","Check the model list. In Select VR Model pick the one according to what you need to remove.‎ If you need to remove multiple noises, follow this pipeline for the best results: Remove instrumental - Remove reverb - Extract main vocals - Remove noise","Check the model list. Pick the one according to what you need to remove.‎ If you need to remove multiple noises, follow this pipeline for the best results: Remove instrumental - Remove reverb - Extract main vocals - Remove noise","Chunk Size: 485100","Cleaning Vocals \uD83D\uDDE3️","Click Browse File& select your audio, or simply drag & drop. The audio will begin to upload.","Click Browse File& select your audio, or simply drag & drop. The audio will begin to upload.‎‎‎‎","Click Select input to input the vocals. Or just drag the files to it.‎","Click Select input to select your audio/s. Or just drag the files to it.‎","Click Separate& when it's done converting it will redirect you to a page, where you can listen the results.‎","Click Spererate! below. Wait a moment for the audio to process.‎","Click the Start processing button at the bottom. And that will be all.","Click the wrench (\uD83D\uDD27) on the left & go to Download Center","De-Noise","De-Reverb","DeNoise by aufr33","Download the result located in the output folder.","Each audio is different, so you'll have to test the ideal value.","Ensure to clean your dataset properly, this includes removing silences and distortions.","Errors","Execute the Gdrive Connection cell by pressing the play button . Grant all the permissions.‎‎‎","Extract Backing Vocals","Extract from vocals","Extract vocals","Extraction","First access the Colab space here.‎","First go to X-minus's website and click the \"Vocal Remover\" at the top right.","First, make an account here.‎","For better results, have the audio in a lossless format( WAV or FLAC), & not MP3.","For free users, you can't convert audios in batches or longer than 10 minutes. If that's your case, trim it into different pieces.","For RVC users, the best app is Ultimate Vocal Remover 5 (or UVR). It can be used either locally or through the cloud.","For unknwon related ZeroGPU issues, the space is broken. We suggest alternatives till it's fixed or we have updates on the situation.","Gabox's voc_fv4","Go to their official website& buy it.","Go to their official website& press Download UVR. If you want to use BS / Mel Roformer you are going to need to install this.","If you want to remove noise manually to avoid ai artifacts you can use RX 11, which is mentioned in this guide.","If you're extracting lead vocals, remember to download the backing ones if you wish to keep them.","If your GPU is compatible with CUDA, toggle GPU Conversion on for a faster process.","If your GPU is compatible with CUDA, toggle GPU Conversion on for a faster process.‎‎","In \"De-Noise\" select \"Mel-Roformer De-Noise\". You can also check the model list to see what is the best model for your needs.","In CHOOSE PROCESS METHOD select MDX-Net, and select either the BS Roformer-Viper-X 1296 or MDX23C model.","In Google Drive, make two folders, named input& output.‎‎‎","In Output encoding select FLAC. We recommend selecting FLAC from now on. Learn more here.‎","In Output encoding select FLAC. We recommend selecting FLAC from now on. Learn more here.","In Process Method select VR.‎","In Select output you can define the folder for the results.","In Separation type select BS Roformer‎","In Separation Type, select DeNoise by aufr33.‎","INST Gabox V7","Instrumental","It will redirect you their GitHub page. Click the download link for your operating system. UVR is available both on Windows & Mac.","It'll finish once the logs say Mounted at /content/drive‎","Logging in is not mandatory, but recommended for shorter waiting lists.","Login so you get more quota","MDX-Net","MDX23C (De-Reverb)","Mel denoiser","Mel Roformer De-Noise","Mel roformer karaoke","MelBand Karaoke","MelBand Roformer","Model","Modify the Aggression Setting value on the right.","Most of the extraction model are behind a pay wall.","MVSEP is a website for isolating vocals, that works similarly as UVR.","Now click the long Start Processing button.","Once it's done it will look like this:","Once it's done uploading, in CHOOSE PROCESS METHOD, select BS/Mel Roformer. Under that you can change Segment Size and Overlap, the defaults are fine.‎‎","Once logged in, go to the main page.","Once the audio is done uploading, click Separate","Once the installer finishes downloading, execute it & follow the instructions. Make sure to tick \uD83D\uDDF9 Create a desktop shortcut for an easier access to UVR.","Overlap: 8","Pay to be an HuggingFace PRO Member to get X5 times more quota","Playable audios will then appear in the output boxes below. To download the output, click the little download icon in the top right.","Process Method","Report your issue here.","Reverb Removal","Reverb removal by anvuew V2 (MelRoformer)","Run the audio through BVE. Modify the Aggression Setting if necessary.","Same thing for the Instrumental, if you wish to keep it.","Select the category of the model (MDX-NET or VR)","Select your model of choice and run the Separation cell. You can look here for the best models","Separation Type","Set Window Size to 320. (optional) Lower Window Size yield a higher output quality, but will take longer to process.‎","So if the output has any undesired noises, follow the procedure on Cleaning Vocals.‎","Tap the Input Audio box & select your audio, or simply drag & drop.‎‎‎‎","Tap the three buttons of the Vocals audio and then Download.‎","Tap the three dots of the audio you need and then Download. If you wish to keep the backing vocals stem, remember to download it too.","The goal is to get an audio sample with clean and natural vocals, which is what RVC needs to give the most accurate & quality results.","The UVR Colab is much faster & convenient for this task. Use MVSEP if you run out of GPU runtime or feel lazy to convert your audio to WAV.","Then click \"select a file\" and choose a audio file, or you can drag and drop a file. And when it's done it will look like this:","Then click the download button (\uD83D\uDCE5). The model will download, which will take a few minutes","Then Log in to your Google account.‎","Then run the Install cell.","Then select \"Music and vocals\" and choose \"Bs Roformer\"","There is a queue so make sure you make an account to skip most of it.","These are the best settings:","They can remove undesired noises, like background noise, reverb, echo, music, etc.","This determines the depth of the extraction. Only the VR method has it.","This step is not mandatory, but recommended for better results.","TIP: To test models/options more efficiently, tick Sample Mode to only process 30 seconds of your sample.","To do these next steps you are going to need Spek and Audacity.","To use RX 11 it is STONGLY recommended that you read this guide on RX 11.","Try running the audio through MelBand Karaoke or BVE. Modify the Aggression Setting if necessary.","Unfold its dropdown & select the model that you need","unwa big beta v5e","unwa instrumental v1e","Using the Separation Type of DeNoise by aufr33, you can modify the Aggressiveness. This determines the depth of the extraction.","Usually songs include reverb & backing vocals. These negatively impact the results in RVC.","Usually songs include reverb & backing vocals. These negatively impact the results in RVC.‎","UVR BVE 2","Vocals","Vocals/Instrumental","VR","Wait","When it's done converting it will redirect you to a page where you can listen the results.","You can now click \"Vocals\" to download the vocals and \"Other\" to download the instrumentals.","You have exhausted the GPU runtime of Colab.","You'll require great specs & GPU to run it effectively. Otherwise, use either the google colab version or the Huggingface space.","ZeroGPU HuggingFace Spaces have a max inference time duration, it’s the time it takes to do an Inference (use the model, not the time of your audio file itself), on default it’s around 1 minute which is what Ilaria RVC uses. You need to retry with a shorter audio, you could also split your audio.","ZeroGPU HuggingFace Spaces have a quota per account, if you aren’t signed in you will get less quota so it’s better to login for more quota. You could get the ‘Sign-up’ part even if you are logged in. The ZeroGPU Quota can’t be seen but it isn’t unlimited. You can either:"]},{"l":"Step 1: Find the Sample Rate","p":["‎","And finally, introduce these values:","Download and install Spek here.","Encoding: 32-Bit Float","Format: WAV","Go go to Effects -> Volume and Compression -> Loudness Normalization","Go go to Tracks -> Align Tracks -> Align End to End","Go to Effects -> Special -> Truncate Silence","If the frequencies don't reach the top of the spectrogram, see at which number peaks & multiply it by 2. Here it reached 20 kHz. Doubling it gives 40kHz. Therefore the ideal target sample rate would be 40k","In Audacity import your audio","LUFS are used over db because hifigan needs perceptual quality and db doesnt offer that.","On the upper right corner go to File and click Export Audio.‎‎‎","Open spek and just drag & drop audio into it.","The most common sample rates are 32, 40, 44.1, & 48. The higher the sample rate, the more information it stores, therefore the higher the quality.","This is a unit in that defines the total amount of samples(data) that can fit within 1 second of an audio. They are measured in kilohertz (kHz).","Use the following values:‎","Use these values:‎","While training in RVC, you'll have to set the target sample rate as your dataset's. This value affects the final quality."]}],[{"l":"Training","p":["Last update: May 5, 2025","In this guide it will be explained how to properly train a model from start to finish.","Properly training a model is just as important as having a great dataset.","It won't be explained how to prosess a dataset and how to acutally train a model since that is difference from fork to fork, please look at the guide for your fork to find this info.","\"Epoch\" is a unit of measuring the training cycles of an AI model.","In other words, the amount of times the model went over its dataset and learned from it."]},{"l":"How many epochs should I use for my dataset?","p":["There isn't a way to know the right amount previous to training. It depends on the length, quality and diversity of the dataset.","If you aim towards a quality model, it's not convenient to input a semi-arbitrary amount of epochs, as it makes it prone to underfitting/overtraining. (explained later)","So it's best to use TensorBoard. With it you can determine exactly for how long you should train. (explained later)"]},{"l":"Do more epochs equal a better model?","p":["No it doesn't, since using a disproportionate amount will overtrain the model, which will affect the quality of it.","In the field of AI, is when an AI model learns its dataset too well, to the point where it centers too much around it & starts replicating undesired data.","The model performs very well with data of the dataset, but poorly with new data, as it has lost its ability to replicate anything that deviates from it.","It happens when the model is trained for too long/is too complex. So to avoid this, RVC users use a tool called TensorBoard."]},{"l":"What is overtraining?","p":["‎","A batch size is the number of training examples used in one iteration before updaing the model's parameters. For 30+ minutes of data batch size 8 is recommended and for less than 30 minutes batch size 4 is recommended.","Bigger batch size:","Can beneficial in cases where your dataset is big and diverse.","Can lead to early overtraining or flat / 'stuck' graphs.","Can lead to instability / divergence or noisy graphs.","Finetune: Trained with a pretrain.","Generalization might be improved.‎","Generalization might be worsened.","Merge: Made by merging pretrains. (These are considered the worst)","More suitable when your dataset is small, less diverse or repetitive.","Overtraining also know as overfitting is where the model doesn't actually learn the underlying patterns of the data and memorizes them instead.","Pretrains are an integral part of making a model, they are basically models that have been trained with many different types of voices, genders, ages, languages, manor of speech and are much longer then normal models. The objective of pretrains is to reduce training time and increase the quality of your model. To make a model without a pretrain you would need several hours of data to make anything decent.","Promotes noisier, less stable gradients.","Promotes smoother, more stable gradients.","Scratch: Trained with no previous pretrain.","Smaller batch size:","Some signs of overfitting are when the sibilances are robotic, when the graphs in the Tensorboard are going up or when the model is unable to produce high end harmonics because it's learning your dataset to well and your dataset doesn't have these high end harmonics.","There are three types of pretrains:","This image is a bit extreme but it gives you a good idea. If you notice your model is poorly creating high end harmonics try using a model several epochs back."]},{"i":"section","l":"‎"},{"l":"How do i use Pretrains?","p":["Go into the training tab and check the 'Custom Pretrained' box and use the drop down to select the pretrain's D and G file.","If you dont see a pretrain in the dropdown that means you need to download a pretrain, go into the 'Downloads' tab then go to 'Download Pretrained Models' then use the dropdown to select your sample rate and what pretrain you would like to download, then finally click download.","If you want to upload pretrains manually go into your Applio folder then go to rvc\\models\\pretraineds\\pretraineds_custom and place your D and G files there.","Asssuming you have the pretrain you want to use go into your mainline folder then go to assets\\pretrained_v2 and place you D and G files there.","Then in the 'Train' tab near the train button you can input the location of your pretrain, replace the ending so it's the name of the pretrain you put in pretrained_v2."]},{"i":"section-1","l":"‎"},{"l":"Where do i find Pretrains?","p":["32k Download:","40k Download:","48k Download:","Base 32k Download:","D Download","Fine-Tuned 32k Download:","G Download","G Download‎","GuideVocalPretrain is a fine-tuned pretrain based on the original pretrain. This contains 58 hours of Korean speech with the goal being to improve Korean speech.","Here is a list of all publicly available pretrains:","KLM 4 is the final HiFi-GAN pretrain that is going to be made by SSS. This version of klm is like all of the others but it follows the original structure of training and contains noise in the dataset so it can handle it better. This was trained with 800 hours of data, with a large portion of it being in Korean.","KLM 4.1 is a fine-tuned based on KLM V7 pretrained and made with around 100 hours dataset (Korean vocal/speech, Japanese vocal/speech and English speech), so it will work better with those languages. Unlike typical pretrained models KLM is a pretrained model created to make vocal guides using short voice recordings from a studio, this means that even with short dataset high pitch information it is possible to implement high-pitched sounds but it is sensitive to noise so it is recommended to use it with high quality datasets","KLM 4.2 maintains the same highly extensive pitch range as before and was developed to be able to handle high-pitched vocal inference even without having the corresponding vocal data of the model you wish to generate. KLM 4.2 was trained with 146 hours of data which mostly contains Korean, Japanese and some English.","Nanashi V1.7 is a fine-tuned based on TITAN pretrained and made with 11 hours of Brazilian music, so it will work better with this language but it can work with other languages without any problems, like TITAN, it allows models to be trained with few epochs and handles the noise better.","Nezox is a fine-tuned pretrain based on the original pretrain. This pretrain contains 43 hours of Indonesian speech with the goal of the pretrain to make Indonesian speech better.","Ov2Super is a fine-tuned based on the original RVC V2 pretrained and made with 30 minutes dataset, works well for small datasets and English language, this pretrained was trained on a precisely chosen clean speech and singing dataset, with bright and emotional voices. Additionally, it allows models to train with very few epochs compared to regular pretrains.","Rigel is a fine-tuned pretrain based on Rigel Base. Rigel Base has 1921 of speech from most langauges, Rigel fine-tuned has 102 of high quality speech also from a ton of langauges. The goal of this pretrain is to be a better base then the original pretrain.","SingerPetrain is a fine-tuned based on Ov2 Super pretrained and made with 14 hours dataset (English singers). It is most suitable for training singers but it works for everything, the vocal range dataset is c1 to db7 so it works well with bass, baritone, tenor, alto, mezzo-soprano, soprano voices.","Snowie is a fine-tuned pretrain based on the original pretrain. This pretrain's goal is to improve Russian speech without effecting English speech. This was trained with 21 hours of Russian speech.","SnowieV3 X RIN_E3 continues the training with Snowie dataset and then finetuned with additional data, so it will work better with English, Russian and Japanese language and also helps models of other languages to pronounce them well.","SnowieV3.1 is a fine-tuned based on Snowie base pretrained (not publicly available) and made with 58 hours dataset (Russian and Japanese), so it will work better with those languages and also helps models of other languages to pronounce them well.","This is a fine-tuned based on the original RVC V2 pretrained and made with 22 hours of dataset aimed towards e-girl, soft male/female and deep male/female voices.","This is a fine-tuned based on the original RVC V2 pretrained and made with a 11.3 hour dataset aimed towards e-girl, soft male/female and deep male/female voices. This model was trained with Mangio-Crepe/Crepe (Applio) therefore it is advisable to use this extraction algorithm with a 128 hop length or below and have a clean dataset due to the sensitivity to noise of this algorithm.","This is a fine-tuned pretrain based off of the original pretrain which aims to improve anime-style speech. This was train with 11 hours of speech.","This is a fine-tuned pretrain based on the original pretrain that improves drum models.","This is a fine-tuned pretrain based on the original pretrains and was made with 10 hours of Italian speech. Itaila was made to improve Italian speech.","This is a fine-tuned pretrain based on the original pretrains and was made with 2 hours of robotic speech which aims to make robotic voices better.","This pretrain is made from scratch with a 140 hour dataset. It is suggested to use this with high quality datasets due to its sensitivity to noise.","TITAN is a fine-tuned based on the original RVC V2 pretrained, leveraging an 11.15-hours dataset sourced from Expresso. It gives cleaner results compared to the original pretrained, also handles the accent and noise better due to its robustness, being able to generate high quality results. Like Ov2 Super, it allows models to be trained with few epochs.","UKA is a fine-tuned pretrain based on the original pretrain. This pretrain has 8 hours of english speech all containing the British accent.","You can find all of the community made pretrains in the \"pretrain-models\" channel in AI HUB."]},{"i":"section-2","l":"‎"},{"l":"How do i make Pretrain?","p":["Creating a pretrain is pretty much the same as training a normal model but the dataset is bigger and longer.","There are two ways of making a pretrain the first being:","From scratch which means you don't use a pretrain when training this. To make a decent from scratch pretrain you are going to need at least 50 hours of low, mid and high quality speech with many different speakers. The second way being:","Finetuning which means you use a pretrain to train this pretrain. To make a good you are going to need at least 10 hours of high quality speech with many speakers.","The big pro of making a Finetune is that you can tailor it to anything, like you can tailor it to improve a certain language, improve accents, types of speech and more. It can even improve the graphs (like grads, g/total etc.) if trained properly."]},{"l":"Misc","p":["3060 (Ti)","3070 (Ti)","3080 (Ti)","3090 (Ti)","4060 (Ti) (8/16gb)","4070 (Ti)","4070 (Ti) (Super)","4080 (Super)","4090","A Tier:","A: There is no \"best pretrain\" it all depends on your needs and what you're ok with sacrificing to get those benefits.","A10, T4","A100 (80gb and 40gb)","A40","B Tier:","C Tier:","D Tier:","Each of these are different in fidelity and require their own pretrains to use.","H100","HiFi-GAN","In Applio you are given the choice between three vocoders:","L4","L40S","MRF HiFi-GAN","P 100","Q: What is the best pretrain?","RefineGAN","S Tier:","This section contains miscellaneous information about pretrains.","To make a pretrain you are going to need a pretty good GPU, because without one it will take a very long time to train. Here is a GPU tier list for training pretrains:","V 100"]},{"l":"HiFI-GAN","p":["The first vocoder choice is HiFi-GAN the original GAN used in RVC which is combatible with all version of RVC and forks. HiFI-GAN is pretty basic and has muddy high ends."]},{"l":"MRF HiFI-GAN","p":["The second choice is MRF HiFI-GAN, this is a modfied version of HiFi-GAN with MRF instead of MPD, new loss functions and non-simplified version of the resolution block.","Pros:","Higher fidelity","44.1k Training","Cons:","Only a slight upgrade from Hifi-GAN","Not many pretrains for it"]},{"l":"RefineGAN","p":["The third and final choice is RefineGAN, this is an entirely different GAN then HiFi. This GAN uses noise to fill in the gaps and has a different resolution block.","Pros:","Higher fidelity and quality","44.1k Training","TensorBoard is a tool that allows you to visualize & measure the training of an AI model, through graphs & metrics.","It's specially useful for determining when to stop training a voice model, since with it you can detect when the overtraining point begins.","Because of this, TB is the most convenient tool for RVC users for perfecting a voice model."]},{"i":"section-4","l":"‎"},{"l":"Installing & Opening"},{"i":"section-5","l":"‎","p":["Download this file & move it inside mainline RVC's folder. Ensure the file path doesn't contain spaces/special characters.","Training"]},{"i":"section-6","l":"‎","p":["Now execute it. It will open a console window & create some folders inside RVC.","If you get the Windows protected your PC issue, click More info& Run anyway.‎","Once it's done, your default browser should open with TensorBoard app.‎","If it doesn't, copy the address of the console at the bottom, and paste it in your browser. Said address will say \" https://localhost\" followed by some numbers.‎"]},{"i":"section-7","l":"‎"},{"l":"Usage Guide","p":["‎‎","‎","Activate Ignore outliers in chart scaling.","And the right one is to center the view.‎‎‎","As you can see in the image above there is an area with several low points, so in this scenario you would try several epochs in that area to find the best sounding epoch.","Click the gear () in the top left corner & turn on Reload data. You can always manually refresh with the refresh symbol (\uD83D\uDD04) in the top right.","d/total shows how well the discriminator is able to differentiate between real and generated audio.","Each graph has three buttons in the corner:","First ensure auto-refresh is on, so the graphs update constantly.","FM shows how well the generator is able to make synthetic data that has similar features to the dataset.","Go to the SCALARS tab.‎‎","grad_norm_d shows the magnitude of gradients during training. If the gradients are becoming too large (over 100 for fintuning) that can cause some training instabilities and if they are becoming small that can lead to slow learning.","grad_norm_g shows the magnitude of gradients during training. If the gradients are becoming too large (over 1,000 for fintuning) that can cause some training instabilities and if they are becoming small that can lead to slow learning.","If it reaches a low point, let it run for longer until it's very clear it's OT.","If the graph is decreasing that means the discriminator is becoming better at distinguishing between real and synthetic data which usually means that the generator is producing realistic audio.","If the graph is decreasing that shows that the generator is making audio with similar distribution of latent variables to real data.","If the graph is decreasing that shows that the generator is producing audio with similar spectral distribution to the dataset.","If the graph is increasing that indicates that the generator is able to make audio that has similar features to the dataset.","If you get the No dashboards are active issue, select SCALARS in the top right corner dropdown.","If you want you can just use the lowest avg g/total point.","If you're fintuning it's best if the gradients don't go above 1,000.","If you're fintuning it's best if the gradients don't go above 100.","In the search bar, type \" g/total\" then look for the avg graph. This will be the graph you'll monitor.‎‎‎‎","Is a mel spectrogram view of audio from your dataset.‎","Is a mel spectrogram view of audio that the generator created in attempt to make it match mel_org.‎","KL makes the generator create similar distribution of latest variables to real data. The KL loss ensures that the generator is not just memorizing real data but it's learning to capture the underlying patterns in the data.","Left one is for going fullscreen.","Middle one to disable Y axis, for a fuller view.","Now let the training go for some time.","Open TB & begin training in RVC.","Select your model in the Runs section below. The models you tick will show in the graphs. (untick /eval if you want)‎‎‎","Set Smoothing to 0.987.","The mel spectrogram loss compares both the real and synthetic mel spectrograms. This loss encourages the generator to produce audio that sounds similar to the dataset.","Then over your mouse over these low points and take note of the step counts. Since this is using the avg graphs you may not find the exact epoch connected to the step count so just choose the closest point.","Then zoom out & lower the smoothening. Then in the avg graph look for low points around where it started to overtrain.","There will be various low points, one after the other, so don't get too anxious if it's OT or not. You can always use a previous checkpoint either way.","To zoom in & out the graphs, press the ALT key + mouse wheel. Remember to center the view after moving around, and after the graph updates.","While looking through the Tensor Board you may come across slice/mel_gen and slice/mel_org.","you can think of this as clarity / fidelity.","You can think of this as how well it can replicate the speakers style.","you can think of this as how well the model can match timbral, spatial and temporal characteristics.","You'll detect OT(overtraining) when the graph hits the lowest point, then stay flat/ rising indefinitely.‎ Example of OT:"]}],[{"l":"Inference Settings","p":["Last update: July 30, 2025"]},{"i":"section","l":"‎","p":["When doing inference in RVC, you'll come across to quite a few options that you can tweak, that influence the conversion process.","Configuring them accordingly can improve the output quality by a lot, as well as reduce artifacting, so we highly recommend learning them.","There are some of them that are either obsolete or not important. So if a setting is not explained here, you can ignore it."]},{"i":"section-1","l":"‎"},{"i":"section-2","l":"‎"},{"l":"Also known as Pitch, it adjusts the tone of voice.","p":["Negative values lower the tone (e.g -2).","Positive ones raise it (e.g 5).","You can use decimals if necessary (e.g -4.3).","You'll usually have to modify this for the pitch to sound perfect. Modify it until it matches the tone of the model."]},{"i":"section-4","l":"‎"},{"l":"Also known as Index Rate, it determines the level of influence of model's .INDEX file:","p":["Higher values will apply more of the .INDEX's characteristics.","Lowering it can reduce artifacting.","Remember, if the dataset had other sounds like background noise, there will be noise in the .INDEX too."]},{"i":"section-5","l":"‎"},{"l":"They're the algorithms for converting the vocals.","p":["*FCPE*","*Mangio-Crepe / Crepe (In Applio)*","*RMVPE*","A Robust Model for Vocal Pitch Estimation in Polyphonic","As the majority of them are obsolete, we'll focus on the 3 best ones: RMVPE, Mangio-Crepe& FCPE.","Better with harmonic-rich voices / fuller voices","Better with soft, whspery or voices with feminine timbres","Decent precision","Each one works in its own way, and has its pros & cons.","Fast","Fast Context-base Pitch Estimator","If you have really clean audio use this over RMVPE","Inference only. Allows you to set the maximum/minimum frequency, to reduce small distortions. Recommended for advanced users.","It determines the time it takes the voice to hit a note","It's crepe, but you can adjust its hop_length","Lowering it too much might lead to voice cracks so it's recommended to not lower it below 64.","Might be better with human softness","RMVPE+:","Should be your go-to algorithm, due to its convenience","Some forks include RMVPE_GPU& RMVPE+. Same algorithm, but with a modification:","The lower the value, the more detailed results you'll get, but will take longer to process","They also work the same for training models.","Training only. Uses more GPU power, making you train faster.","Useful for Realtime. If you have poor performance in Realtime, use this over RMVPE","Useful when the audio/model performs drastic note shifts","Usually sounds a little harsh","Very fast, less precise and prone to noise"]},{"i":"section-11","l":"‎"},{"l":"Also known as Protection, they suppress breath sounds:","p":["Decrease the value to remove more breath sounds, as they cause some artifacting.","A value of 0.5 disables this feature.‎","Be careful, lowering it too much will make it voice sound \"inhumane\" & suppress part of the words."]},{"i":"section-12","l":"‎"},{"l":"Also known as Remix Mix Rate, controls the loudness of the output:","p":["The closer to 0, the more the output will match the loudness of the input audio.","The closer to 1, the more it will match the loudness of the dataset the model was trained on.","Basically, leave it at 0 if you want the audio to try to keep its original volume."]},{"i":"section-13","l":"‎"},{"l":"Gives a faster inference & more consistent output volume:","p":["In RVC sometimes there's an error where the volume of the output lowers in some parts.","To prevent this, Split Audio divides the audio & infers them one by one. Then unites them at the end.","Doing it this way is faster too."]},{"i":"section-14","l":"‎"}],[{"l":"Deiteris' W Okada Fork","p":["Last update: July 30, 2025","W-Okada is a realtime voice changer that uses RVC for its conversion.","There are two versions of this realtime voice changer, the offical original made by Wok, and the Deiteris fork made by Deiteris. Note that those 2 links are just for reference to the Source Code Github Repositories of both projects, you should instead follow the guide below.","This guide will be about the Wokada Deiteris fork since it has better preformance and quality compared to the Original Wokada.","RVC does NOT mean realtime voice changer. RVC means Retrieval-based-Voice-Conversion."]},{"l":"Is The W-Okada Deiteris Fork Safe?","p":["RVC Models are PyTorch Models, a Python library used for AI. PyTorch uses serialization via Pythons' Pickle Module, converting the model to a file. Since pickle can execute arbitrary code when loading a model, it could be theoretically used for malware, but Wokada Deiteris Fork has a built-in feature to prevent code execution along the model. Also, HuggingFace has a Security Scanner which scans for any unsafe pickle exploits and uses also ClamAV for scanning dangerous files.","‎"]},{"l":"Pros & Cons"},{"l":"*Learn more*","p":["Currently stable","Good Performance","Has great support for Nvidia, AMD, Intel, Mac, Linux, Windows","Uses a Web User Interface, meaning it can be run on the Cloud","Uses FP16 Inference by default, and let's you choose to use FP32 for better quality/precision","Uses a Web User Interface, having issues on some browsers, and bugs with renaming or deleting models on it","Doesn't have an active development recently","Has Cut Off Issues Using an Extra superior to 2.7","Doesn't let you choose the embedder, using only RVC models trained on contentvec (the majority)"]},{"i":"section","l":"‎"},{"l":"System & Hardware Requirements","p":["Windows 10 or Later","macOS 12 Monterey or later. With Apple Silicon or Intel CPU","Any Linux Distro","and","At least 6GB of RAM","At least 6GB of free disk storage"]},{"l":"For GPU-conversion","p":["TLDR: Make sure you have Nvidia RTX 20xx or AMD Radeon RX 5xxx or better. GTX 10xx or RX 580 will also work, but may run into issues with games and higher delay. If you have an iGPU (mostly AMD Radeon Graphics or Vega) use online hosted alternative instead.","Long answer:","Minimum:","An integrated graphics card: AMD Radeon Vega 7 (with AMD Ryzen 5 5600G) or later; with 2GB VRAM (in FP32 mode), ~ 1GB VRAM (in FP16 mode, if supported). But this is NOT recommended at all and we will most likely not recommend you to download the realtime voice changer with iGPUs.","A dedicated graphics card: Nvidia GeForce GTX 900 Series or later, or AMD Radeon RX 400 series or later, or Intel Arc A300 series or later.","Recommended:","A dedicated graphics card Nvidia GeForce RTX 20 Series or later, or AMD Radeon RX 5000 series or later, or Intel Arc A500 series or later."]},{"l":"For CPU-conversion","p":["TLDR: don't bother. You can't run games, discord usage might be the only thing that will work decently, but you might potentially damage your CPU. People with no GPU usually have old CPU's, so delay will be high too. Not worth it.","Minimum:","Intel Core i5-4690K or AMD FX-6300.","Recommended:","Intel Core i5-10400F or AMD Ryzen 5 1600X.","If you plan on playing games at the same, do not use CPU-conversion. With CPU, the delay will be massive and your PC will not run smoothly at all. If you have a higher-end CPU you can make it work, but those that have higher end CPUs most likely also have higher end GPUs, so you should be using your GPU if possible."]},{"l":"Online Alternatives [Colab/Kaggle]"},{"l":"Kaggle","p":["It's free, but you will need a phone number verification.","Read the Tutorial HERE"]},{"l":"Google Colab","p":["You need the Google Colab Paid Tier to run this, as it uses a Web User Interface, else you could risk getting disconnected or getting banned off Colab.","Read the tutorial HERE"]},{"l":"Virtual Audio Cable"},{"l":"A Virtual Audio Cable (VAC) is what you need to use the realtime voice changer on Discord & Games.","p":["A VAC (Virtual Audio Cable) makes a fake audio device, used to re-route the audio of different programs.","In Wokada Deiteris Fork context, it's used to get the output of Wokada Deiteris Fork as the input in other programs such as Discord.","Download this: VAC Lite (Virtual-Audio-Cable by Muzychenko).(Be sure to not use any toher vac like VB Audio Cable.)","Run setup64, not 64a, after extracting the zip to a new folder","After installing the VAC Lite, it changes your default audio system. Click Yes when it asks you to open the audio device settings (or press WIN+R, type \"mmsys.cpl\" if you closed it already), and change your Recording and Playback devices back to your usual devices. Same for communications device aswell (right click -> set as default communication device)","Download either: Blackhole Virtual Audio Cable or VB-Audio","For Debian / Ubuntu-based Systems (Ubuntu, Mint, Pop!_ OS), run in the terminal:","For Fedora / RHEL-based Systems (CentOS, Rocky Linux), run in the terminal:","For Arch / Arch-based Systems (Endeavour, Manjaro Linux), run in the terminal:"]},{"l":"Windows","p":["Download based on your GPU. You don't know what GPU you have? Open Task Manager > Performance tab and check for your GPU0 and GPU1 names. Prioritize the Nvidia one if you have one, else use the other."]},{"i":"#","p":["Use Online Hosted if you have an integrated GPU (AMD Radeon Graphics ; AMD Radeon Vega ; Intel UHD) and if you do not have a GPU at all"]},{"l":"Download NVIDIA on Windows","p":["The latest version as of December 7th 2024 is: nvidia-b2332 (click here to download)","If you have a GTX 700 card or below, use AMD/Intel version instead."]},{"l":"Download NVIDIA RTX 5000-series on Windows","p":["NVIDIA RTX-5000 series, the newest release of GPU's, require a separate download. You do not need it if you have an older GPU, follow the normal Nvidia link in that case. nvidia-5000-Series (click here to download)","Download all 3 files, then extract the .zip file, it will automatically extract ALL 3 FILES into one. Then open the MMVCServerSIO folder and run MMVCServerSIO.exe(or called MMVCServerSIO if you don't have extensions activated)."]},{"l":"Download AMD, INTEL and CPU on Windows","p":["The latest version as of December 7th 2024 is: dml-b2332 (click here to download)","Intel UHD Graphics do NOT work at this point in time. Use Online Alternative."]},{"l":"Opening on Windows","p":["First Make sure you have 7zip or WinRAR for extracting / unzipping.","After the download, you extract the zip file. You open the folders until you see an exe application called MMVCServerSIO and run that.","If nothing opens, then open a browser and type in http://127.0.0.1:18888/. This is a local URL, it runs on the WebUI."]},{"l":"Mac"},{"l":"Download Mac Silicon","p":["The latest version as of December 7th 2024 is: arm-b2332 (click here to download)"]},{"l":"Download Mac Intel","p":["The latest version as of December 7th 2024 is: macos-amd-b2332 (click here to download)"]},{"l":"Opening on Mac","p":["Double click the voice-changer-macos-arm64-cpu.tar.gz file. The realtime voice changer will unpack and the MMVCServerSIO folder will appear.","Open the extracted MMVCServerSIO folder.","Double-click MMVCServerSIO to run the realtime voice changer.","You do not get a popup notification for this, so if it does not open or says \"Pytorch is damaged\", do the following:","Open the Terminal","Run the following command: xattr -dr com.apple.quarantine PUT IN THE PATH TO YOUR MMVCServerSIO FOLDER HERE For example, if you extracted the realtime voice changer to your desktop, the command may look as follows: xattr -dr com.apple.quarantine ~/Desktop/MMVCServerSIO","Now, open the extracted MMVCServerSIO folder and run MMVCServerSIO to run the realtime voice changer.","If nothing opens, then open a browser and type in http://127.0.0.1:18888/. This is a local URL, it runs on the WebUI."]},{"l":"Linux","p":["Installation of CUDA Toolkit or AMD HIP SDK is NOT REQUIRED. All other necessary libraries are bundled with the application."]},{"l":"Download on Nvidia on Linux","p":["you need to download both these files:","https://github.com/deiteris/voice-changer/releases/download/b2332/voice-changer-linux-amd64-cuda.tar.gz.aa","https://github.com/deiteris/voice-changer/releases/download/b2332/voice-changer-linux-amd64-cuda.tar.gz.ab"]},{"l":"Download on AMD on Linux","p":["you need to download all these files:","https://github.com/deiteris/voice-changer/releases/download/b2332/voice-changer-linux-amd64-rocm.tar.gz.aa","https://github.com/deiteris/voice-changer/releases/download/b2332/voice-changer-linux-amd64-rocm.tar.gz.ab","https://github.com/deiteris/voice-changer/releases/download/b2332/voice-changer-linux-amd64-rocm.tar.gz.ac"]},{"l":"Download on CPU on Linux","p":["you need only this file:","https://github.com/deiteris/voice-changer/releases/download/b2332/voice-changer-linux-amd64-cpu.tar.gz"]},{"l":"Opening on Linux","p":["I'm not sure about the capabilities of UI tar archive extractors, but you can extract these archive parts with the following command that will merge them and extract: cat voice-changer-linux-amd64-cuda.tar.gz.* | tar xzf -, change cuda to rocm or cpu depending on your PC GPU.","After you extract the files using the command above, a new folder called MMVCServerSIO will appear.","Open a Terminal and navigate into that folder:","You may need to make the application executable. Run this command just in case:","Now, run the realtime voice changer:","After the server finishes loading in your terminal, it will not open a window on its own. Open a web browser and go to http://127.0.0.1:18888/ to access the user interface."]},{"l":"Opening on Multi-PC Setups","p":["This is only for the people that have 2 PCs, and want to use 1 PC for Gaming, the other only for Wokada Deiteris Fork.","Create a file named .env on the same folder where MMVCServerSIO.exe is located. Open it up with a notepad, copy paste the settings from the GitHub link.","After that, you create another file with the file extension ending .bat, open it up with a notepad, copy paste what is needed in there again from the GitHub link.","Now run the bat file. After it starts, you should be able to open the link. For example, if you specified HOST=192.168.0.1 and ALLOWED_ORIGINS='[https://192.168.0.1:18888]'), you should be able to open https://192.168.0.1:18888 in your browser and use the realtime voice changer UI from other machines in your local network."]},{"l":"Voice Models"},{"l":"Managing Models"},{"l":"Adding Models"},{"i":"#","p":["Click on Edit on the small blue square located around the the top left side","Pick any slot you want, click upload","Only RVC models will work. If you have a gpt-sovits one or any other, they will not work.","Select Type: RVC, then select file on the Model slot and upload your .pth file.","No need for an Index file, but you can upload it. This controls the accent of the voice model."]},{"l":"Renaming Models","p":["Attempting to rename a model directly within the Web User Interface will cause the program to crash. This is a known bug. Use one of the two methods below to safely rename your models.","Method 1: Re-uploading the Model","This is the simplest method.","Find the model's .pth file on your computer.","Rename the file to your desired new name.","In the voice changer UI, click Edit, select the slot of the model you want to rename, and click upload.","Re-upload the renamed .pth file to the same slot. This will overwrite the old model and update its name.","Method 2: Editing the Configuration File","This method doesn't require re-uploading.","Navigate to your MMVCServerSIO folder.","Inside, open the model_dir folder. You will see several numbered folders, each corresponding to a model slot in the UI.","Open the folder for the slot number you want to rename.","Inside this folder, you will find a params.json configuration file. Open this file with a text editor like Notepad.","Look for the name: field in the file. Change the text in the quotes to your desired new model name.","Save the .json file. The name will be updated in the voice changer UI."]},{"l":"Deleting Models","p":["If you wish to delete a model, you can simply overwrite the slot with a new model by following the steps in the Adding Models section. If you want to completely empty a slot, navigate to the MMVCServerSIO/model_dir folder, open the folder of the slot number you want to delete, and delete all the files inside it."]},{"l":"Merging Models (Merge Lab)","p":["The Merge Lab allows you to combine multiple RVC V2 voice models (.pth Weights only, not indexs too) into a single, new hybrid model. This is useful for creating unique voices.","Open Merge Lab: Scroll down in the user interface and click on the Merge Lab button.","Select Model Type: From the Type dropdown menu, choose the type of models you wish to merge. Only models that share the same sample rate and type (e.g., \"pyTorchRVCv2, 32000Hz, 768\" which are all RVC v2 models with the 32kHz Sample Rate, or \"pyTorchRVC, 40000Hz, 256\" which are all RVC v1 models with the 40kHz Sample Rate) will be shown and can be merged together.","Adjust Weights: Use the sliders next to each model's name to set its \"weight\" (RVC models are PyTorch files, the .pth is the weight containing the voice) or influence in the merged model. The numbers (from 0 to 100) represent the percentage of each voice in the mix.","Merge and Download: Once you have set the desired proportions, click the Merge button. Your browser will automatically download the new, merged.pth model file, which you can rename to whatever you want.","The merged model is not automatically added to your model list. You must upload it to an empty slot yourself by following the steps in the Adding Models section.","You can't merge indexs(in rvc context, the trained accent of the voice). Only the .pth actual voice file."]},{"l":"Audio Setup"},{"l":"Discord & Games","p":["On the realtime voice changer app wokada, you select:","Input: Your microphone","Output: Virtual Audio Cable","Monitor (if you wish to hear the realtime voice changer on your headphones aswell): Your headphones","On discord and games, you select:","Input: Virtual Audio Cable","Output: Your headphones"]},{"l":"Client and Server Setup","p":["Audio: CLIENT","Uses MME (normal audio processed through windows. You use this automatically with every application)","You can use the boxes echo, sup1, sup2 using this","Audio: SERVER","Use S.R. 48000","I recommend using [Windows WASAPI] on all prefixes for less delay, because this uses your audio devices (e.g. microphone) directly, before processing through windows.","Both Input and Output has to be the same (Windows WASAPI), you can't use MME for input and then Windows WASAPI for Output. You may also use ASIO.","You can not use the in-built noise suppressions in this mode","ASIO > WASAPI > MME as a general thumbrule (this also affects delay)","Sometimes Client does not work, then use SERVER with prefix \"MME\" or \"Windows WASAPI\". You can not use the in-built noise suppression and echo fix if you use SERVER."]},{"l":"Settings Explained","p":["PASSTHRU button: Sends your actual voice and not the realtime voice changer through the virtual audio cable. You want this to be GLOWING GREEN or GREY (grey for dark mode users) for the realtime voice changer to work.","F0 det: Pitch extraction algorithm. Both RMVPE (for the best precision and robustness) and FCPE (for less precision & robustness but lower delay) are good options.","Chunk: Controls the delay (lower number means less delay, but please check out the recommended settings for what your GPU is capable of).","Extra: Controls voice model quality. 2.7s is the max, anything above is not worth it and can cause issues for no benefit."]},{"l":"VOL:","p":["in: This raises the microphone volume before it goes into the realtime voice changer (Recommended to leave it on the default or if needed, not to go too high, else it increases background noise and makes the voice sound worse).","OUT: Raising realtime voice changer volume on the output.","MON: Increases volume of your headphones that you set on \"mon\" if you selected to hear yourself with the realtime voice changer.","Pitch: This is the pitch. Going into negative will make it lower pitch, going higher will make it higher pitch. If you have a male voice using a female voice, aim for 10 - 14, this depends on your voice, try around those numbers until you find a sweet spot.","Formant Shift: Alters harmonic frequencies and changes the voice timbre without affecting the pitch","Index: This controls the accent of the voice model. In most cases, using Index on Realtime Voice Changer can add realism if you speak the language the model was trained in. If you have a heavy foreign accent, you may use this at a low rate. Beware, this increases CPU usage","In. Sens: microphone threshold, increasing this will cause less background noise to get picked up if it's a problem","Sup2: Noise suppression on your microphone.","Sup1: Noise suppression but weaker, not recommended to use this at all, because it barely has any impact whilst reportedly, making the voice inconsistent","Echo: if you experience echo issues despite having sup2, In. Sens to the right and having lowered your windows system value, then this will help you as a last resort"]},{"l":"Settings"},{"l":"Advanced Settings","p":["Protocol: rest (Use SIO if you want less delay but if you encounter any issues with SIO switch back to rest. Rest has slightly more delay than SIO)","Crossfade length: Controls how smoothly the AI stitches different processed parts \"chunks\" of your voice back together. 0.1 or 0.15 (0.1 for fastest voice, 0.15 for improved quality but increases delay by 50 ms)","SilenceFront: Reduce GPU usage when idle. This only reduces GPU resources when you're not talking or making sounds","Force FP32 mode: on (THIS IS OFF BY DEFAULT! Turning this on improves stability. Increases VRAM usage by 200 MB)","Disable JIT compilation: off for faster loading speed of the program, on for slightly better performance (10-15 ms) for Nvidia only)","Convert to ONNX: Reduces delay and slightly reduces gpu usage. Enabling this increases CPU usage by around 5-10%. Reduces the quality of the voice a bit. If you decide to enable this, pair it with rmvpe_onnx for even less delay","Protect: Reduces the occurrence of robotic sibilants and robotic breathing, but also reduces the effect of the index file. Lower values increase this protection, higher values decrease it. The default value is 0.5, which means that the protection is disabled, reduce this value to 0.33 to enable it"]},{"l":"Finding my own settings for Chunk","p":["First start with 500 ms, check what number your perf is and go closer to that number but not lower.","Example: if your perf is 200, go down to 250 with your chunk. Chunk affects perf value, and Extra as well.","If your perf value is green, your selected chunk is stable. You can experiment and go down in chunk for less delay, or increase extra for more quality (would not recommend to go above 2.7s extra. Anything above uses more resource for no clear benefit).","If your perf value is yellow, your selected chunk is enough, but audio may be unstable if you run other processes at the same time. Operation in this range will also incur high GPU usage. Increasing Chunk size or reducing Extra is recommended.","If your perf value is red, the realtime voice changer is unstable. Increase chunk size or reduce Extra."]},{"l":"Known working settings for Chunk and Extra","p":["100 - 120 + 2.7s extra","110 - 130 ms chunk + 2.7s extra","128 ms + 2.7s extra","128ms + 2.7s extra","140 - 180 ms chunk + 2.7s extra","140 - 200ms + 2.0s extra","200 ms chunk + 2.0s extra","250 ms chunk + 1.0s extra","256 ms + 2.7s extra","30 - 60 ms chunk + 2.7s extra","50 - 80 ms chunk + 2.7s extra","50 - 90 ms chunk + 2.7s extra","500 ms chunk + 0.6s extra","5xxx cards","5xxx XT cards","60 - 80 ms + 2.7s extra","60 - 90 ms chunk + 2.7s extra","600 ms + 0.6s extra","6xxx cards","6xxx XT cards","70 - 100 ms + 2.7s extra","700 ms + 1.0s extra","7xxx cards","7xxx XT cards","80 - 120 ms + 2.7s extra","AMD Radeon RX Vega 10 (with Ryzen 7 3700U)","AMD Radeon RX Vega 8 (with Ryzen 3 3200G)","AMD Radeon(TM) Graphics (with Ryzen 7 5800H)","bugged 256 ms + 2.7s extra","fcpe ; for chunk check the perf number and add 50 to it ; 1.0s extra","fcpe ; for chunk check the perf number and add 50 to it ; 2.7s extra","fcpe + 230ms + 2.7s extra","GTX 10xx-series","GTX 16xx-series","GTX 900-series","If you are playing a video game with the realtime voice changer, you will have to increase the chunk higher than what you usually can handle. This is because the game runs on GPU and the realtime voice changer aswell. The game will always take higher priority by default, so the listed settings are safe options that should run with most games. If you run into issues, you will need to lower quality and limit your FPS, or increase chunk. It is best to first tweak your game's settings first","It is recommended to go up to Finding my own settings after you are comfortable with the program","Mac M1","Mac M1 Air","Mac M2","Mac M2 Air","MX 330","perf number + 100 ms chunk","perf number + 40 ms chunk","perf number + 50 ms chunk","perf number + 60 ms chunk","perf number + 80 ms chunk","rmvpe_onnx + 260 ms + 0.6s extra","rmvpe_onnx + 650ms + 1.0s extra","RTX xx50 (e.g. 3050)","RTX xx60 (e.g. 3060)","RTX xx60 Ti (e.g. 3060 Ti)","RTX xx70 (e.g. 3070)","RTX xx70 Ti (e.g. 3070 Ti)","RTX xx80 (e.g. 3080)","RTX xx80 Ti (e.g.3080 Ti)","RTX xx90 (e.g. 3090)","RX 560","RX 570","RX 580","RX 6600M","Ryzen 7 5800x"]},{"l":"Extras"},{"l":"Information","p":["This fork is a lot better for AMD GPU's compared to the original w-okada. On the original it requires converting models to onnx models which is annoying, requires more CPU and GPU resources, has a lot more delay and other little inconveniences/bugs.","Example: AMD RX 6650 XT lowest latency is 298 ms chunk on original w-okada. On this fork lowest latency is around 60 - 80 ms chunk","Deiteris' fork is better for NVIDIA users who normally use the prebuilt w-okada version, because this version uses GPU accelerated extra compared to the original which uses CPU.","For the RTX GPUs the delay performance differences are minimal, but quality performance is better. For older cards like GTX or MX, this fork performs better in all aspects.","Example: NVIDIA RTX 3070 on prebuilt w-okada reaches 170 - 213 ms chunk latency. On manually set up environment of w-okada reaches 42 ms chunk latency. On this fork it can reach 30 - 38 ms chunk latency, depending on the extra set. Keep in mind these are settings tested to the max, without a video game or intense operations running in the background"]},{"l":"Reduce more Delay (Windows Only)"},{"l":"Prerequisite: Match Sample Rates (for both WASAPI & ASIO)","p":["Afterwards, download and install the FlexASIO GUI: FlexASIO GUI Download","Assuming you completed the prerequisite step, you can now select the correct inputs and outputs in the voice changer as follows:","AUDIO: SERVER","Backend: Windows WASAPI","Buffer Size:✅ Set to 256","Click SAVE TO DEFAULT FLEXASIO.TOML. Do not forget this step. You can close the GUI afterwards.","Download and run the installer from here: FlexASIO Download","Ensure both options for Exclusive Mode are activated.","First, you need the .NET Desktop runtime. Download and install it from here: .NET 6.x Desktop runtime","Go to the last tab, Advanced, and set the sample rate to 48000 Hz.","Having the input latency at 0.0 can make your microphone crackle. Using 0.1 often works fine. If you experience crackles, experiment with this value (e.g., 0.12, 0.15) until it stops. The lower you can go, the better. If you don't want to experiment, you can keep it at 0.2.","I would recommend using WASAPI first if you are a normal user, as ASIO is more complex to set up.","If you don't know how to open your sound devices, press WIN+R, type \" mmsys.cpl\", then hit enter.","In the Advanced tab, adjust the sample rate to match your microphone: 48000 Hz.","In the voice changer app:","Input Device: Select your Microphone.","Input: Your Virtual Audio Cable (e.g., Line 1 Output)","Input:[WINDOWS WASAPI] Your Microphone","Latency:✅ Set Input Latency: 0.2; ✅ Set Output Latency: 0.2","Like WASAPI, ASIO accesses your audio devices directly, bypassing multiple layers within the Windows audio subsystem that \"MME\" (the default driver) has to go through. It has a lower algorithmic delay and can reduce total delay by 50-80ms.","Navigate to the Recording tab, right-click on your microphone, and select Properties.","Now, go to the Playback tab. Right-click on your virtual audio cable (e.g., Line 1) and go to Properties.","Output Device: Select your Virtual Audio Cable (e.g., Line 1).","Output: Your Headphones/Speakers","Output:[WINDOWS WASAPI] Your Virtual Audio Cable (e.g., Line 1)","Output:✅ Set: ;✅ AutoConvert","Run FlexASIO GUI. If it doesn't open, you missed installing the .NET runtime from the previous step. Copy the following settings:","S.R.: Match the sample rate you chose above, which should be 48000.","Select AUDIO: Server","Select S.R.: 48000","Select the input and output from ASIO. You can select \"ALL\" in the first column to filter for ASIO devices to make it easier.","The Deiteris Fork works with ASIO, while some older versions of the original w-okada do not.","Then, on your game or Discord, you select:","This first step is mandatory for both methods. You must select the same sample rate for your microphone and the virtual audio cable before proceeding.","WASAPI accesses your audio devices directly, while the driver that you use by default (which is \"MME\") goes through multiple layers within the Windows audio subsystem, causing more delay. This will in total cut down 50-80ms delay.","With the sample rates matched, you can now proceed to configure either WASAPI or ASIO.","You cannot use the noise suppression ( sup1, sup2) or echo functions in SERVER mode.","You did not match the sample rate of your virtual audio cable to your microphone. Return to the prerequisite step and ensure both are set to 48000 Hz.","You did not match the sample rate of your virtual audio cable to your microphone. Return to the prerequisite step and ensure both are set to the same value (48000 Hz)."]},{"l":"Models to try","p":["You will need to connect your account to weights.com to be able to download these models","Click on the 3 dots (...) on weights.com models, then Download model. You will need an account","Female:","Gawr Gura: Hugging Face Link/ Weights Link","Male:","Bob Ross voice made by dieseldog34","Markiplier voice made by hobqueer"]},{"l":"Help"},{"l":"How to fix \"Failed to download or verify\"","p":["After you start the program for the first time and it finished downloading files, but you have slow/unstable internet connection it might say Failed to download or verify: ... followed by \"Press Enter to continue\" at the end, then the pretrain download failed. To fix it, you can either:","Retry with a better connection later.","Go to the \"pretrain\" folder in the MMVCServerSIO folder.","Delete everything inside it if there is anything.","Download the Zipped Version of the Pretrained folder","Extract the pretrain.zip, be sure the pretrain folder contains only the files, not a pretrain folder inside another pretrain folder with the files.","Then run MMVCServerSIO.exe again, this time it should work."]},{"l":"Crackle Fix","p":["Open Task Manager > Details","Right click audiodg.exe and set priority to High","Right click audiodg.exe again > set affinity > uncheck everything except CPU 2 (only ✅ CPU 2, turn off the rest)","With a program called ProcessLasso you can automate this to always be active, since Windows resets these sometimes. Or you can open up CMD/Powershell (or make a bat file) and type in:","powershell ForEach($PROCESS in GET-PROCESS audiodg) { $PROCESS.ProcessorAffinity=4; $PROCESS.PriorityClass='High' }"]},{"l":"Discord Crackle Fix","p":["Make sure to do the Crackle Fixes in this step before doing this to see if it fixes your issue","If the voice sounds fine in the app AND it sounds fine in games, but ONLY sounds weird on discord, then:","Turn off Echo Cancellation","Turn off Noise Suppression (sometimes causes issues, maybe not. Check for yourself)"]},{"l":"GPU Idling","p":["Sometimes your GPU will start idling after the program is in the background for a while and affect performance.","In the folder where w-okada is located there should be a .bat file called force_gpu_clocks.bat, run that and it should fix your gpu idling.","Once you no longer want your gpu clock speed to be forced anymore you can run reset_gpu_clocks.bat."]},{"l":"FAQ"},{"l":"Why does it run in a browser and not it's own window?","p":["Because it uses a Web User Interface (WebUI) coded in JavaScript & TypeScript, the majority of (Open Source) AI programs are designed to run on the browser (even tho usually using things like Gradio) since it can be used both on cloud and locally. The original wokada also ran on a WebUI, just that it made it's own window"]},{"l":"What browser should I use?","p":["It's better you try and test, some people had issues on Chrome, some others on Firefox, it might depend on the settings you use and also Java/Type Script having issues. The browser that usually is reported by most people to have issues is OperaGX, which is why we don't suggest it much"]},{"l":"Why are most YouTube (Video) Tutorials old? Is there going to be an updated one?","p":["YouTube Tutorials take way more time to make, and get outdated easily in this case, as AI progresses fast and continues to change in better, with more different settings and versions. Written guides are easier to update, since you don't have to remake an entire video. It's unknown if we will ever release a video since they easily get outdated, but if we will, it will be linked inside of this guide."]},{"l":"Do I need an extremely expensive mic for good quality?","p":["We had a conversation about this in https://discord.com/channels/1159260121998827560/1159290161683767298/1352325982689951765& https://discord.com/channels/1159260121998827560/1159290161683767298/1356265862704926907, RVC works by downsampling your audio voice to 16khz because f0 estimators only works at that sample rate, after that the model outputs the results using it's original sample rate (without any upscaling). So there won't be the need of having a super extremely expensive, a decent one should do the job."]},{"l":"Are there unique Voice Models?","p":["RVC Voice Models need to be trained on something, so the models themselves can't be unique, but you can use the Merge Lab to create a new unique merged model."]},{"i":"section-3","l":"‎"}],[{"l":"Vonovox","p":["Last update: July 30, 2025","Vonovox is a realtime voice changer that uses RVC for its conversion.","Vonovox was developed by dr87.","RVC does NOT mean realtime voice changer. RVC means Retrieval-based-Voice-Conversion."]},{"l":"Is Vonovox Safe?","p":["RVC Models are PyTorch Models, a Python library used for AI. PyTorch uses serialization via Pythons' Pickle Module, converting the model to a file. Since pickle can execute arbitrary code when loading a model, it could be theoretically used for malware, but Vonovox has a built-in feature to prevent code execution along the model. Also, HuggingFace has a Security Scanner which scans for any unsafe pickle exploits and uses also ClamAV for scanning dangerous files.","‎"]},{"l":"Pros & Cons"},{"l":"*Learn more*","p":["Has an active development","Good Performance","Currently stable","It doesn't use a Web User Interface, meaning that it is less prone to errors and opens on it's own window","Easily reduces delay on Windows via facilitating the WASAPI/ASIO Backend process","Lets you choose the embedder, including spin","Uses TF32 Inference by default, which is more precise than FP16, and has very very slightly less precision/quality but better performance compared to FP32","Fixed 2.7+ Extra Time Cut Off Issues","Extra Effects, such as \"Noise Gate\"","Not Open Source (at the moment, but the dev might be working on an Open Source version)","Supports only Nvidia GPUs on Windows","It doesn't use a Web User Interface, meaning that it can't be run on the Cloud","Many Effects are Premium (paid), such as \"Low Quality Mic\""]},{"i":"section","l":"‎"},{"l":"System & Hardware Requirements","p":["Windows 10 or Later","and","At least 6GB of RAM","At least 6GB of free disk storage"]},{"l":"For GPU-conversion","p":["TLDR: Make sure you have Nvidia RTX 20xx better. GTX 10xx or RX 900 will also work, but may run into issues with games and higher delay. If you have an iGPU (mostly AMD Radeon Graphics or Vega) use Wokada Deiteris Fork Cloud instead.","Long answer:","Minimum:","A dedicated graphics card: Nvidia GeForce GTX 900 Series or later.","Recommended:","A dedicated graphics card Nvidia GeForce RTX 20XX Series or later."]},{"l":"Virtual Audio Cable"},{"l":"A Virtual Audio Cable (VAC) is what you need to use the realtime voice changer on Discord & Games.","p":["A VAC (Virtual Audio Cable) makes a fake audio device, used to re-route the audio of different programs.","In Wokada Deiteris Fork context, it's used to get the output of Wokada Deiteris Fork as the input in other programs such as Discord.","Download this: VAC Lite (Virtual-Audio-Cable by Muzychenko)","Run setup64, not 64a, after extracting the zip to a new folder","After installing the Virtual Cable, it changes your default audio system. Click Yes when it asks you to open the audio device settings (or press WIN+R, type \"mmsys.cpl\" if you closed it already), and change your Recording and Playback devices back to your usual devices. Same for communications device aswell (right click -> set as default communication device)"]},{"l":"Windows","p":["Make sure you have a Nvidia and a good enough one to run Vonovox. You don't know what GPU you have? Open Task Manager > Performance tab and check for your GPU0 and GPU1 names."]},{"i":"#","p":["Use Online Hosted if you have an integrated GPU (AMD Radeon Graphics ; AMD Radeon Vega ; Intel UHD) and if you do not have a GPU at all"]},{"l":"Download NVIDIA on Windows","p":["Make sure you have the Microsoft Visual C++ Redistributable Package, if you don't already.","Go to Vonovox's Github Repository and download the Latest Stable Release Source Code.","You could also optionally get access to Beta/ Early Access versions via Becoming a Vonovox Supporter (and also gaining Premium Effects) or checking for any Free Versions in the Vonovox Official Discord Server. They may have bugs, fixes or settings/options not explained in the guide yet.","If you have a GTX 800 card or below you can't use Vonovox."]},{"l":"Opening on Windows","p":["First Make sure you have 7zip or WinRAR for extracting / unzipping.","After the download extract the zip file. Open the folders until you see an .bat file called setup.bat and run that.","Vonovox will start downloading everything it needs to run. Be patient as it can take up to 5 minutes to download everything it needs.","Once it's done downloading everything it will display Setup complete! in the command line. You can now go ahead and run start.bat."]},{"l":"Opening on Multi-GPU Systems","p":["This is ONLY For users with 2 GPUs in the same system, you must do the following:","Open NVIDIA Control Panel","Go to Manage 3D Settings > Program Settings Tab","Add python.exe from the Vonovox runtime folder (runtime\\python.exe)","Set both settings \"CUDA - GPUs\" and \"OpenGL rendering GPU\" to the GPU you want to use for conversion","This will hide the other GPU from being used by the application which is required"]},{"l":"Voice Models"},{"l":"Adding Models"},{"i":"#","p":["Click on Select .pth file on the blue square located around the the top","Only RVC models will work. If you have a gpt-sovits one or any other, they will not work.","Select your .pth file and click upload.","No need for an Index file."]},{"l":"Changing Models","p":["If you wish to use a different a model, you can overwrite the model you are currently using with a new model."]},{"l":"Audio Setup"},{"l":"Discord & Games","p":["In Vonovox select:","Input: Your microphone","Output: Virtual Cable or your headphones if you wish to hear the model first","On discord and games, you select:","Input: Virtual Cable","Output: Your headphones"]},{"l":"Settings"},{"l":"Current Model Settings:","p":["Embedder: Select between contentvec or spin trained models. Most current models are trained on contentvec. Make sure you read the model's description to find out what embedder it uses. Spin has kinda better breaths, more robust to noise, has some training related differences, but it's less used and newer.","Pitch: This is the pitch. Going into negative will make it lower pitch (masculine), going higher will make it higher pitch (feminine). If you have a male voice using a female voice, aim for 10 - 14, this depends on your voice, try around those numbers until you find a sweet spot.","Formant: Alters harmonic frequencies and changes the voice timbre without affecting the pitch (AKA Formant Shift)."]},{"l":"Audio Device Settings:","p":["Audio Backend: Use WASAPI unless you have an ASIO interface and know what you're doing (advanced users)","Exclusive Mode: WASAPI exclusive mode. It has much lower latency but the issue is if you don't lock your gpu clocks with something like msi afterburner, it will pop nonstop , because it needs something like a ~ 45-50ms gpu delay max to function (advanced users)","Sample Rate: Only 48000Hz is available. This is only the outgoing sample rate that matches your VAC line - It is compatible with 32000, 40000, or 48000 models","F0 det: Pitch extraction algorithm. Both RMVPE (for the best precision and robustness) and FCPE (for less precision & robustness but lower delay) are good options.","Pitch Smoothing Factor: Pitch smoothing will dampen pitch changes. It still follows the exact curve of the f0 predictor allowing it to maintain 100% accuracy, just to a lower magnitude. This allows normal speaking voices to have better stability, since sometimes f0 can be over aggressive and cause pitch wobble on minor pitch fluctuations.","Output volume: Controls how loud the output volume is."]},{"l":"Noise Reduction:","p":["Noise reduction algorithms are not compatible with singing or whispering. Turn them off if you need to sing or whisper.","RNNoise Reduction: Greatly filters input background noise for very minimum latency. This can mitigate the chances of Vonovox trying to infer on noise.","VAD Noise Reduction: Completely mutes the output when speech is not detected. When speech is detected, it uses a 400ms release window. It is also much better at filtering breathe noises than RNNoise.","AP-BWE 48k Upscaler: This is an upscaler that extends the bandwidth of speech by adding missing frequency information up to 48k."]},{"l":"Voice Settings:","p":["Block Size: Critical setting. The optimal block size is the lowest you can get without audio being choppy. Listen to your output. This is GPU dependent, the more powerful the gpu, the lower the block size you can use. However the optimizations I made allow much smaller block sizes to work on lower end GPUs. At extremely low block sizes, quality may be reduced. This setting is similar to the Chunk in Wokada Deiteris Fork. Vonovox 0.30 Block size = Wokada Deiteris Fork 300ms Chunk. Use the GPU Delay to adjust it.","Extra Time: Gives the model more or less context to work with. Recommended 2.0 for best quality/latency ratio. The added latency of this setting is far less impactful than the block size. This setting is known as Extra in Wokada Deiteris Fork, and Vonovox fixed the certain cut off issues experienced in some models over the value 2.7.","Crossfade Duration: Controls how smoothly the AI stitches different processed parts \"chunks\" of your voice back together. 0.08-0.1 or 0.15 (0.08-0.1 for fastest voice, 0.15 for improved quality but increases delay by ~ 50 ms)"]},{"l":"Effects","p":["Most of the default values are already decent.","Processing from all effects, premium and free, are done directly in the pipeline as the output voice is being produced, making them extremely low latency. Many effects can greatly enhance voice quality if used properly, while some are just for fun.","Note: If you move sliders while in the middle of speaking, sound will have some minor popping. This is completely normal as you are applying effects in the middle of a block of audio being processed."]},{"l":"Basic Effects","p":["Those are the Free Effects.","Noise Gate: A simple noise gate so the application doesn't try to process low background noise that made it past RNNoise","EQ Band (1 2): Boosts or cuts specific frequency ranges of your voice to shape its overall tone.","Frequency (Hz): Selects the center of the frequency range you want to adjust.","Gain (dB): Sets how much to raise or lower the volume of the selected frequency.","Q: Adjusts the width of the frequency band. A low Q affects a wide range of frequencies, while a high Q is more narrow and precise."]},{"l":"Premium Effects","p":["Add Static: Overlays crackling static noise for a more distorted effect.","Attack (ms): How quickly the compressor reacts to loud sounds.","Chorus: Makes a single voice sound like multiple voices by adding slightly detuned and delayed copies.","Compressor: Evens out your voice's volume, preventing sudden loud peaks and making quieter sounds more audible.","Cutoff (Hz): The frequency above which sounds will be removed.","Cutoff (Hz): The frequency below which sounds will be removed.","Damping: Absorbs high frequencies in the echoes, making the reverb sound warmer or darker.","Delay (ms): Sets the time delay between your original voice and the copies.","Depth: Determines the intensity of the pitch variations.","Dry Level: Adjusts the volume of your original, unprocessed voice.","EQ Band (3 4): Boosts or cuts specific frequency ranges of your voice to shape its overall tone.","Feedback: Feeds some of the effected sound back into the input for a more intense, swirling effect.","Frequency (Hz): Selects the center of the frequency range you want to adjust.","Gain (dB): Sets how much to raise or lower the volume of the selected frequency.","High Pass Filter (12 dB/oct): Cuts low frequencies, which can remove low-end rumble and make a voice sound thinner.","Low Pass Filter (24 dB/oct): Cuts high frequencies, making the voice sound more muffled or distant.","Low Quality Mic: Simulates the sound of a bad microphone or a telephone call.","Mix: Blends the amount of the original (dry) voice with the chorused (wet) voice.","Q: Adjusts the width of the frequency band. A low Q affects a wide range of frequencies, while a high Q is more narrow and precise.","Rate (Hz): Controls the speed of the subtle pitch variations in the chorus effect.","Ratio: How much the volume is turned down after crossing the threshold.","Release (ms): How quickly the compressor stops after the sound is no longer loud.","Resonance: Adds a slight boost to frequencies right at the cutoff point for emphasis.","Reverb: Simulates the sound of being in a physical space by adding echoes and reflections.","Room Size: Controls the perceived size of the simulated room, from a small closet to a large hall.","Strength: Controls the overall intensity of the low-quality sound effect.","Telephone Effect: Narrows the frequency range to mimic the sound of a phone line.","Those are the Paid Effects, you can learn how to get them by clicking \"Manage License\" -> \"How To Purchase\".","Threshold (dB): The volume your voice must reach before the effect starts turning it down.","Wet Level: Adjusts the volume of the reverb effect itself."]},{"l":"Update","p":["To Update Vonovox, you can either:","Click the Update Check Symbol at the bottom right of the program.","Download the latest source code the next time a new version comes out, replace the files, run setup.bat and start.bat."]},{"l":"Extras"},{"l":"Realtime Sound File Inferencing","p":["You are able to load and play sound files, converted to your model's voice in realtime.","The sound file replaces your input mic while active. Whatever sound is coming from your loaded file is your \"new microphone\" while the sound is playing. That means it will infer and play the sound file as if it was your own voice in realtime. You can play speech, singing, or whatever you want. Just make sure the audio is clean, as the client still needs to inference it, no different than the real mic.","When a sound file is playing, it will zero out the input from your real mic, meaning you don't have to worry about overlapping your voice with playback. Mic will automatically unmute when sound is playing again. Also mute and unmute is handled properly when pausing and resuming the playback of audio files.","Seek timer and playback timer so you can go to specific times in your sound file.","‎","If you are playing singing files with high pitch, you must turn off all noise suppression options as suppression models are trained on speech, not high pitch singing.","Supports wav, mp3 and flac."]},{"l":"Models to try","p":["You will need to connect your account to weights.com to be able to download these models","Click on the 3 dots (...) on weights.com models, then Download model.","Female:","Gawr Gura: Hugging Face Link/ Weights Link","Male:","Bob Ross voice made by dieseldog34","Markiplier voice made by hobqueer"]},{"l":"Known Issues / Bugs","p":["License caching is currently not working, meaning some licenses might not work."]},{"l":"FAQ"},{"l":"Do I need an extremely expensive mic for good quality?","p":["We had a conversation about this in https://discord.com/channels/1159260121998827560/1159290161683767298/1352325982689951765& https://discord.com/channels/1159260121998827560/1159290161683767298/1356265862704926907, RVC works by downsampling your audio voice to 16khz because f0 estimators only works at that sample rate, after that the model outputs the results using it's original sample rate (without any upscaling). So there won't be the need of having a super extremely expensive, a decent one should do the job."]},{"l":"How can I hear myself?","p":["Currently, Vonovox is missing the Monitor feature in Wokada Deiteris Fork, till it get's added, you have to use some workarounds:","Press WINDOWS+R, type \"mmsys.cpl\" and press Enter. Go to the Recording devices, find Line 1 and check it's Properties, go to the Listen tab and check \"Listen to this device\"."]},{"l":"What are the benefits of premium? Is it forever or monthly?","p":["You can get premium by a monthly subscription at dr87's Patreon, but the creator said he might make a lifetime version. The benefits are:","Early access.","Premium Effects and Features.","Supporter role and access to the Vonovox Official Discord Server."]},{"l":"Why are there Multiple EQ Bands Effects, which some are free and some others are paid?","p":["Having Multiple EQ bands provides the flexibility to precisely shape and refine the tone of your voice far more effectively than a single band ever could. It's made so you can adjust multiple parts of your voice range with each."]},{"i":"section-3","l":"‎"}],[{"l":"Deiteris' W Okada Fork Kaggle","p":["Last update: August 1, 2025"]},{"i":"section","l":"‎","p":["Kaggle is a cloud platform for using AI apps, powered by virtual machines with powerful GPU's.","It's a good cloud alternative for W-Okada for those who don't have good enough GPUs.","You only get 30 free GPU hours per week."]},{"i":"section-1","l":"‎"},{"l":"Create an Account"},{"i":"section-2","l":"‎"},{"l":"1. Set up account.","p":["Start by making an account here.‎","‎","Verify your acount with a phone number so you can turn on the \"internet\" option."]},{"i":"section-3","l":"‎"},{"l":"Clone and Notebook Setup"},{"i":"section-4","l":"‎"},{"l":"2. Clone Notebook","p":["Go to the realtime voice changer notebook and click \"Copy and Edit\"","Under \"Session options\" in the sidebar turn on \"internet\". Make sure persistance is on for both files and varibles.","Turn on T4 X2 GPUs in accelerator.","d: (Optional) Turn on headless mode so you can run so you can run the GPU on all sessions and save your progress. Go to the top right and click \"Save version\" then open the advanced dropdown.","‎","Your runtime will continue draining when you're not running any cells with this option on."]},{"i":"section-5","l":"‎"},{"l":"Installation"},{"i":"section-6","l":"‎"},{"l":"3. Installation Cells","p":["Starting from the top run the first cells, with the first being:","a2. When it's done, which may take a couple of minutes, it will output Done! Proceed with the next steps.","Run the third cell which is:"]},{"i":"section-7","l":"‎"},{"l":"Ngrok & Sever Setup"},{"i":"section-8","l":"‎"},{"l":"4. Ngrok Setup","p":["Scroll down to the last cell and you should see a section where you put your ngrok token. If you dont have a ngrok acount sign up here. a2. Once you have an acount you can authenticate your ngrok tunnel agent here: https://dashboard.ngrok.com/get-started/your-authtoken‎","put the Ngrok token in the last cell like so:","Once the Ngrok token is there run the cell and let it download what it needs then you can click on the ngork link and start using W-Okada.","There is a monthly limit rate with Ngrok so dont be supprised if training is suddenly interrupted.","(Optional) Directly under where you put your ngrok token there is region selection. You can change it to any of these servers to get less latency:","us -> United States (Ohio)","ap -> Asia/Pacific (Singapore)","au -> Australia (Sydney)","eu -> Europe (Frankfurt)","in -> India (Mumbai)","jp -> Japan (Tokyo)","sa -> South America (Sao Paulo)","From here it's pretty much the same as using local W-Okada."]},{"l":"Virtual Audio Cable"},{"l":"A Virtual Audio Cable (VAC) is what you need to use the realtime voice changer on Discord & Games.","p":["A VAC (Virtual Audio Cable) makes a fake audio device, used to re-route the audio of different programs.","In Wokada Deiteris Fork context, it's used to get the output of Wokada Deiteris Fork as the input in other programs such as Discord.","Download this: VAC Lite (Virtual-Audio-Cable by Muzychenko).(Be sure to not use any toher vac like VB Audio Cable.)","Run setup64, not 64a, after extracting the zip to a new folder","After installing the VAC Lite, it changes your default audio system. Click Yes when it asks you to open the audio device settings (or press WIN+R, type \"mmsys.cpl\" if you closed it already), and change your Recording and Playback devices back to your usual devices. Same for communications device aswell (right click -> set as default communication device)","Download either: Blackhole Virtual Audio Cable or VB-Audio","For Debian / Ubuntu-based Systems (Ubuntu, Mint, Pop!_ OS), run in the terminal:","For Fedora / RHEL-based Systems (CentOS, Rocky Linux), run in the terminal:","For Arch / Arch-based Systems (Endeavour, Manjaro Linux), run in the terminal:"]},{"l":"Audio Setup"},{"l":"Discord & Games","p":["On the realtime voice changer app wokada, you select:","Input: Your microphone","Output: Virtual Cable","Monitor (if you wish to hear the realtime voice changer on your headphones aswell): Your headphones","On discord and games, you select:","Input: Virtual Cable","Output: Your headphones"]},{"i":"section-9","l":"‎"}],[{"l":"Deiteris' W Okada Fork Colab","p":["Last update: July 17, 2025","This cloud version is hosted in Google Colab, remember that you have a random GPU runtime of max 4 hours daily.","You need the Google Colab Paid Tier to run this, as it uses a Web User Interface, else you could risk getting disconnected or getting banned off Colab."]},{"i":"section-1","l":"‎"},{"l":"Installation"},{"i":"section-2","l":"‎"},{"l":"1. Installation Cells","p":["Click here to access the colab. Then starting from the top run the first cell:","a2. When it's done, which may take a couple of minutes, it will output Done! Proceed with the next steps.","Run the second cell:"]},{"i":"section-3","l":"‎"},{"l":"Ngrok & Sever Setup"},{"i":"section-4","l":"‎"},{"l":"2. Ngrok Setup","p":["Scroll down to the last cell and you should see a section where you put your ngrok token. If you dont have a ngrok acount sign up here. a2. Once you have an acount you can authenticate your ngrok tunnel agent here: https://dashboard.ngrok.com/get-started/your-authtoken‎","When you have your ngrok token place it in the text box that says TOKEN_HERE","Once the Ngrok token is there run the cell and let it download what it needs then you can click on the ngork link and start using W-Okada.","There is a monthly limit rate with Ngrok so dont be supprised if training is suddenly interrupted.","(Optional) Directly under where you put your ngrok token there is region selection. You can change it to any of these servers to get less latency:","us -> United States (Ohio)","ap -> Asia/Pacific (Singapore)","au -> Australia (Sydney)","eu -> Europe (Frankfurt)","in -> India (Mumbai)","jp -> Japan (Tokyo)","sa -> South America (Sao Paulo)","From here it's pretty much the same as using local W-Okada."]},{"l":"Virtual Audio Cable"},{"l":"A Virtual Audio Cable (VAC) is what you need to use the realtime voice changer on Discord & Games.","p":["A VAC (Virtual Audio Cable) makes a fake audio device, used to re-route the audio of different programs.","In Wokada Deiteris Fork context, it's used to get the output of Wokada Deiteris Fork as the input in other programs such as Discord.","Download this: VAC Lite (Virtual-Audio-Cable by Muzychenko).(Be sure to not use any toher vac like VB Audio Cable.)","Run setup64, not 64a, after extracting the zip to a new folder","After installing the VAC Lite, it changes your default audio system. Click Yes when it asks you to open the audio device settings (or press WIN+R, type \"mmsys.cpl\" if you closed it already), and change your Recording and Playback devices back to your usual devices. Same for communications device aswell (right click -> set as default communication device)","Download either: Blackhole Virtual Audio Cable or VB-Audio","For Debian / Ubuntu-based Systems (Ubuntu, Mint, Pop!_ OS), run in the terminal:","For Fedora / RHEL-based Systems (CentOS, Rocky Linux), run in the terminal:","For Arch / Arch-based Systems (Endeavour, Manjaro Linux), run in the terminal:"]},{"l":"Audio Setup"},{"l":"Discord & Games","p":["On the realtime voice changer app wokada, you select:","Input: Your microphone","Output: Virtual Cable","Monitor (if you wish to hear the realtime voice changer on your headphones aswell): Your headphones","On discord and games, you select:","Input: Virtual Cable","Output: Your headphones"]},{"i":"section-5","l":"‎"}],[{"l":"Realtime TTS","p":["Last update: July 28, 2025"]},{"i":"section","l":"‎","p":["A VAC (Virtual Audio Cable) makes a fake audio device, used to re-route the audio of different programs.","After installing a VAC you need to:","After installing the VAC Lite, it changes your default audio system. Click Yes when it asks you to open the audio device settings (or press WIN+R, type \"mmsys.cpl\" if you closed it already), and change your Recording and Playback devices back to your usual devices. Same for communications device aswell (right click -> set as default communication device)","After that:","Audio Setup in Games:","Choose any TTS in our TTS Tools.","Download either: Blackhole Virtual Audio Cable or VB-Audio","Download this: VAC Lite (Virtual-Audio-Cable by Muzychenko).(Be sure to not use any toher vac like VB Audio Cable.)","For Debian / Ubuntu-based Systems (Ubuntu, Mint, Pop!_ OS), run in the terminal:","For Fedora / RHEL-based Systems (CentOS, Rocky Linux), run in the terminal:","Hold down the option key on your keyboard then click the sound icon in the upper right corner of the menu bar and then a dropdown menu will appear which allows you to select the VAC as the output device.","In Discord go to Settings then Voice & Video and select your VAC for your input.","In Wokada Deiteris Fork context, it's used to get the output of Wokada Deiteris Fork as the input in other programs such as Discord.","It's the same thing for game too, go into its audio settings and set its input to the VAC.","Press Command + Spacebar and type Sound and set the default audio device back to your headphones and microphone , make sure you do this for default and communication default.","Press WIN+CTRL+V and select the VAC as the output device.","Press WIN+R and type mmsys.cpl and change the default audio devices back to your headphones and microphone, make sure you do this for default and communication default.","Run setup64, not 64a, after extracting the zip to a new folder","This is a guide on how to use most of the TTS' listed in the TTS Tools section in realtime."]},{"i":"---","l":"‎ ‎"}],[{"l":"TTS Tools","p":["Last update: Dec 12, 2024"]},{"l":"Introduction","p":["TTS is an abbreviation of Text To Speech, an AI that converts any given text into vocal speech.","The ones listed here offer a decent variety of features & options, such as model training, fine-tuning, 0 shot training, or being mixed with RVC.","Here's an index of the best TTS tools out there:‎"]},{"l":"ElevenLabs/11Labs","p":["ElevenLabs is a freemium service that offers TTS, training TTS models & translating videos from different languages.","‎"]},{"l":"Fish Speech","p":["Fish speech is a 0shot multilingual TTS model created by Fish Audio.","This is one of the best 0shot TTS as of now, it rarely hallucinates.","It can be used either locally or on the cloud.","Offical github repo","Offical site","HuggingFace Space"]},{"l":"F5 TTS","p":["F5 is the best 0shot TTS model.","F5 gives fairly high quality outputs that rarely hallucinate.","But it is limited with issue like: Reading to fast = you are using a reference audio that is more the six seconds long or 100 characters. Hallucinates on low voices.","It can be used either locally or on the cloud.","Offical github repo","HuggingFace Space"]},{"i":"section","l":"‎"},{"l":"Dia TTS","p":["Dia generates high quality English only dialogue from a transcript.","Dia can create nonverbal sounds like laughter, coughing, clearing throat, etc.","It can be used either locally or on the cloud.","Offical github repo","HuggingFace Space"]},{"i":"section-1","l":"‎"},{"l":"Edge TTS","p":["\uD83D\uDCD2 Google Colab","\uD83E\uDD17 Hugging Face","Applio Colab","Download the browser.","Ilaria RVC","In the TTS input the text you want & click Generate. Stop recording when the voice is done.","It can only be used online via their API, through their web browser, a HF/Colab space or mixed with RVC.","Local Applio‎","Open Microsoft Edge & drag the .html to it.","Open your Notepad & paste the following code:","Rename it to “Microsoft Edge TTS.html”","Save it as “Microsoft Edge TTS.txt”","These being mixed with RVC means it generates the speech & runs the output through RVC, applying the voice model.","This is Microsoft Edge TTS, which is good quality, multilingual & works great on long sentences.","Use Audacity to record the audio. Set the recording mode to loopback to record the internal audio (Realtek driver might be needed).","You can then select Voice Options in the toolbar & change the speed to a faster/slower speech."]},{"i":"section-3","l":"‎"},{"i":"section-4","l":"‎"},{"l":"XTTS2","p":["Built on \uD83D\uDC22 Tortoise TTS & developed by Coqui AI, which has been discontinued unfortunately.","Has important model changes that make cross-language 0 Shot voice cloning& multilingual speech generation super easy.","You need less training data. Just least a 2 minute audio.","Can use it either online or locally:","Official XTTS 2 Guide","XTTS 2 UI Fork","Inference 0 Shot Training UI Colab(Run it & click the Public Link)","Training & Inference UI Colab","Inference 0 Shot Training HF Space"]},{"i":"section-5","l":"‎"},{"l":"Zonos","p":["0 Shot TTS with great emotion controls","Can be used with English, French, German, Chinese and Japanese","Can be used locally or online","Official Github Repo","Offical Playground","HuggingFace Space"]},{"i":"section-6","l":"‎"},{"l":"Kokoro-TTS","p":["CLI TTS","Only has premade voices","Voice bleeding for English, British English French, Itailian, Japanese and Chinese","Not the best emotion control","Official Github Repo","Offical HuggingFace Space"]},{"i":"section-7","l":"‎"},{"l":"OpenVoice","p":["Has Versatile Instant Voice Cloning (aka 0 Shot Training)","Contains cross-lingual & flexible voice style control","Available both locally & online:","Official GitHub repo","Inference GUI Colab","Official Demo Part 1 Colab","Official Demo Part 2 Colab","Official HF Space"]},{"i":"section-8","l":"‎"},{"l":"Piper","p":["Fast TTS","Great multilingual support","Works for almost all languages","Decent quality","Official Github Repo","Github Repo with Colabs(For training and inference)","Several HuggingFace Spaces for each Language"]},{"i":"section-9","l":"‎"},{"l":"MeloTTS","p":["MeloTTS is a high-quality multilingual TTS library, made by MyShell.ai","Includes almost real-time inference.","It can be used both locally and online:","Official GitHub Repo","UI Colab","NO UI Colab","HF Space"]},{"i":"section-10","l":"‎"},{"l":"GPT-SoVITS","p":["GPT-SoVITS has cross language inference, but there could be some noises.","It's very good with Chinese, but also with English.","Most parts are in japanese & not deeply tested. Expect some instability.","Can be used both locally & online:","Official GitHub Repo","Colab Space(with fine-tuning, inference & UI)"]},{"i":"section-11","l":"‎"}],[{"l":"GPT-SoVITS","p":["Last update: Mar 8, 2024"]},{"i":"section","l":"‎","p":["See original guide","GPT-SoVITS is an open-source repository focused on TTS & cross-language inference, with a Colab port coming soon. Credits to RVC-Boss.","Currently it only supports Chinese, English & Japanese. More languages are coming soon.","You'll require great specs & a NVIDIA GPU with >=6G VRAM to run it smoothly. Otherwise, use the Colab.","This guide is a translation of the original one with a few tweaks, made by Delik. [Discord: @delik - Wechat: Dellikk ]‎"]},{"i":"section-1","l":"‎"},{"i":"section-2","l":"‎"},{"l":"1. Download prezip","p":["Download the prezip of the latest version here."]},{"i":"section-3","l":"‎"},{"l":"2. Extract","p":["Unzip the folder. It's advisable to use 7-ZIP to do so."]},{"i":"section-4","l":"‎"},{"l":"3. Launch","p":["Open the folder & run go-web.bat to open WebUI."]},{"i":"section-5","l":"‎"},{"i":"section-6","l":"‎"},{"l":"1. Prepare dataset","p":["The dataset should be between 1 - 30 minutes. But you must prioritize quality over quantity.","For the best results, ensure the audio is properly cleaned, free of undesired noises & distortions.","GPT-So-VITS is made for TTS only, so it's also best to remove any singing/muffly voice parts."]},{"i":"section-7","l":"‎"},{"l":"2. Audio Slicer","p":["Copy the path file of your dataset & paste it in the Audio slicer input bar.","Create a new folder somewhere. Copy its path folder & paste in Audio slicer output. This is where the outputs are getting stored.","Adjust the parameters if needed.","Finally, click Start Audio Slicer to complete this step."]},{"i":"section-8","l":"‎"},{"l":"3. ASR","p":["The Input folder path should be the same as Audio slicer output. Jst copy the path & paste it inside the bar.","If the dataset is in English/Japanese, use Faster-Whisper large v3.","If it's in Chinese, use 达摩ASR.","Then click Start batch ASR.","If you run GPT-SoVITS for the first time, you might need to wait for a few minutes for it to download the ASR model you select.","Finally, locate the .list file & copy the path. It will be in output/asr_opt, if you didn't change the folder for Output folder path."]},{"i":"section-9","l":"‎"},{"l":"4. Text Labelling (optional)","p":["Paste the .list file path into .list annotation file path.","Tick Open labelling WebUI to open Text Labelling WebUI. A new tab will open.","Listen to each clip & edit the text if it's not transcribed properly.","The functions are self-explanatory. Use next index& previous index to check the next/previous page.","If you make changes, remember to save file& submit text."]},{"i":"section-10","l":"‎"},{"l":"5. Formatting","p":["Click 1-GPT-SOVITS-TTS& 1A-Dataset formatting to enter the training page.","Input the name of your model in Experiment/model name, & the .list file path to Text labelling file.","Scroll down to the end & start One-click formatting to begin formatting."]},{"i":"section-11","l":"‎"},{"l":"6. SoVITS Training","p":["Scroll up then click 1B-Fine-tuned training.‎‎"]},{"i":"section-12","l":"‎","p":["2| Use 1 if the GPU has 6GB VRAM.","8","<= 0.4","4","After that, click Start SoVITS training"]},{"i":"section-13","l":"‎"},{"l":"7. GPT Training","p":["2 (1 if your gpu has 6G vram)","10","5","disabled (explained later)","‎","After that, click Start GPT training"]},{"i":"section-14","l":"‎"},{"l":"DPO training (optional)","p":["DPO training greatly improves the performance (not audio quality) & stability of the model.","It can infer more text at once without slicing & it's less prone to errors (like repeating/skipping words) when inferring.","A GPU with 12G VRAM or more.","A very high quality dataset (you need to do text labelling) to enable this.","Using a batch size of 1. Keep the other settings same as above.","Otherwise, this will worsen the model."]},{"i":"section-15","l":"‎","p":["Go to the 1C-inference tab.","Press refreshing model paths& select your models from the dropdowns respectively.","Tick Open TTS inference WEBUI.","Upload a clip for reference audio ( must be 3-10 seconds). Then fill-in the Text for reference audio, which is what does the character say in the audio. Choose the language on the right.","The reference audio is very important as it determines the speed & emotion of the output. Try different ones to polish your output.","You can reopen the text proofreading tool to download the reference audio, and copy & paste the text for reference audio.","Hover above the \"duration\" to adjust the length of the reference audio, & hover above \"it\" to delete the current reference audio.","No reference text mode exists, but it's not advisable to use it. It affects the quality a lot.‎","Fill the Inference text& set the Inference language, then click Start inference.","If the text is too long choose the options in How to slice the sentence.","If you did not get your desired output, you can infer it again or change reference audio and/or adjust GPT parameters."]},{"i":"section-16","l":"‎"}],[{"l":"Model Maker Role","p":["Last update: October 20, 2024","To upload your Voice Models in the AI HUB Discord Server's #voice-models forum channel, you need to pass a Quality Control (QC) check to be sure that you post good voice models."]},{"i":"section","l":"‎","p":["Model's .PTH file.","Model's .INDEX file.","General information about the model.","General information about its training process.","A Hugging Face or weights.com account.","At least 1 raw audio sample of the model WITH NO MUSIC."]},{"l":"It lacks the correct files.","p":["The .ZIP file must contain both the correct.INDEX& .PTH file.","The correct .index is the one named added_.","The added index contains the voice's accent and speech manor.","The correct .pth is the one that has your model's name, for example: TylerSwift_e60_s120.pth","The .pth contains the actual model and pitch data."]},{"i":"section-2","l":"‎"},{"l":"Model is low quality.","p":["A bad model:","Sounds scratchy/screechy.","Has a muffled sound.","Sounds inaccurate to the source.","Is incapable of hitting certain notes.","Has slurred speech.","Is unable of pronouncing words correctly in its intended language.","Has artifacting.","Has noise."]},{"i":"section-3","l":"‎"},{"l":"An outdated extraction method was used.","p":["Only Mangio-Crepe& RMVPE are allowed. Learn about them here","Harvest, Dio, Crepe-Tiny, PM, etc. are obsolete."]},{"i":"section-4","l":"‎"},{"l":"The audio demo contains instrumental.","p":["Don't include ANY music in the audio demo, even if it's not copyrighted. This is due to:","Concerns over copyright.","In many cases, the music can \"hide\" the flaws of the voice model, making it harder to judge its quality."]},{"l":"The audio demo is altered.","p":["Don't add reverb, equalize, or alter the demo in any way, as it won't be a faithful representation of the model. It must be the raw, unmodified output from the inference.","Trimming silences at the beginning/end of the audio demo is allowed."]},{"i":"section-5","l":"‎"},{"l":"Is a robotic or non-human voice.","p":["Robotic, sound effect and drum models will also be rejected, because with these types of voices it is difficult to determine if you know how to clean a dataset properly.","However once you get model maker you will be able to post robotic, sound effect or drum models."]},{"i":"section-6","l":"‎"},{"i":"section-7","l":"‎"},{"l":"Step 3: Prepare the submission.","p":["Once your model is ready, head over to the AI HUB's #model-maker-role channel.","Click the Submit Model button.","‎‎","Its name.","The technology used for its training.","The extraction method you used.","Total epochs amount.","Its download link from Hugging Face or Weights.","An audio sample of it talking/singing.","Optional. Add more context about the model if you want.","You can attach more samples when you repost the model to #voice-models."]},{"i":"section-8","l":"‎"},{"l":"Step 4: Send submission.","p":["Once you are done filling the information it will send your model to get QCed","‎‎","Now, wait for a QC(quality checker) to verify your model. You'll be notified once it has been reviewed.","If you made a mistake in your submission or you want to change something you can cancel your submission by clicking on the cancel button that is attatched to the message you get when you send a submission.","‎","If your model gets approved, the bot will notify you with a message like this:","You can then repost the model (& future models) to the #voice-models forum."]},{"i":"section-9","l":"‎"}],[{"l":"Illusion Diffusion","p":["Last update: Mar 9, 2024"]},{"i":"section","l":"‎"},{"i":"section-1","l":"‎"},{"l":"1. Upload","p":["Go to the Illusion Diffusion Hugging Face Space.","Click on Input Illusion.","Upload the image you wish to blend.‎"]},{"l":"2. Illusion Strength","p":["Modify the \"Illusion Strength\" slider.","Higher values will make the picture more visible.","Lower ones will hide it more.‎"]},{"l":"3. Prompt","p":["Describe the characteristics of the output.","These can be environments (medieval castle - flowery meadow), traits (high quality - a specific image style), etc.‎"]},{"l":"4. Negative Prompt (optional)","p":["Specify what the AI should NOT do when creating the image (e.g., low quality, specific styles to avoid, etc.).‎"]},{"l":"5. Run","p":["Press Run to begin processing.","Be patient; there might be a queue.","If you encounter an error, try clicking again until it works."]},{"i":"section-2","l":"‎"}],[{"l":"Glossary","p":["Last update: June 15, 2024",":","Vocal lines that contribute to the sound of the lead vocals in a song."]},{"i":"section","l":"‎"},{"l":"Bit depth","p":["In the field of digital audio, it defines the dynamic range of each sample.","This determines the difference between the quietest & loudest sound.","Basically, higher bit depths represent more accurately the loudness of an audio."]},{"i":"section-1","l":"‎"},{"l":"Bitrate","p":["The amount of data processed per certain unit of time, usually in kilobits per second (KBPS).","Higher bitrate equals a higher quality.","You can think of it as video resolution (240, 480, 1080, etc.)."]},{"i":"section-2","l":"‎"},{"l":"Checkpoints","p":["In RVC, these are files of a model that generate over the course of training, that can be very useful.‎","The rate at which they're saved is determined by the save frequency value (or save rate or similar names). For newbies, it's recommended use a value of 15.‎","They are divided by two types:‎","Weights:","These are actual models.","They're organized with this format: modelname_epoch_step.pth","Example: Tyler_e60_s120.pth‎","G and D:","Named G_ and D_, followed by the step number & .pth.","Example: G_70.pth and D_70.pth","These allow you to resume training, if G and D's numbers match."]},{"i":"section-3","l":"‎"},{"l":"Cloud-based","p":["Any software or application that's stored, managed, and available through the provider's virtual servers, and is accessed through a web browser.","The opposite of local running."]},{"i":"section-4","l":"‎"},{"l":"CUDA","p":["A technology developed by NVIDIA, that uses the power of graphics cards to perform calculations that require great processing power.","It's especially useful for AI tools, such as RVC and UVR, which greatly optimizes the process.","CUDA is downloaded automatically as a part of the NVIDIA driver."]},{"i":"section-5","l":"‎"},{"l":"DAW","p":["It stands for Digital Audio Workstation, and it's any software used for making and mixing music.","For basic audio editing, we recommend Audacity.","For professional mixing, FL Studio."]},{"i":"section-6","l":"‎"},{"l":"Fine-tuning","p":["Further improving an AI model, training it with a another dataset."]},{"i":"section-7","l":"‎"},{"l":"Fork","p":["It's a copy of a main GitHub project. It aims to make a different version of the project with improvements, changes & new features."]},{"i":"section-8","l":"‎"},{"l":"Gradio","p":["Gradio is an open-source Python packag that makes it easy for developers to create user-friendly web interfaces for machine learning models and other applications, such as RVC.","It deploys the program on a Local URL, which is the one running locally on the machine, and a Public Share Link, which is a tunnel that exposes the Local URL. The Public Share Link is used, for example, in Google Colabs, powered by their Share API. Sometimes, the Share API goes down, you can check its status."]},{"i":"section-9","l":"‎"},{"l":"Google Colab","p":["Google Colaboratory is a product of Google that allows anybody to write & execute arbitrary python code through websites.","It's free version is slower & with a usage time of their GPUs of around 3 hours a day. Once you exhaust it, you'll have to wait 12 - 24 hours.","Learn how to bypass their limitations here."]},{"i":"section-10","l":"‎"},{"l":"GPU","p":["It stands for Graphics Processing Unit. It's designed to rapidly manipulate and alter memory to accelerate creation of images.","In AI training, is used for quick parallel independent computations, which increases the speed substantially.","Basically the speed at which RVC/UVR will work will depend on how good your GPU is."]},{"i":"section-11","l":"‎"},{"l":"Inference","p":["In the context of AI, it's using an AI model to complete any task.","For this, using the GPU is more convenient as it's faster. Though normally you can still use CPU, which takes longer.","For example, in RVC is when a voice model is used to transform an audio, to make it sound like the model."]},{"i":"section-12","l":"‎"},{"l":"Local running","p":["Running locally is a process that involves running apps in your own machine, using its resources.","The opposite of cloud-based."]},{"i":"section-13","l":"‎"},{"l":"Lossless Formats","p":["Audio formats that don't compress(lose) the original quality.‎","They're recommended for RVC, as the more quality an audio has, the more accurate results they'll offer.‎","The main ones are WAV & FLAC:‎","FLAC:","Its algorithm compresses the data without losing quality.","It's recommended over WAV since it's space-efficient.‎","WAV:","Doesn't do any kind of compression. It's purely the original data.","Therefore it has a much bigger file size.","It's more accurate to describe it as an uncompressed format","Both formats give the same results & don't have an audible difference. Converting a lossy audio to a lossless one won't restore the lost quality."]},{"i":"section-14","l":"‎"},{"l":"Lossy Formats","p":["Audio formats that compress(lose) the original quality. They're built to be space-efficient.","So by getting rid of some data (in this case, quality), they achieve a smaller file size.","Common lossy formats are MP3, OGG, OPUS, M4A, etc."]},{"i":"section-15","l":"‎"},{"l":"Localtunnel","p":["Localtunnel is a tunnel made to expose a local url (like http://localhost:3000).","It's used in Google Colabs to expose the Local URL so that users on Cloud can access the program."]},{"i":"section-16","l":"‎"},{"l":"Model training","p":["In the field of AI, is the process where an AI model is fed with its dataset & learns from it."]},{"i":"section-17","l":"‎"},{"l":"Specs","p":["It refers to a computer's specifications. Hardware like GPU, CPU, RAM, etc.","The performance of the hardware of a computer directly correlates to the performance of all its software."]},{"i":"section-18","l":"‎"},{"l":"0 Shot Training","p":["Doing inference on an AI model without explicitly training on it.","It's faster but with less quality, and you won't be able to save the model.","For example, in TTS you do inference by cloning a voice with an audio, a data it hasn't seen before.","Different from making a dataset & doing the long training process, based on lots of criteria such as epochs.","In some cases you can do it on GPU, some in CPU."]},{"i":"section-19","l":"‎"},{"l":"Optim","p":["It is a shorter way to say optimizer.","A optimizer is an algorithm used to minimize the loss function during the training of neural networks. It helps adjust the model's weights and biases."]},{"i":"section-20","l":"‎"}],[{"l":"Contributions","p":["Last update: July 28, 2025"]},{"l":"Contributions","p":["We'll appreciate any feedback, big or small. If you wish to contact anyone who worked on this you can find them in AI Hub.","Leave suggestions in the #suggestions channel.","To report issues use #ai-help-forum. You can also send issues to our GitHub Repository.","If you would like to add to this doc please make a pull request in the GitHub Repository."]},{"i":"section","l":"‎"}],[{"l":"License"},{"l":"License CC BY-NC-SA 4.0","p":["This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."]},{"l":"You are free to:","p":["Share: Copy and redistribute the material in any medium or format.","Adapt: Remix, transform, and build upon the material.","The licensor cannot revoke these freedoms as long as you follow the license terms."]},{"l":"Under the following terms:","p":["Attribution: You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.","NonCommercial: You may not use the material for commercial purposes.","ShareAlike: If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original."]},{"l":"You are not required to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation."},{"l":"No warranties are given. The license may not give you all the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.","p":["Read the full text of the license."]}]]