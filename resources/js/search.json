[[{"l":"Home","p":["Last update: August 5, 2025"]},{"i":"section-2","l":"‎"},{"l":"Introduction","p":["This site is a documentation of General AI tools, mostly RVC-related apps. Made by staffers/members of AI HUB.","See simple & convenient guides regarding RVC model training, RVC inference, realtime voice changing, audio isolation, datasets, TensorBoard, & more. Verified by the experts & for all devices."]},{"i":"section-3","l":"‎"},{"l":"Contributions","p":["We'll appreciate any feedback, big or small. You can directly contact us in AI Hub: @eddycrack864","Leave suggestions in the #suggestions channel. To report issues, use #ai-help-forum.","You can also send an issue/pull request to our GitHub Docs Repository."]},{"i":"section-4","l":"‎"},{"l":"Disclaimer","p":["The tools and guides provided on this site are for informational and educational purposes only. Good and ethical fun only. The creators and contributors of this documentation do not condone and are not responsible for any misuse of the information or tools provided.","Illegal Activities: We strictly prohibit the use of any information or tools from this site for illegal activities, including but not limited to scamming, catfishing, and any form of fraud.","Misuse of AI: The use of AI should be done responsibly. We do not support the use of AI for creating harmful, deceptive, or malicious content.","Copyright: Users are responsible for ensuring they have the necessary rights and permissions for any content they use with these tools. We do not condone copyright infringement.","Responsibility: By using the information and tools provided, you agree that the creators and contributors of this documentation are not liable for any damages or issues that may arise from your actions."]},{"i":"section-5","l":"‎"},{"l":"Credits","p":["Lead by: Nick088, Eddy, Julia (Used to)","General Help: Poopmaster Raid, Light, Faze Masta, Alexolotl, Delik, Razer, Nick088","Reviewing: Faze Masta, Alexolotl, SimplCup, Delik, Litsa, Lyery, Razer","OG Guides: Litsa, Faze Masta, MrM0dz, FDG, Eddy, Julia, Nick088","Backend: Eddy & Yui","Branding: Grvyscale & Cthulhu"]},{"i":"section-6","l":"‎"},{"l":"To-Do","p":["Nothing \uD83D\uDE04"]}],[{"l":"How to Make AI Cover","p":["Last update: July 17, 2025"]},{"l":"- Simple AI cover tutorial, using RVC -"},{"i":"section","l":"‎"},{"l":"1. Extract vocals","p":["Have the audio file of your song ready, & let's extract the vocals from it with an audio isolation software.","RVC is designed to work with only voices, so to get the best results the sample must be clean, without undesired noises."]},{"i":"section-1","l":"‎"},{"l":"2. Get voice model","p":["Learn about them & how to search one here. Be sure to leave credits to the model maker.","In case the model doesn't exist, click here."]},{"i":"section-2","l":"‎"},{"l":"3. Convert the vocals","p":["After obtaining the vocals & model, it's time to set up RVC & do inference.","There are plenty of versions of RVC, but these are the best ones for beginners. Pick according to your needs:"]},{"i":"section-3","l":"‎","p":["For local users, first ensure you meet the minimum requirements."]},{"i":"section-4","l":"‎"},{"l":"Tips","p":["Congratulations, you've made it to the final part. Now it's to mix the song.‎","You're free to use any DAW, but we recommend FL Studio or BandLab, as they are beginner-friendly. You can start by searching some of their mixing tutorials on YouTube.‎","Recommendations for the mix:","Match the volume of the vocals to the same level as the original ones.","Add reverb to the vocals (not to the instrumental), to the same level as the original one.","Add delay if the original vocals had it.","Remove the very low frequencies, ranging from 20 to 100.","For presence and clarity, increase the high range a bit.","Normalize the audio.","Use compressor on vocals.‎","Regarding what to do with the backing vocals, you have 4 options:","Simply leave the original ones in.","Convert them using Mangio-Crepe with a higher hop length.","Record yourself singing them & convert the audio with RVC.","Make vocals from scratch using a voice synthesizer (like SynthV) & convert them with RVC."]},{"i":"section-5","l":"‎"}],[{"l":"How to Make Voice Models","p":["Last update: July 17, 2025"]},{"l":"- Simple model training tutorial, using RVC -"},{"i":"section","l":"‎"},{"l":"1. Prepare dataset","p":["In the context of RVC, the dataset is an audio file containing the voice the model will replicate. It can be either speaking or singing.","For the best results, having a clean dataset is crucial, so take the time to remove any undesired noises."]},{"i":"section-1","l":"‎"},{"l":"2. Set up RVC","p":["With your dataset ready, it's time to set up RVC to train the model.","There are plenty of versions of RVC, but these are the best ones for beginners. Pick according to your needs:"]},{"i":"section-2","l":"‎","p":["For local users, first ensure you meet the local minimum requirements."]},{"i":"section-3","l":"‎"},{"l":"3. Train the model","p":["Before you start training, we inform you that the training guides are oriented around using TensorBoard. Read about it & install it after setting up RVC.","Good luck & remember to be patient! As this won't be an instant process.","‎"]}],[{"l":"Voice Models","p":["Last update: July 17, 2025","In the field of AI, is a program that was trained to recognize certain patterns or make certain decisions.","In this case, voice models are models trained to replicate a voice, and with AI they apply it to the input audio.","There are plenty of them uploaded to the internet, made by the public. And the best way to make them is with RVC."]},{"i":"section","l":"‎"},{"l":"Voice Model Files"},{"l":"*They are made up of two files:*","p":["This file is the model itself.","Contains data regarding pitch.","While training, RVC generates other .PTHs named D_ and G_, but these are the checkpoints, not usable models.","Contains data regarding the voice's accent and speech manner.","File is additional, but usually crucial for the quality of the model.","While training, RVC generates two .INDEX file, but the right one will be named added_ by default.","As people sometimes upload them incorrectly."]},{"i":"section-3","l":"‎"},{"i":"section-4","l":"‎","p":["‎‎","‎ ‎ ‎ ‎ ‎ ‎ ‎","\uD83D\uDC40‎ If there are multiple models, click the View other models... bar to see the others.","\uD83D\uDCE4‎ Click View Model to View it, and by being logged in, you can click the 3 dots and Download it.","\uD83D\uDD17‎ Click Use Model to Use it on Weights.com.","Access the website here& login by clicking the icon on the top right corner.","Check the description, likes, comments, & audio sample. Feedback can help you know how great the model is.","Click the Hugging Face link to download the model, or copy it if that's what you need.","Click the model & go to the Files and versions tab.","Download the correct files of the model. Then if you need its link, upload it to HF.","Go to the models page& search the model in the Filter by name bar.","Head over to the #find-models channel.","If it only exists in weights.com, download the .ZIP & upload it to HF.","If you get models from different years, remember, the person's voice changes overtime.","If you haven't already, join AI Hub here.","If you need a link for it, use the other methods.","If you're curious about the epochs, learn more here.","In the upper search bar, search your model & click the post.","It searches the models uploaded on Weights.com/AI Hub Discord server.","Models uploaded in AI Hub get automatically stored here too.","Reminder: This is a General AI Platform, not every model is an RVC one.","Searching here is specially useful if you need the model as a link, as the posts include one.","Select the Weights command","Send the message","Tap the three dots & Download model. It will download a .ZIP file of it.‎‎‎","The sample of the gender & vocal style according to the model gives the most accurate representation.","Then go to the #voice-models forum channel.","There's also its web version.","This a website where people can upload voice models.","This is a Discord bot developed by the Weights.com team.","This is a forum channel in AI Hub where people upload their own voice models.","This is a free & open-source platform for storing Any Type AI models, interactive AI apps, & datasets.","This step is specially useful if you get multiple results of the same model.","To download it, click the download symbol ( ) on the right of the .ZIP file. If you need its link, right-click it and copy the address.","Type /find","Type the model","Type the name of the model in the Search bar & click a result.","Users can read/share feedback about the models through comments & likes.","You can listen to the audio sample to get a preview of the it."]},{"i":"section-17","l":"‎"},{"l":"If you couldn't find one, you have 3 options:","p":["Make the model yourself","Pick a different one.","Request a free model via AI HUB's #request-models forum channel. Be aware that we don't allow paid commissions"]},{"i":"section-18","l":"‎"},{"l":"1. Zip the model","p":["Select the correct .PTH & .INDEX& zip them into a .ZIP file.","Ensure it's .ZIP & not .RAR or .7ZIP."]},{"i":"section-19","l":"‎"},{"l":"2. Log in","p":["If you haven't already, create an account& log in."]},{"i":"section-20","l":"‎"},{"l":"3. Upload the Model","p":["Once logged in, go to Upload a Voice Model in the train models tab.","‎","In Model name you name the model.","Describe the model in Model Description.","Select a Tag, such as English, Anime, etc.","Upload an Image for the model, like the character's image.","Upload the Model Zip containing the .PTH and .INDEX file.","You're done!"]},{"i":"section-21","l":"‎"},{"i":"section-22","l":"‎"},{"i":"1-zip-the-model-1","l":"1. Zip the model","p":["Select the correct .PTH & .INDEX& zip them into a .ZIP file.","Ensure it's .ZIP & not .RAR or .7ZIP."]},{"i":"section-23","l":"‎"},{"i":"2-log-in-1","l":"2. Log in","p":["If you haven't already, create an account& log in."]},{"i":"section-24","l":"‎"},{"l":"3. Make repository","p":["Once logged in, tap your profile on the upper right corner, & then New Model.","‎","In Model name you name the repo as you want.","Make sure License is set as openrail& the repo is set as Public.","Once done, hit Create model."]},{"i":"section-25","l":"‎"},{"l":"4. Upload model","p":["It will redirect you to the repo. Go to the Files and versions tab on the center, click + Add file on the right & then Upload files.","‎‎","Tap the upload box & submit the ZIP. Or just drag & drop.","Tap on Commit changes to main& the model will begin to upload."]},{"i":"section-26","l":"‎"},{"l":"5. Copy link (optional)","p":["Once it's done, it will redirect you to the files list.","So if you need its link, right-click the download button ( ) of the .ZIP file on the right, and click Copy Link."]},{"i":"section-27","l":"‎"}],[{"l":"What's RVC","p":["Last update: August 9, 2025"]},{"l":"Introduction","p":["RVC ( Retrieval-Based Voice Conversion) is an advanced AI voice cloning software, developed by the RVC-Project team. It's considered the best free & open-source one to date.","It was designed for desktop, requiring great specs to run it effectively, specially GPU for training models.","Though it can be executed through the cloud& be used in any device, in case you don't meet the previous requirement.","RVC doesn't have any major quality improvements since 2023, since its original devs are focused on other projects, RVC is hard to optimize, and it has limitations like non speech sounds such as realistic laughing, screaming, etc. Though, there are commmunity driven Forks that try to experiment with it, mostly about adding new features and performance improvements."]},{"l":"Forks","p":["A fork is a copy of a main GitHub Project. It aims to make a modified version of the project, with improvements, new features & modifications.","RVC has quite a few forks made by the community, each one meeting different needs for the user, and with its pros & cons.","These are the main ones, along with their cloud-based counterparts:","‎"]},{"l":"FAQ"},{"l":"Frequently asked questions."},{"l":"*What's the best fork?*","p":["As explained before, it depends on your needs. It's best to try them yourself.","For local users, Applio is a great starting point. For cloud users you can use either the Applio Colab or applio kaggle."]},{"l":"*What are the requirements for RVC locally?*","p":["30 GB","6 GB","6GB","8GB","For inference, the storage requirement varies depending on the fork. It can be around 5 to 9 GB","GPU","If you don't meet the minimum requirements, it's more convenient to use RVC on the cloud.","MINIMUM REQUIREMENT","NVIDIA GTX 900 Series / AMD RX580 (Mac isn't supported)","NVIDIA RTX 20 Series or later / AMD Radeon RX 5xxx or later (Mac isn't supported)","NVIDIA RTX 20 Series or later / AMD Radeon RX 5xxx or later / Apple M3","RAM","SPEC","Storage","SUGGESTED REQUIREMENT","The minimum specs vary depending if it's for training models or inference.","VRAM"]},{"l":"*Can I use it on my AMD GPU?*","p":["You can, but it's going to be slower, as they don't have CUDA cores.","So it's more convenient using RVC through the cloud.","If you're willing to use a slower version you can go ahead and follow this guide on how to get zluda working with Applio Zluda."]},{"l":"*How long does it take to train?*","p":["The total time depends on a lot of factors, like dataset length, batch size, pretrains, specs, etc.","A 10 min dataset with RMVPE may take around 1 to 2 hours."]},{"l":"*Can I run it on a Mac?*","p":["Yes, on Macs of recent generations.","But you can only do inference& it's a little unstable."]},{"l":"*Do I need internet to use it?*","p":["If you're using RVC locally, no (the only exception would be Applio TTS as it uses Microsoft's Edge TTS as a base).","If you're using it through the cloud, then yes."]},{"l":"*Is there a scientific paper for RVC to understand more about it?*","p":["There isn't an official one, but there's an unofficial complex blog to understand how it works."]},{"i":"section-9","l":"‎"}],[{"l":"Applio","p":["Last update: August 9, 2025","Applio is a VITS-based Voice Conversion Tool developed by the IA Hispano team. It's a Fork of Original/Mainline RVC.","It's liked for its great UI, performance improvements and lots of extra features, such as TTS (with RVC models too), plugins, automatic model upload, customizable theme & more.","Because of its user-friendly experience & active development, it's considered to be one of the best forks.","Applio has it's own Applio Docs, which may have more info about the tool."]},{"l":"Are RVC Models Safe?","p":["RVC Models are PyTorch Models, a Python library used for AI. PyTorch uses serialization via Pythons' Pickle Module, converting the model to a file. Since pickle can execute arbitrary code when loading a model, it could be theoretically used for malware, but this fork has a built-in feature to prevent code execution along the model. Also, HuggingFace has a Security Scanner which scans for any unsafe pickle exploits and uses also ClamAV for scanning dangerous files.","‎"]},{"l":"Pros & Cons"},{"l":"*Learn more*","p":["Very complete","Has an active development","Currently stable","Faster interface","Faster Training","Has (not Mangio) Crepe for Training","TTS features","Automatic model upload","User-friendly UI","TensorBoard included","Extra features: (plugins, model fusion, etc)","None \uD83D\uDE04"]},{"l":"System & Hardware Requirements","p":["Check if you meet the requirements to run it locally.","If you don't meet the requirements, there are 4 Cloud Versions:","Applio UI Kaggle","Applio UI Google Colab","Applio NO UI Google Colab","Applio UI Lightning.AI"]},{"i":"section","l":"‎"},{"i":"section-1","l":"‎","p":["Don't put it in a folder with privileged access.","Don't run the run-install.bat as an administrator.","Make sure the path does not contain any spaces or special characters.","Deactivate your antivirus and firewall to avoid missing dependencies."]},{"l":"Nvidia on Windows (Precompiled)","p":["RTX 5000 Series Users require version 3.3.0 or newer.","The easiest way to download Applio is by going to Applio's Hugging Face repo, and clicking the [download] button on the right-hand side.","Unzip the folder. This may take a few minutes.","Open Applio's folder & execute run-applio.bat.","‎‎","A console tab will appear, and after a moment your default browser, with Applio ready to use.‎"]},{"l":"Linux & macOS","p":["The easiest way to download Applio is by going to Applio's Hugging Face repo, and clicking the [download] button on the right-hand side.","Unzip the folder. This may take a few minutes.","Make sure you have Python 3.10.12 or 3.11.x installed. You can check your version by running python --version.","Open a terminal in the Applio directory you just extracted.","Run the commands corresponding to your Linux distribution:","Run Applio","In the terminal, run the following commands to make the script executable and launch the application:","‎","A console tab will appear, and after a moment your default browser, with Applio ready to use.‎"]},{"l":"Debian/Ubuntu"},{"l":"Arch"},{"l":"Fedora"},{"l":"AMD on Windows (Precompiled Fix)","p":["‎","A console tab will appear, and after a moment your default browser, with Applio ready to use.‎","Add the bin directory of your HIP SDK installation to your system's Path environment variables.","Download a compiled version of Applio(v3.2.5 or higher) and unzip it to your desired folder.","Download and install the VC++ Runtime.","First, check the official System Requirements on the AMD ROCm™ documentation site. In the \"Windows-supported GPUs\" section, determine which steps to follow below.","For AMD GPU users, follow these steps to set up Applio:","For HIP SDK 5.7: C:\\Program Files\\AMD\\ROCm\\5.7\\bin","For HIP SDK 5.7: Run patch_zluda_hip57.bat.","For HIP SDK 6.1: C:\\Program Files\\AMD\\ROCm\\6.1\\bin","For HIP SDK 6.1: Run patch_zluda_hip61.bat.","It's assumed your primary AMD GPU has an index of 0. If you have an iGPU that is listed first in Device Manager (under 'Display Adapters'), you must edit the run-applio-amd.bat file and change the value from 0 to 1.","Move all .bat files from this folder to the main (root) Applio folder.","Navigate to the assets\\zluda folder inside your Applio directory.","Open a Command Prompt in the Applio folder (type CMD in the address bar and press Enter). Run the following commands to install the correct version of PyTorch for Zluda.","Run run-applio-amd.bat to start Applio.","Run the patch file that corresponds to your HIP SDK version:","The very first time you run a task (like inference or training), Applio may appear to freeze for 15-20 minutes. This is normal. Zluda is compiling the necessary kernel code in the background. Subsequent runs will be fast.","This guide is for AMD GPU users on Windows. It uses Zluda to enable CUDA compatibility."]},{"l":"GPU has a green check in the HIP SDK column","p":["Install either v6.1.2 or v5.7.1 HIP SDK from the AMD ROCm Hub."]},{"l":"GPU is RX 6600, 6600XT, 6650XT, 6700, 6700XT, or 6750XT","p":["Install v5.7.1 HIP SDK from the AMD ROCm Hub.","Download the correct archive for your GPU:","For 6700, 6700XT, 6750XT, download the gfx1031 archive.","For 6600, 6600XT, 6650XT, download the gfx1032 archive.","Navigate to C:\\Program Files\\AMD\\ROCm\\5.7\\bin\\rocblas\\ and rename the library folder to library.old.","Create a new, empty folder named library in its place.","Unzip the content of the archive you downloaded into this new library folder."]},{"l":"All other AMD GPUs","p":["Find your GPU's gfxNNNN value. You can do this by searching \"techpowerup your_gpu_name\" (e.g., \"techpowerup RX 7900 XTX\") and finding the \"Shader ISA\" on the specifications page.","Follow the steps for your corresponding gfx value:","Install v5.7.1 HIP SDK from the AMD ROCm Hub.","Download this archive.","Navigate to C:\\Program Files\\AMD\\ROCm\\5.7\\bin\\rocblas\\ and rename the library folder to library.old.","Unzip the content of the archive directly into the C:\\Program Files\\AMD\\ROCm\\5.7\\bin\\rocblas\\ folder.","Visit this repository and follow the specific instructions provided there."]},{"i":"section-2","l":"‎"},{"i":"section-3","l":"‎","p":["If you encounter an issue, be sure to read the Troubleshooting chapter."]},{"i":"section-4","l":"‎"},{"i":"section-5","l":"‎"},{"l":"1. Upload voice model.","p":["Go to the Download tab. You have two ways of uploading it: through its link or manually inputting its files.","Go to the Download tab & paste the link of the model in the Model Link bar. It must be from Hugging Face or Google Drive.‎‎","Press Download Model.","Drag & drop the model's .PTH in the Drop files box below.‎‎","Then drag the .INDEX.","‎"]},{"l":"2. Select voice model.","p":["Return to the Inference tab & click the Refresh button on the right.","‎","Select your model in the Voice Model dropdown."]},{"i":"section-6","l":"‎"},{"l":"3. Input vocals.","p":["With Applio you can convert audios individually or in batches:","Drag & drop the audio or click the upload box to search it.‎‎","Then select it in the dropdown below.‎","Go to the Batch tab.‎","In the Input Folder bar, paste the path folder containing the audios.","In Output Folder you can paste a path folder for the results.","Ensure the paths don't contain spaces/special characters.","‎"]},{"l":"4. Modify settings. (optional)","p":["Unfold Advanced Settings if you wish to modify the inference settings for better results, or to determine the output folder.","‎"]},{"i":"section-7","l":"‎"},{"l":"5. Convert.","p":["Click Convert at the bottom. The audio will begin to process. The processing time will mainly depend on your specs, length of audio & the algorithm picked.","Once it's done, you can hear the results in the Export Audio box below.","By default the output files will be in the \" audios\" folder: \\ApplioV3.0.7\\assets\\audios"]},{"i":"section-8","l":"‎"},{"i":"section-9","l":"‎","p":["The training guide will be centered around using TensorBoard. Read about it first if you haven't already. If you encounter an issue, be sure to read the Troubleshooting chapter."]},{"i":"section-11","l":"‎"},{"l":"1. PREPROCESS","p":["Go to the Train tab. Input a name for your model in Model Name. Don't include spaces/special characters.","‎","Paste the path file of your dataset in the Dataset Path bar. Ensure the path doesn't contain spaces/special characters.","Select your dataset's sample rate. If you don't know the amount, click here.","Ensure RVC Version is set as V2& click Preprocess Dataset.","It'll finish when the output box says preprocessed successfully."]},{"l":"2. EXTRACT","p":["Select the algorithm you want. Use either RMVPE(most suggested) or Crepe. Applio removed pm. dio and harvest as they are outdated.","Select the Embedder Model you want. Contentvec is the most used.","If you select \"custom\" for embedders, you can add your own, like Spin(which seems to have better pronunciation than contentvec and better for realtime as it handles context differently)","Give it a Folder Name, like \"spin\".","Upload the .bin and .json files, which for example you can find them at https://huggingface.co/IAHispano/Applio/tree/main/Resources/embedders/spin for spin.","Click \"Move files to custom embedder folder\".","After you added your custom embedder, Refresh Embedders and select it from the Dropdown menu at the left of the refresh embedders button.","Press Extract Features. It'll finish when it says extracted successfully."]},{"l":"3. TRAIN","p":["‎‎‎","‎","But since we'll use TensorBoard, use an arbitrarily large value like 1000","Check the Custom Pretrained box to use your own files. This will open the Pretrained Custom Settings section.","Click Generate Index. This will create the model's .INDEX file.","Doing this makes the Tensor Board's graphs accurate.","Download Custom Pretrained Models (Optional):","E.g: with a value of 10, they will be saved after the epoch 10, 20, 30, etc.","Frequency of the saving checkpoints, based on the epochs.‎","Go to the Download tab, go to the download custom pretrain, and select the community made ones like TITAN and for which sample rate you need.","If you are a newbie, simply leave it at 15, but if you wish to be percise set it to 1.","If you are a newbie, use 8. But in case your dataset is short (around 2 minutes or less), use 4.","If you can't find the pretrain you want, you can check AI HUB's #pretrain-models or here","If you have multiple GPUs, tick GPU Settings to use a specific one for the training.","Input the total amount of epochs(training cycles) for the model.‎","Press Start Training to begin the training process.‎","RVC uses the Orignal Pretrain by default ( Pretrained is always checked, unchecking it is highly not recommended as you won't use even the Original Pretrain and train from scratc) to significantly reduce training duration and enhance overall quality. You can use the original pretrain, or community made models downloaded via the Download tab or upload them yourself.","Select: After uploading, click Refresh Custom Pretraineds. Then, select your custom generator and discriminator from the Custom Pretrained G and Custom Pretrained D dropdown menus. These dropdowns will also show any custom pretrained models you have downloaded from the Download tab.","Sync graphs trains a single epoch and sets the log interval to the amount of steps that epoch trained for.‎","The latter will show you errors if they happen, and information about the epochs & checkpoints.","To open TB, execute run-tensorboard in Applio's folder. Remember to monitor it, as well as the console just in case.‎","To use a pretrained model that you downloaded from the Download tab, simply check the Pretrained box.","Upload: Click Upload Pretrained Model to open a file dialog. Here, you can drag and drop your files or click to upload the Generator ( G) and Discriminator ( D) .pth files. This is for when you want to upload your own pretrains which aren't in the community download pretrains tab.","Use Custom Pretrained Models (Optional):"]},{"l":"4. FINAL STEP","p":["When you're very sure of overtraining, you can stop training by going to the Settings tab & press Restart Applio.","‎","Create a new folder anywhere named as the model.‎","Open Applio's folder, go to logs, and open the folder named as the model.‎","Select the .INDEX named added_& move it to your newly made folder.‎‎‎","In said folder you'll also find all the checkpoints.‎","Select the one closest to before the overtraining point, and move it to the new folder.","The checkpoints will be organized with this format: ModelName_Epoch_Step.pth Example: arianagrande_e60_s120.pth","‎‎","And that's all, have fun with your model. To test it, do a normal inference as usual."]},{"l":"5. RESUMING","p":["In case the training finished but the model still needed training, you don't have to start from scratch.‎","Simply enter the same settings & criteria that you've previously inserted. You don't have to do the preprocess or train the .INDEX again.‎","You can change the save frequency, or increase the Total Epoch amount in case you didn't input enough before.‎","Begin training again & remember to monitor TB & console like before."]},{"i":"section-50","l":"‎"},{"i":"section-51","l":"‎"},{"i":"section-52","l":"‎","p":["+ with any RVC model"]},{"i":"section-53","l":"‎","p":["Applio is also known for having one TTS tool by default, with plenty of voices to choose for.","You can also use it with RVC models& apply the inference settings if you wish.","Aditionally, you can download the Eleven Labs TTS plugin."]},{"i":"section-54","l":"‎"},{"l":"Instructions:","p":["Go to the TTS tab.","‎"]},{"i":"section-55","l":"‎","p":["If you want to use an RVC model, download it, go to TTS, click Refresh& select it in Voice Model& Index File.","‎‎","To modify the inference settings or the output folder for the TTS/RVC audio, unfold Advanced Settings."]},{"i":"section-56","l":"‎","p":["In TTS Voices select the voice of your desired language, accent & gender.","In Text to Synthesize input your text. Then click Convert.","‎‎","If you are using an RVC model, select a voice that matches the model the most, to guarantee great results."]},{"i":"section-57","l":"‎","p":["Once it's done, you'll be able to hear the result in the Export Audio box. By default, the output audio will be in the \" audios\" folder. < \\ApplioV3.0.7\\assets\\audios>"]},{"i":"section-58","l":"‎"},{"i":"section-59","l":"‎"},{"i":"section-60","l":"‎","p":["To Update Applio, you need to firstly Save your audios and models, then Delete the current Applio folder and reinstall the latest version."]},{"i":"section-61","l":"‎"},{"i":"section-62","l":"‎"},{"i":"section-63","l":"‎","p":["Applio has an Extra menu, containing an audio analyzer.","Making it convenient for determining the sample rate of datasets when training models.","It also contains the model fusion tool, ideal for advanced users."]},{"i":"section-64","l":"‎"},{"l":"Audio Analyzer:","p":["Go to the Extra tab & press the upload box to input your audio. Or simply drag & drop.","‎"]},{"i":"section-65","l":"‎","p":["Once it's done uploading, click Get information about the audio."]},{"i":"section-66","l":"‎","p":["In Sampling rate you'll see the audio's full sample rate. Use said value for training."]},{"i":"section-67","l":"‎","p":["If the frequencies don't reach the top of the spectrogram, see at which number peaks & multiply it by 2."]},{"i":"section-68","l":"‎","p":["‎‎","Here it reached 20 kHz. Doubling it gives 40kHz. Therefore the ideal target sample rate would be 40k"]},{"i":"section-69","l":"‎"},{"i":"section-70","l":"‎","p":["Plugins are components that you can add to Applio, that add new features & enhance your experience.","These are made by the public, and are free & easy to install.","You can find them on their GitHub page. More will be added in the future."]},{"i":"section-71","l":"‎"},{"l":"Installation:","p":["Access their GitHub page & click on the name of the plugin you want.","‎"]},{"i":"section-72","l":"‎","p":["Click on the ZIP file.","‎‎","Click on the download button on the right. This will download the ZIP file of the plugin."]},{"i":"section-73","l":"‎","p":["Open Applio & head over to the Plugins tab. Drag & drop the ZIP file to the upload box.","‎‎","You will be able to see its installation process in the console."]},{"i":"section-74","l":"‎","p":["Go to the settings tab & click Restart Applio at the bottom. Then you'll be able to see the plugin in the Plugins tab."]},{"i":"section-75","l":"‎"},{"i":"section-76","l":"‎"},{"i":"section-77","l":"‎"},{"l":"*There's no option for my sample rate.*","p":["If it's lower than 32k: select 32k.‎","If it's 44.1k: select 40k.‎","If i'ts higher than 48k: select 48k."]},{"l":"*The voice glitches out.*","p":["This a phenomenon called artifacting. To fix it, read here."]},{"l":"*I couldn't find my answer.*","p":["Report your issue here."]},{"i":"section-81","l":"‎"}],[{"l":"Mainline","p":["Last update: August 3, 2025"]},{"i":"section","l":"‎","p":["Mainline RVC is the base, original, & unmodified official version of RVC. Made by the RVC-Project team. It can be called either Original/Mainline RVC.","It has less features compared to other forks, but still has the necessary tools to do a decent job.","It's specially liked because it's a little faster than other forks, as it's less bloated in a way.","Its actual name is not \"Mainline\", but it was given by the public to properly distinguish it from the other versions.‎"]},{"l":"Pros & Cons"},{"l":"***Unfold***","p":["Easy to install.","Simpler to use.","Doesn't have an active development.","Has less features.","Doesn't include Mangio-Crepe.","Manual model upload.","Won't work for RTX 50 Series GPUs."]},{"i":"section-1","l":"‎"},{"i":"section-2","l":"‎","p":["Go to their download page here","Click the Download word. RVC will begin to download.‎‎","Once it's done, unzip the folder.","Open RVC's folder, find the \" go-web.bat\" file and execute it.‎‎ It will then open a console, & after a moment your default web browser with RVC ready to be used.‎‎‎‎","(Optional) To access RVC more easily, make a shortcut of the go-web file.‎"]},{"i":"section-3","l":"‎","p":["If you encounter an issue, be sure to read the Troubleshooting chapter."]},{"i":"section-4","l":"‎"},{"l":"1. Upload voice model.","p":["Open RVC's folder, go to the assets folder and put your model's .PTH file inside the weights folder.‎‎","Return to the previous folder & put the model's .INDEX file in the logs folder."]},{"i":"section-5","l":"‎"},{"l":"2. Select voice model.","p":["In RVC, click the Refresh voice list and index path button.","‎","In its left, click Inferencing voice& select your model."]},{"i":"section-6","l":"‎"},{"l":"3. Select vocals.","p":["In Enter the path of the audio file paste the path file of your audio. Ensure the path doesn't include spaces or special characters."]},{"i":"section-7","l":"‎"},{"l":"4. Modify settings. (optional)","p":["If you wish, modify the inference settings on display accordingly for better results."]},{"i":"section-8","l":"‎"},{"l":"5. Convert.","p":["Click the long Convert button at the bottom & it will begin to convert.","The processing time will mainly depend on your specs, length of audio, & the algorithm picked."]},{"i":"section-9","l":"‎"},{"l":"6. Download output.","p":["Once it's done processing, a playable audio will pop up in the Export audio box. To download, click the three dots on the right & hit Download.","‎"]},{"i":"section-10","l":"‎","p":["The training guide will be centered around using TensorBoard. Read about it first if you haven't already.","If you encounter an issue, be sure to read the Troubleshooting chapter."]},{"i":"section-12","l":"‎"},{"i":"section-13","l":"‎"},{"l":"1. Go to training area.","p":["Open RVC & head over to the Train tab."]},{"i":"section-14","l":"‎"},{"l":"2. Name the model.","p":["In Enter the experiment name you insert a name for your model. Don't include special characters or spaces."]},{"i":"section-15","l":"‎"},{"l":"3. Select Target Sample Rate.","p":["In Target sample rate select the number that matches your datasets' sample rate. Inputting an incorrect one might screw up the final quality."]},{"i":"section-16","l":"‎"},{"i":"section-17","l":"‎"},{"l":"4. Select dataset.","p":["In Enter the path of the training folder paste the path file of your dataset. Ensure the path doesn't include special characters/spaces.","‎","If there's any text in the bar, delete it beforehand."]},{"i":"section-18","l":"‎"},{"l":"5. Process data.","p":["Click the Process Data button on the center.","RVC will process the previous criteria for the training. But also the dataset file, which might take a moment depending on how big it is.","‎","It'll finish when the output box on the right says end preprocess."]},{"i":"section-19","l":"‎"},{"i":"section-20","l":"‎"},{"l":"6. Select GPUs.","p":["In Enter the GPU index(es) determine which GPU(s) you'll use for training, by indicating the index followed by the dash (e.g: 0)."]},{"i":"section-21","l":"‎"},{"l":"7. Select pitch extraction algorithm.","p":["At the right select the Pitch extraction algorithm. Only use RMVPE_GPU or Crepe, as the rest are obsolete.","‎"]},{"i":"section-22","l":"‎","p":["Now click the Feature extraction button on the right.","‎‎‎ It'll finish when the output says all-feature-done."]},{"i":"section-23","l":"‎"},{"l":"8. Create .INDEX.","p":["Press Train feature index at the bottom center. This will create the .INDEX file.","‎‎ It'll finish when the output box says something like this:"]},{"i":"section-24","l":"‎"},{"i":"section-25","l":"‎"},{"l":"9. Select save frequency.","p":["Frequency of the saving checkpoints, based on the epochs.","If you are a newbie, simply leave it at 15.","E.g: with a value of 10, they will be saved after the epoch 10, 20, 30, etc.","‎"]},{"i":"section-26","l":"‎"},{"l":"10. Input epochs amount.","p":["In Total training epochs you determine the total amount of epochs(training cycles) for the model.","But since we'll use TensorBoard, use an arbitrarily large value like 2000."]},{"i":"section-27","l":"‎"},{"l":"11. Select batch size.","p":["Leave Batch size per GPU at 8 if you aren't familiar with it.","If your dataset is short (around 2 minutes or less), use 4 instead."]},{"i":"section-28","l":"‎"},{"l":"12. Launch TensorBoard.","p":["Now before you start training, open TB.","If you haven't already, start reading about it here here."]},{"i":"section-29","l":"‎"},{"l":"13. Begin training.","p":["Start training the model by clicking Train model.","‎‎‎ Remember to monitor TB, & also the console just in case. The latter will show you errors if they happen, and information about the epochs & checkpoints.‎"]},{"i":"section-30","l":"‎"},{"l":"14. Stop training.","p":["When you are very sure of overtraining, you can stop training by pressing the Stop training button where Train model used to be."]},{"i":"section-31","l":"‎"},{"l":"15. Gather model's files.","p":["Create a new folder anywhere named as your model.","Open RVC's folder, go to logs, and open the folder named with the model. Select the .INDEX named added_& move it to your newly made folder.","‎","Now go to the weights folder. Here you'll find the model's checkpoints.","Select the one closest to before the overtraining point, and move it to the new folder","These files will be organized with this format: ModelName_Epoch_Step.pth Example: kalomaze_e60_s120.pth","‎‎","And that's all. Have fun with your model. To test the model, do a normal inference as usual."]},{"i":"section-32","l":"‎"},{"i":"section-33","l":"‎","p":["If the training finished but the model still needed training, you don't have to start from scratch. Follow this procedure:","Simply enter the same settings and criteria that you previously inserted. Model name, sample rate, dataset, batch size, etc. You don't have to press Process Data or train the .INDEX again.","You can change the save frequency, or increase the epochs amount in case you didn't input enough before.","Begin training again & remember to monitor TB & console like before."]},{"i":"section-34","l":"‎"},{"i":"section-35","l":"‎"},{"l":"*There's no option for my sample rate.*","p":["If it's lower than 32k: select 32k.‎","If it's 32k: on the right in Version, press v1& press v2 again. Ensure you leave it as v2. You should be able to see a 32k option now.‎","If it's 44.1k: select 40k.‎","If i'ts higher than 48k: select 48k."]},{"l":"*The voice glitches out.*","p":["This a phenomenon called artifacting. To fix it, read here."]},{"l":"*I don't see the Stop Training button.*","p":["This is a common bug. Close the console to stop RVC entirely."]},{"l":"*I couldn't find my answer.*","p":["Report your issue here."]},{"i":"section-39","l":"‎"}],[{"l":"AICoverMaker","p":["Last update: August 3, 2025"]},{"i":"section","l":"‎","p":["AICoverMaker (or known as RVC-AI-Cover-Maker-WebUI) is an Applio RVC Fork developed by the Eddy, as a better and updated version of the old AICoverGen.","It's liked for its great UI& Automated AI Cover Process, making it the easiest way to make AI Covers, as it automatically separates instrumentals & vocals, and mixes them back with the converted vocals."]},{"l":"Are RVC Models Safe?","p":["RVC Models are PyTorch Models, a Python library used for AI. PyTorch uses serialization via Pythons' Pickle Module, converting the model to a file. Since pickle can execute arbitrary code when loading a model, it could be theoretically used for malware, but this fork has a built-in feature to prevent code execution along the model. Also, HuggingFace has a Security Scanner which scans for any unsafe pickle exploits and uses also ClamAV for scanning dangerous files.","‎"]},{"l":"Pros & Cons"},{"l":"*Learn more*","p":["Automatically separates instrumentals and mixes them with converted ones","Currently stable","Faster interface","Automatic model upload","Has Mangio-Crepe","User-friendly UI","Can't Train models","No Precompiled versions for Non-Windows Users","Doesn't support Mac nor any NON-Nvidia GPUs"]},{"l":"System & Hardware Requirements","p":["SPEC","MINIMUM REQUIREMENT","OS","Windows 10 or later / Any Modern Linux Distro","RAM","6GB","Storage","6 GB","SUGGESTED REQUIREMENT","GPU","NVIDIA RTX 20 Series or later","6GB+","In case you don't meet the requirements to run it locally, it also has a 2 Cloud Versions: Kaggle & Colab"]},{"i":"section-1","l":"‎"},{"i":"section-2","l":"‎","p":["Don't put the folder in a directory with privileged access (like C:\\Program Files).","Make sure the file path does not contain any spaces or special characters.","It's recommended to temporarily deactivate your antivirus and firewall to avoid missing dependencies during installation."]},{"l":"Precompiled (Windows)","p":["The easiest way to download RVC-AI-Cover-Maker-WebUI is by going to Eddy's Latest GitHub Release, and clicking the Precompiled version.","Unzip the folder. It may take a few minutes.","Open the AICoverMaker folder & execute run.bat.","‎‎","A console tab will appear, and after a moment your default browser will open with the WebUI ready to use."]},{"l":"Source / Manual (mainly for Linux)","p":["‎","Download the source code, either .zip (which is the most suggested usually) or .tar.gz, from the latest release link.","‎‎","If you download the .zip from the latest release make sure to rename the folder from rvc-ai-cover-maker-ui-v1.0.5(or whatever version is the latest version) to just rvc-ai-cover-maker-ui otherwise you may run into missing dependencies issues.","Extract the folder. It may take a few minutes.","Open the AICoverMaker folder & execute the script run.sh for Linux, or run.bat for Windows.","A console tab will appear, and after a moment your default browser will open with the WebUI ready to use."]},{"i":"section-3","l":"‎"},{"i":"section-4","l":"‎"},{"l":"Music Download","p":["1. When in the UI look at the top left and look for the tab named Download Music and click it.","‎","2. Then put the song you want to download in the text box and click download."]},{"l":"Model Download","p":["1. In the UI look at the top left and look for the tab named Download Model and click it.","‎","2. Then put the model you want to download in the text box and click download.","3. You can also drag and drop your model in the Drop files box to upload them directly."]},{"i":"section-5","l":"‎"},{"i":"section-6","l":"‎","p":["Please use our Inference Settings guide to find out the inference settings do what.","TTA- results in longer separation time, it gives a little better SDR score but hard to tell if it's really audible in most cases\". it “means \"test time augmentation\", it will do 3 passes on the audio file instead of 1. 1 pass with be with original audio. 1 will be with inverted stereo (L becomes R, R become L). 1 will be with phase inverted and then results are averaged for final output."]},{"i":"section-7","l":"‎"},{"i":"section-8","l":"‎","p":["To Update AICoverMaker, you can either:","Open AICoverMaker's folder & execute the script update.sh for Linux, or update.bat for Windows.","Download the latest precompiled the next time a new version comes out and replace the files."]},{"i":"section-9","l":"‎"}],[{"l":"Weights.com","p":["Last update: August 3, 2025","Weights.com is an easy, freemium AI Cloud Website platform that provides tools for creating AI voice covers, text-to-speech, and more. It serves as an all-in-one web UI, eliminating the need for powerful local hardware or complex setups.","The platform is designed to be user-friendly, catering to both beginners and experienced users. You can train your own custom voice models or use a vast library of high-quality, community-made models, from both the AI Hub Discord& the Weights.com Models Page.","Weights is partnered with AI HUB& DreamTavern.","It's named Weights, as it was originally meant only for RVC with the domain Weights.gg, storing RVC Models, which are PyTorch Weights. Later on, the domain changed to Weights.com and expanded to other things, but for this specific guide, we will focus on the RVC-related side."]},{"l":"Pros & Cons"},{"l":"*Learn more*","p":["All-in-one web cloud platform.","No need for a powerful PC.","Mobile friendly, with even an app.","User-friendly interface suitable for beginners.","Large library of public RVC voice models (anime, singers, etc.).","Free tier available for basic usage.","Integrated TTS and direct recording options.","Active community for sharing and collaboration.","Free plan has limitations on creations and features.","Processing and training times could be slow due to queues in certain, but it's better on Paid Tiers.","Less granular advanced control compared to forks like Applio or Mainline/Original RVC, Because it's meant for everyone and not just tech experts."]},{"l":"Training Disclaimer","p":["Before you train your first model, Weights will present you with a disclaimer that you must accept. This ensures that the tool is used ethically and legally.","Never use the voice of someone without their consent.","Don’t use voices for fraud, scams, or bullying.","Protected intellectual property rights must be respected.","Voices should be used for good, not for harm. Have fun!","You can review the full legal documents here:","Terms of Service","End-User License Agreement (EULA)"]},{"l":"Instructions:","p":["1. Navigate to the Train Model tab from the main menu. You will be presented with the \"New Voice Model\" page.","2. Fill in the Model Details:","Select or drop image: Upload a picture to serve as the thumbnail for your model.","Model Name: Assign a clear and descriptive name.","Model Description: Add optional details about the voice or its intended use.","Private Model: Enable this option if you want the model to be visible only to you. Premium only","3. Upload your Input Audio:","This section is for your dataset, the collection of voice recordings the AI will learn from.","You can drag and drop audio files directly. Minimum 3 minutes, It is recommended to upload at least 5-10+ minutes of clean audio with minimal background noise for good results.","Premium users can train longer datasets, up to 30 minutes.","Weights will automatically clean your dataset.","4. Start the training process by clicking Start Training.","Depending on your subscription, you may see different options, such as \"Train and Publish\" or \"Use Premium Training Item\".","Learn more about Free Premium Items via Streaks.","Once you have a model, you can use it to generate voice covers. This process is also known as inference."]},{"i":"instructions-1","l":"Instructions:","p":["1. Choose a Voice to Use","You can select one of your own trained models or browse the extensive public library of Voice Models.","Use the search bar to find specific characters or artists.","2. Choose your Input Audio","You have three main options for providing the audio to be converted:","Upload an audio file from your device. This is the most common method for creating song covers.","Write text directly into the provided box. Since RVC is Speech-To-Speech Natively, You will need to select a base TTS voice (like \"John\") which will be used as an input for RVC Model.","Record audio directly through your microphone, which is useful for quick tests or generating voice lines."]},{"l":"3. (Optional) Choose Your Settings","p":["Before creating the cover, you can adujst the output with several settings."]},{"l":"Basic Settings","p":["Pre-Stemmed: Turn this on if your uploaded audio file contains only vocals without any instrumental backing.","Pitch: Adjust the pitch of the output voice. A positive value makes it higher, while a negative value makes it lower."]},{"l":"Advanced Settings","p":["De-Echo & Reverb: Removes echo and reverb from the source vocal track for a cleaner conversion.","Isolate Main Singer: Attempts to convert only the lead vocals, ignoring background singers.","Instrumental Pitch: Changes the pitch of the instrumental track separately from the vocals.","Volume Envelope: Lower values make the output volume closer to the original vocal's volume dynamics.","Consonant Protection: Helps reduce audio artifacts and slurring on consonants, especially at lower volumes."]},{"l":"4. Create and Download","p":["Click the Create button to start the conversion process.","Your request will be added to a queue. You can view its status on the My Creations page, where it will show as \"Processing\".","Once finished, you can play the result directly and download it to your device.","Weights.com operates on a freemium model. It offers a Free Plan with core features, alongside paid Basic and Pro plans for users who need more and better resources.","Subscriptions increase the number of daily covers, queued items, and weekly voice trainings, while also allowing for longer maximum audio lengths.","Limits and features are subject to change. Always check the Weights.com Official Pricing Page for the most current information.","Weights.com rewards users for daily activity through a Streak system.","To keep your streak going, you must create at least one item (Voice Cover, Image, or Chat) each day before the streak resets.","As a reward for consistency, you will earn a Free Premium Training every 5 days you maintain your streak. This allows you to train more RVC voice models with a bigger dataset without a paid subscription."]}],[{"l":"Applio Kaggle","p":["Last update: August 7, 2025"]},{"i":"section","l":"‎","p":["This is a cloud-based alternative to run Applio, RVC Fork, only for people who don't have a good PC GPU, via the Kaggle Service. With a Web User Interface.","Check the Kaggle Glossary for more info on Free Tier, Limits, Verification, Pricing and other things."]},{"l":"Pros & Cons"},{"l":"***Learn more***","p":["Has good GPU's","Has 30 GPU hours","Fast","TensorBoard included","You can leave training unsupervised.","Takes some time to set up."]},{"i":"section-1","l":"‎"},{"l":"Create an Account"},{"i":"section-2","l":"‎"},{"l":"1. Set up account.","p":["Start by making an account here.‎","‎","Verify your acount with a phone number so you can turn on the \"internet\" option."]},{"i":"section-3","l":"‎"},{"l":"Notebook Creation & Setup"},{"i":"section-4","l":"‎"},{"l":"2. Clone Notebook","p":["Go to Kaggle and click \"Create\" then \"New Notebook\" at the top left.","‎","Under your session's name click \"File\" then \"Import Notebook\".","On the new window that appeared on the right click \"Link\" then type in the box this link https://github.com/IAHispano/Applio/blob/main/assets/Applio_Kaggle.ipynb.","Click \"Import\" on the bottom right once you've done this.","When it's done importing it will display this text window.","Under \"Session options\" in the sidebar turn on \"internet\". Make sure persistance is on for both files and varibles.","Turn on T4 X2 GPUs in accelerator.","‎ g: (Optional) Turn on headless mode so you can run so you can run the GPU on all sessions and save your progress. Go to the top right and click \"Save version\" then open the advanced dropdown.","Your runtime will continue draining when you're not running any cells with this option on."]},{"i":"section-5","l":"‎"},{"l":"Ngrok Setup"},{"i":"section-6","l":"‎"},{"l":"3. Ngrok Setup","p":["Scroll down to the fifth cell and you should see a section where you put your ngrok token. If you dont have a ngrok acount sign up here. a2. Once you have an acount you can authenticate your ngrok tunnel agent here: https://dashboard.ngrok.com/get-started/your-authtoken‎","put the Ngrok token in the second cell like so:","Once the Ngrok token is there run the cell.","There is a monthly limit rate with Ngrok so dont be supprised if training is suddenly interrupted."]},{"i":"section-7","l":"‎"},{"l":"Installation"},{"i":"section-8","l":"‎"},{"l":"4. Installation Cells","p":["Starting from the top run all the cells, with the first being:","a2. When it's done it will output Finished.","Now run the last cell which is:"]},{"i":"section-9","l":"‎"},{"l":"Using Applio"},{"i":"section-10","l":"‎"},{"l":"5. Ngrok Links","p":["Click on the Applio URL link to open Applio's UI, click the Tensorboard Url link to open the Tensorboard and click File Url to open the file manager.","‎","Once you've click the Applio Url it will take you to Applio's UI where it operates the same as normal Applio. If you happen to not know how to use Applio you can read about it in the Local Applio Docs, it operates similarly."]},{"i":"---","l":"‎ ‎"}],[{"l":"Applio Colab","p":["Last update: August 7, 2025","This is a cloud-based alternative to run Applio, RVC Fork, only for people who don't have a good PC GPU, via the Google Colab Service. With a Web User Interface.","Check the Google Colab Glossary for more info on Free Tier, Limits, Verification, Pricing and other things."]},{"l":"Pros & Cons"},{"l":"*Learn more*","p":["Very complete","Has an active development","Currently stable","Very fast","TTS features","Automatic model upload","Has (not Mangio) Crepe","User-friendly UI","TensorBoard included","Extra features: (plugins, model fusion, etc)","Usage limit for free users"]},{"i":"section","l":"‎"},{"i":"section-1","l":"‎","p":["Access the Colab space here. Then log in to your Google account."]},{"i":"section-2","l":"‎","p":["Execute the Install Applio cell. This will take around 2 minutes.","‎‎","It'll finish when you see a tick symbol on the left."]},{"i":"section-3","l":"‎","p":["If you are going to train models, upload your dataset to your Google Drive storage & run the Extra cell.","‎‎","To save time, unfold it & cancel the custom pretrain download, if you aren't going to use them."]},{"i":"section-4","l":"‎","p":["Grant the permissions to Google Drive.‎‎"]},{"i":"section-5","l":"‎","p":["Select your sharing method then execute Start Applio.","‎‎","Then open the public URL.","If you select ngrok put your ngrok token, which can be found here in the text box."]},{"i":"section-6","l":"‎","p":["Be sure to read the Troubleshooting chapter if any issue arises."]},{"i":"section-7","l":"‎"},{"l":"1. Upload voice model.","p":["Go to the Download tab. You have two ways of uploading it: through its link or manually inputting its files.","Go to the Download tab & paste the link in the Model Link bar. It must be from Hugging Face or Google Drive.‎‎","Press Download Model.","Below in Drop files, press the upload box & input the model's .PTH.‎‎","Then input the .INDEX.","‎"]},{"l":"2. Select voice model.","p":["Return to the Inference tab & click Refresh on the right.","‎","Select the model in the Voice Model& Index File dropdown."]},{"i":"section-8","l":"‎"},{"l":"3. Input vocals.","p":["With Applio you can convert audios individually or in batches:","Press the upload box & input your audio.‎‎‎‎","Then select it in the dropdown below.‎","Go to the Batch tab.‎‎‎","Go to the file explorer in Colab. Go to drive, right-click the folder containing the audios & click Copy Path.","Paste the path in the Input Folder bar.","In Output Folder you can define the path folder for the results.","Ensure the paths don't contain spaces/special characters.","‎"]},{"l":"4. Modify settings. (optional)","p":["Unfold Advanced Settings if you wish to modify the inference settings for better results.","‎"]},{"i":"section-9","l":"‎"},{"l":"5. Convert.","p":["Click Convert at the bottom to process the audio.","Once it's done, you can hear the results in the Export Audio box below. To download it, press the download symbol on its right.","‎"]},{"i":"section-10","l":"‎"},{"i":"section-11","l":"‎"},{"l":"1. PREPROCESS","p":["Go to the Train tab. Input a name for your model in Model Name. Don't include spaces/special characters.","‎","Upload your dataset to your GD storage if you haven't already.‎","In Colab click the folder on the left ( ) & click the reload button.‎‎‎(For mobile users: tap the three lines on the top left & Show file browser)‎","Open drive, localize your dataset, right-click it & click Copy path.‎‎‎‎","Then paste it on the Dataset Path bar.‎‎","Select your dataset's sample rate. If you don't know the amount, click here.","Ensure RVC Version is set as V2& click Preprocess Dataset.","It'll finish when the output box says preprocessed successfully."]},{"l":"2. EXTRACT","p":["Select the algorithm you want. Use either Crepe or RMVPE, as the rest are outdated.","Press Extract Features. It'll finish when it says extracted successfully."]},{"l":"3. TRAIN","p":["‎‎‎","‎","But since we'll use TensorBoard, use an arbitrarily large value like 1000","Click Generate Index. This will create the model's .INDEX file.","Disconnect from your Internet.","Don't solve the captchas that (might) pop up occasionally.","E.g: with a value of 10, they will be saved after the epoch 10, 20, 30, etc.","Frequency of the saving checkpoints, based on the epochs.‎","If after around 2:30 hours of training you don't detect OT download the model of the lowest point, in case it's already OT, and the .INDEX.‎","If you are a newbie, simply leave it at 15.","If you are a newbie, use 8. But in case your dataset is short (around 2 minutes or less), use 4.","Input the total amount of epochs(training cycles) for the model.‎","Press Start Training below to begin the training process.‎‎","Run out of GPU runtime.","Stay AFK for a long time.","TB will be available in the Colab. Remember to monitor it, as well as the cell's logs just in case.","The latter will show you errors if they happen, and information about the epochs & checkpoints.‎‎‎‎","Then once your GPU runtime resets, begin the retraining procedure.","Tick Save Only Latest"]},{"l":"4. DOWNLOAD","p":["When you're very sure of overtraining, you can stop training by going to the Settings tab & press Restart Applio.","‎‎","Come back to the Colab & open the new public URL.","Open the file explorer, go to logs, and open the folder named as the model.‎","Download the .INDEX named added_.‎‎‎","In said folder you'll also find all the checkpoints.‎","Select the one closest to before the overtraining point, and move it to the new folder.","The checkpoints will be organized with this format: ModelName_Epoch.pth Example: arianagrande_60e.pth‎‎‎‎","You can determine the Step number of the checkpoints by looking at its epoch number on the logs.","‎‎‎","And that's all, have fun with your model. To test it, do a normal inference as usual."]},{"l":"5. RESUMING","p":["In case the training finished but the model still needed training, you don't have to start from scratch.","Simply enter the same settings & criteria that you had previously inserted. You don't have to do preprocess, extract feature or train the .INDEX again.‎","You can change the save frequency or increase the Total Epoch amount, in case you didn't input enough before.‎","If you're resuming from a new session, unfold the Extra cell in Colab & input the model name you assigned before.‎‎‎","For this, the Auto Backup cell must've ran in the previous session.‎‎","Begin training again & remember to monitor [TB]https://docs.aihub.gg/rvc/resources/training/#tensorboard) as before."]},{"i":"section-44","l":"‎"},{"i":"section-45","l":"‎","p":["+ with any RVC model"]},{"i":"section-46","l":"‎","p":["Applio is also known for having one TTS tool by default, with plenty of voices to choose for.","You can also use it with RVC models& apply the inference settings if you wish.","Additionally, you can download the Eleven Labs TTS plugin."]},{"i":"section-47","l":"‎"},{"l":"Instructions:","p":["Go to the TTS tab.","‎"]},{"i":"section-48","l":"‎","p":["If you want to use an RVC model, download it, go to TTS, click Refresh& select it in Voice Model& Index File.","‎‎","To modify the inference settings or the output folder for the TTS/RVC audio, unfold Advanced Settings."]},{"i":"section-49","l":"‎","p":["In TTS Voices select the voice of your desired language, accent & gender.","In Text to Synthesize input your text. Then click Convert.","‎‎","If you are using an RVC model, select a voice that matches the model the most, to guarantee great results."]},{"i":"section-50","l":"‎","p":["Once it's done, you'll be able to hear the result in the Export Audio box. To download it, click the download button on its right ( )."]},{"i":"section-51","l":"‎"},{"i":"section-52","l":"‎"},{"i":"section-53","l":"‎","p":["Applio has an Extra menu, containing an audio analyzer, F0 Curve and Model Information.","Making it convenient for determining the sample rate of datasets when training models.","It also contains the model fusion tool, ideal for advanced users."]},{"i":"section-54","l":"‎"},{"l":"Audio Analyzer:","p":["Go to the Extra tab & press the upload box to input your audio.","‎"]},{"i":"section-55","l":"‎","p":["Once it's done uploading, click Get information about the audio."]},{"i":"section-56","l":"‎","p":["In Sampling rate you'll see the audio's full sample rate. Use said value for training."]},{"i":"section-57","l":"‎","p":["If the frequencies don't reach the top of the spectrogram, see at which number peaks & multiply it by 2."]},{"i":"section-58","l":"‎","p":["‎‎","Here it reached 20 kHz. Doubling it gives 40kHz. Therefore the ideal target sample rate would be 40k"]},{"i":"section-59","l":"‎"},{"i":"section-60","l":"‎","p":["Plugins are components that you can add to Applio, that add new features & enhance your experience.","These are made by the public, and are free & easy to install.","You can find them on their GitHub page. More will be added in the future."]},{"i":"section-61","l":"‎"},{"l":"Installation:","p":["Access their GitHub page & click on the name of the plugin you want.","‎"]},{"i":"section-62","l":"‎","p":["Click on the ZIP file.","‎‎","Click on the download button on the right. This will download the ZIP file of the plugin."]},{"i":"section-63","l":"‎","p":["Open Applio & head over to the Plugins tab. Press the upload box & upload the ZIP."]},{"i":"section-64","l":"‎","p":["Go to the Settings tab & click Restart Applio at the bottom. Go back to the Colab & open the new public URL.","Then you'll be able to see the plugin in the Plugins tab."]},{"i":"section-65","l":"‎"},{"i":"section-66","l":"‎"},{"i":"section-67","l":"‎"},{"l":"*There's no public URL.*","p":["In case the public URL doesn't show up, there might be a problem with Gradio, you can check if it's down here.","To fix this, instead of waiting until Gradio is back online, just check the share_tunnel* checkbox on the Start Applio cell.","‎","Applio will use localtunnel instead of the Gradio Public Share Link now, copy paste the Password IP(Don't worry, it's the Google PC's IP, not yours).","Then open the Share Link given by the colab and paste the \"Password IP\" in \"Tunnel Password\", finally click Submit."]},{"l":"*There's no option for my sample rate.*","p":["If it's lower than 32k: select 32k.‎","If it's 44.1k: select 40k.‎","If i'ts higher than 48k: select 48k."]},{"l":"*The voice glitches out.*","p":["This a phenomenon called artifacting. To fix it, read here."]},{"l":"*Cannot connect to GPU backend.*","p":["You have exhausted the GPU runtime of Colab."]},{"l":"*I couldn't find my answer.*","p":["Report your issue here."]},{"i":"section-73","l":"‎"}],[{"l":"Applio Lightning Ai","p":["Last update: August 8, 2025"]},{"l":"Introduction","p":["This is a cloud-based alternative to run Applio, RVC Fork, only for people who don't have a good PC GPU, via the Lightning.AI Service. With a Web User Interface.","It's a great alternative for training RVC voice models through the cloud, since it has the best GPUs with tons of VRAM.","Check the Lightning.AI Glossary for more info on Free Tier, Limits, Verification, Pricing and other things."]},{"l":"Pros & Cons"},{"l":"***Learn more***","p":["Has good GPU's.","Has lots of VRAM","TensorBoard included.","You can leave training unsupervised.","Takes some time to set up.","Needs a phone number.","Low/Decent GPU time depending on what GPU you choose.","2-3 Day verification wait time."]},{"l":"Create an Account"},{"l":"1. Set up account.","p":["First make an account with Lightning Ai","Make sure you verify yourself with a phone number. Once you've done that you will get an email that looks like this:","You will need to wait 2-3 business days to become fully verified","Once you are verified Lightning Ai will send you a email that conatins this:"]},{"l":"Studio Setup & Installation"},{"l":"2. Access the Notebook","p":["After creating your Lightning.AI account, open the Applio Notebook and Clone it."]},{"l":"3. Activate/Switch GPU","p":["If you aren't on a GPU environment by default, you must switch to a GPU environment. This is crucial for performance.","On the right-hand lateral menu, click on Studio Environment(the processor icon).","Click Switch To GPU, select an available GPU, and wait for the environment to restart.","75 hours monthly of T4 16gb","31 hours monthly of L4 24gb","15 hours monthly of L40 48gb"]},{"l":"4. Clone Repository and Install Dependencies","p":["Run the first code cell. This will download the latest version of the realtime voice changer and install necessary dependencies.","This step may take a few minutes to complete. It will print \"Installed!\" when finished."]},{"l":"Tunnels"},{"l":"5. Launch the Server via Tunnels","p":["(Optional) To potentially reduce latency, select a geographical Region from the list of parameter options next to it, that is closest to you.","After configuring your chosen tunnel, run the cell. The first time you run it, it might download the necessary files, which might take a minute or two.","Click on Port Viewer and then click Add a new port.","Click the + at the bottom of the right tab, click on Web Apps and install Port Viewer.","Click the URL. A new page will ask for a password.","Click your Port in Port Viewer, you can also click Open to open it in an external tab.","Cloudflare (Easy, No Account Needed)","Copy the password from the notebook output and paste it into the password prompt in your browser to access the voice changer.","Enter 18888 as the Port Number and optionally give it a name (e.g., \"Voice Changer\").","Go to the Horizon Dashboard and sign up. On the second step of the setup, you will see a command like hrzn login YOUR_ID. Copy that YOUR_ID part.","Go to the Ngrok Dashboard to get your free authtoken.","Horizon (Fast, Requires Account & ID)","How it works: Horizon is another tunneling service that requires a free account and a personal ID for authentication.","How it works: LocalTunnel is another free service that doesn't require an account. For security, it generates a unique URL that is protected by a password.","How it works: Ngrok is a popular service that creates secure tunnels. It requires a free account and an authentication token. It has a 1GB Bandwidth Free Monthly Limit https://ngrok.com/docs/pricing-limits/free-plan-limits/.","How it works: This is a built-in Lightning.AI feature. It's the most straightforward method as it doesn't require any external accounts or tokens.","How it works: This option uses Cloudflare's free trycloudflare service. It's very easy to use as it requires no accounts or tokens.","In the notebook cell, paste this ID into the Token field.","In the notebook cell, paste your token into the Token field, replacing 'Ngrok | Horizon TOKEN'.","In the right-hand sidebar of the Lightning.AI interface, click the Web Apps tab.","LocalTunnel (No Account, Password Protected)","Navigate to the third code cell, titled \"Start Server using Tunnels\". This cell boots up the Wokada Deiteris Fork application inside your Lightning.AI Studio.","Ngrok (Fast, Popular & Reliable)","Once the setup is complete, the output will display a message with your public URL. Click this link to open the Applio interface and start using the program.","Port Viewer (Recommended & Default method)","Run the cell.","Run the cell. The first time you use it, the output may ask you to authorize the connection by clicking a link ( https://hrzn.run/dashboard/settings/cli-token-requests/...). Click this link and approve the request in your Horizon dashboard.","Run the cell. The public Ngrok URL (ending in ngrok.io) will be printed in the output once the server is ready. Click on it to access the UI.","Run the cell. The script will automatically download the necessary tools. After a few moments, a public URL (ending in trycloudflare.com) will be printed in the output. Click it to open the interface.","Run the code cell. Wait for the output to show that the server is listening.","Select \"Cloudflare\" from the Tunnel code.","Select \"LocalTunnel\" from the Tunnel code.","Select \"Port Viewer\" from the Tunnel code.","Select a Tunnel: A tunnel securely exposes the application running in your private cloud environment to the public internet. The notebook gives you five different services to do this. Choose one from the Tunnel code menu in the code cell.","Steps:","The output will display two key pieces of information: the public URL (ending in loca.lt) and a Local Tunnel Password below it.","The public Horizon URL (ending in hrzn.run) will then be printed in the output. Click it to access the UI.","The server runs in the foreground. If you stop the cell or close the Lightning.AI site, the server will shut down. Keep the cell running to use the program.","This final code cell is the most important one—it starts the voice changer's server and uses a \"tunneling\" service to create a secure, public web address (URL) for you to access it from your own computer.","You can optionally go back to the Jupyter session in the right-hand sidebar of the Lightning.AI interface, to check if any error appears in the code output."]},{"l":"Server Setup"},{"l":"6. Accessing Files.","p":["To upload a dataset, upload audio or anything else find the Teamspace Drive button on the right and click it.","‎","The path to Applio is Studio this_studio Applio Applio","Once you're there you can just drag and drop files.","To download files click on the file then click the three dots on the right of it and click download"]},{"l":"7. Opening the TensorBoard.","p":["Find the TensorBoard icon on the right side bar and click it.","Once you've clicked it, Start it.","Once you've done that it will open the TensorBoard. you can open it externally in another tab/window via clicking Open. To learn how to use it go here"]},{"l":"8. Opening the notebook.","p":["If you want to go back to the notebook simply click on the Jupyter icon on the right."]},{"l":"Usage","p":["Now that you have the web interface running via Lightning.AI, the rest of the process is identical to using a local installation.","For all subsequent steps, including application settings and model usage, please continue by following the Local PC guide."]},{"l":"Maintenance"},{"l":"Deleting Everything","p":["If you need to update Applio or start fresh, you can run the final cell in the notebook, \"Delete everything\". This will remove all downloaded files and configurations from your persistent storage, allowing for a clean installation by re-following the notebook with perhaps a changed branch variable."]},{"i":"---","l":"‎ ‎"}],[{"l":"Applio no UI Colab","p":["Last update: August 7, 2025","This is a cloud-based alternative to run Applio, RVC Fork, only for people who don't have a good PC GPU, via the Google Colab Service. Without a Web User Interface.","Check the Google Colab Glossary for more info on Free Tier, Limits, Verification, Pricing and other things."]},{"l":"Pros & Cons"},{"l":"*Learn more*","p":["Very complete","Has an active development","Currently stable","Very fast","TTS features","Automatic model upload","Has (not Mangio) Crepe","User-friendly UI","TensorBoard included","Extra features: (plugins, model fusion, etc)","Usage limit for free users"]},{"i":"section","l":"‎"},{"l":"Installation"},{"i":"section-1","l":"‎"},{"l":"1. Running cells.","p":["Start by accessing the colab here.","Then run the Installation cell to install all the requirements."]},{"i":"section-2","l":"‎"},{"l":"Training"},{"l":"2. Preprocess Dataset.","p":["Name your model whatever you want.","Then upload your dataset to your google drive.","Type in the path to your dataset into dataset_path.","Select your sample rate.","Run the cell."]},{"i":"section-3","l":"‎"},{"l":"3. Extract Features.","p":["Choose the f0 method you want, usually RMVPE is the best.","You can also change the Embedder Model.","Run the cell."]},{"i":"section-4","l":"‎"},{"l":"4. Training.","p":["Set the total number of epoch you want to train for.","Choose your batch size, 8 is the best for most cases.","Enable cleanup if this is your first time training a model and you're not resuming.","Set how many epochs you are going to save. If you want to get the best epoch set this to 1 but if you're ok with close enough you can set this to a higher number.","Turn on save_only_latest.","Run the cell to start training!"]},{"i":"section-5","l":"‎"},{"l":"5. Resuming Training.","p":["Set the model names to exactly what you had before.","Run the first cell.","Select your sampe rate and f0 method in the second cell.","Run the final cell.","Then run the training cell again."]},{"i":"---","l":"‎ ‎"}],[{"l":"AICoverMaker","p":["Last update: August 7, 2025","This is a cloud-based alternative to run AICoverMaker, Applio RVC Fork, only for people who don't have a good PC GPU, via the Google Colab Service& Kaggle Service. With a Web User Interface.","Check the Google Colab Glossary for more info on Free Tier, Limits, Verification, Pricing and other things."]},{"l":"Installation & Setup","p":["1. Go to the AICoverMaker Colab and run the first cell.","‎","Installation may take a couple of minutes, be patient.","2. Next go to the second cell and put your ngrok token in the text box and run it. If you dont have a ngrok acount sign up here and you can authenticate your ngrok tunnel agent here.","There is a monthly limit rate with Ngrok so dont be supprised if training is suddenly interrupted.","2a. You can choose not to use Ngrok if you desire to by running the third cell.","3. Once you've run either the cell with or without ngrok a link under the cell will apear, click it and it will take you the WebUI.","Check the Kaggle Glossary for more info on Free Tier, Limits, Verification, Pricing and other things."]},{"i":"installation--setup-1","l":"Installation & Setup","p":["1. Go to the AICoverMaker Kaggle notebook and click Copy Edit on the top right.","‎","2. Once you're in the notebook run the first installation cell. Once it's done it will output Requirements installed.","Installation may take a couple of minutes, be patient.","3. Next go to the second cell and put your Ngrok token in the TOKEN HERE spot and run it. If you dont have a Ngrok acount sign up here and you can authenticate your ngrok tunnel agent here.","There is a monthly limit rate with Ngrok so dont be supprised if training is suddenly interrupted.","3. Once you've run the final cell a ngrok link will apear under the cell, click it and it will take you the WebUI."]},{"l":"Downloading Music & Models"},{"l":"Music Download","p":["1. When in the UI look at the top left and look for the tab named Download Music and click it.","‎","2. Then put the song you want to download in the text box and click download."]},{"l":"Model Download","p":["1. In the UI look at the top left and look for the tab named Download Model and click it.","‎","2. Then put the model you want to download in the text box and click download.","3. You can also drag and drop your model in the Drop files box to upload them directly."]},{"l":"Inference","p":["Please use our Inference Settings guide to find out the inference settings do what.","TTA- results in longer separation time, it gives a little better SDR score but hard to tell if it's really audible in most cases\". it “means \"test time augmentation\", it will do 3 passes on the audio file instead of 1. 1 pass with be with original audio. 1 will be with inverted stereo (L becomes R, R become L). 1 will be with phase inverted and then results are averaged for final output."]}],[{"l":"Mainline Kaggle","p":["Last update: August 1, 2025"]},{"l":"Introduction","p":["This is a cloud-based alternative to run Mainline/Original RVC, RVC Fork, only for people who don't have a good PC GPU, via the Kaggle Service. With a Web User Interface.","Check the Google Colab Glossary for more info on Free Tier, Limits, Verification, Pricing and other things.","The Hina Modified Mainline Kaggle has reached EOL (End Of Life), meaning it has had it's last update, it won't be suggested anymore nor fixed for any future issues. We would suggest users to use other alternatives if it breaks, and if it breaks this guide will be removed in the future.","You only get 30 free GPU hours per week."]},{"l":"Pros & Cons"},{"l":"***Learn more***","p":["Has good GPU's","Has 30 GPU hours","Fast","TensorBoard included","You can leave training unsupervised.","Reached End Of Life.","Takes some time to set up.","Doesn't have Mangio-Crepe"]},{"i":"section","l":"‎"},{"l":"How to Setup"},{"i":"section-1","l":"‎"},{"l":"1. Set up account.","p":["Start by making an account here.","Verify your acount with a phone number so you can turn on the \"internet\" option."]},{"l":"2. Clone notebook and setup.","p":["Go to Hina's mainline notebook and click \"Copy and Edit\"","Under \"Session options\" in the sidebar turn on \"internet\". Make sure persistance is on for both files and varibles.","Turn on T4 X2 GPUs in accelerator.","d: (Optional) Turn on headless mode so you can run so you can run the GPU on all sessions and save your progress. Go to the top right and click \"Save version\" then open the advanced dropdown.","‎","Your runtime will continue draining when you're not running any cells with this option on."]},{"i":"section-3","l":"‎"},{"l":"3. Ngrok.","p":["Scroll down to the fifth cell and you should see a section where you put your ngrok token. If you dont have a ngrok acount sign up here.‎ 2. Once you have an acount you can authenticate your ngrok tunnel agent here: https://dashboard.ngrok.com/get-started/your-authtoken‎","put the Ngrok token in the quotation marks like so:","There is a monthly limit rate with Ngrok so dont be supprised if training is suddenly interrupted."]},{"l":"3. Starting the Cells.","p":["From top to bottom execute all the cells. With first being:","The second cell will take ~ 5 minutes to load.","when its finished it will look like this:","If you want to use a pretrain now is the time to download it. Add a new code cell and type this in then run it:"]},{"i":"#","p":["!wget LINK TO PRETRAIN","Run the third cell.","Once you run the final cell it will give you three links.","RVC url: is to open RVC's gui.","File url: is to open Imjoy Elfinder gui.","Tensorboard: is to open the Tensorboard.","The interface should look like this with your D and G files being located here. Here you can manage your files within Kaggle. Whenever you want to download files from the Imjoy GUI just double click. Downloading files uses up the Ngrok bandwith data."]},{"i":"section-6","l":"‎"},{"l":"Starting RVC"},{"i":"section-7","l":"‎"},{"l":"1. Click the RVC link.","p":["It should take you to the GUI where you can then go to the top and click the 'Train' tab."]},{"l":"2. Setup.","p":["Run through the normal RVC setup with setting your model name, sample rate and such. If you are unable to see the 32k sample rate click on V1 then swap back to V2.","Only use V2 Don't use V1 or you will get an error.","In the file manager create a folder named dataset anywhere then drag and drop your dataset in it. Then continue with normal RVC setup and training"]},{"l":"3. Syncing Graphs.","p":["For syncing graphs you need to train however many epochs you have set you save frequency then go into the file manager and find your model which should be in assest/weights, in its name it should have a step count and epoch count like this: model_name_e(number)_s(number).","E means epochs and S means steps.","Once you know how many steps the model trained for stop training the model by stopping the cell.","Then start the same cell and open the file manager and navigate to assest/weights and delete all of your previous models.","Then navigate to the 32k.json file which is located in the V2 folder of /configs and download it by double clicking it, then delete it.","Open the file you have just downloaded in notepad and edit log_interval to the amount of steps your model took, save it then replace the old 32k.json file.","Now go to your /logs/ folder and do the same thing. Modify the log interval of the config.json with your step count. Delete the config.json that's already in the /logs/ folder and replace it with your copy.","Now delete Eval folder, tf-events file, G D_23333333 files and train log file in /logs/ folder."]},{"l":"4. Resuming Training.","p":["Do not process and feature extract again because those files are already in the /logs/ folder. Use the same model name, sample rate, batch size, pretrain, and save frequency to train the model again in the RVC GUI."]},{"i":"---","l":"‎ ‎"}],[{"l":"Mainline Colab","p":["Last update: August 7, 2025","This is a cloud-based alternative to run Mainline/Original RVC, RVC Fork, only for people who don't have a good PC GPU, via the Google Colab Service. With a Web User Interface.","Check the Google Colab Glossary for more info on Free Tier, Limits, Verification, Pricing and other things.","The Hina Modified Mainline Google Colab has reached EOL (End Of Life), meaning it has had it's last update, it won't be suggested anymore nor fixed for any future issues. We would suggest users to use other alternatives if it breaks, and if it breaks this guide will be removed in the future.","‎"]},{"l":"Pros & Cons"},{"l":"***Learn more***","p":["Has TensorBoard.","Reached End Of Life.","Inconvenient.","Takes some time to set up.","You can't leave training unsupervised.","Doesn't have Mangio-crepe."]},{"i":"section","l":"‎"},{"l":"How to Setup"},{"i":"section-1","l":"‎"},{"l":"1. Running cells.","p":["Start by accessing the colab here.","Then run the first two cells to install all the requirements."]},{"l":"2. Installing Pretrains.","p":["If you wish to install a custom pretrain go to the 'Download Custom Pretrains' cell and go into the dropdown menu and find the pretrain you want.","If the pretrain you want isn't there go to the top left and click '+ Code'.","Then in the new cell type in !wget LINK TO PRETRAIN"]},{"i":"section-3","l":"‎"},{"l":"3. Ngrok.","p":["Scroll down to the fifth cell and you should see a section where you put your ngrok token. If you don't have a ngrok acount sign up here.‎ 2. Once you have an acount you can authenticate your ngrok tunnel agent here: https://dashboard.ngrok.com/get-started/your-authtoken‎","put the Ngrok token in like so:","‎","There is a monthly limit rate with Ngrok so don't be supprised if training is suddenly interrupted."]},{"i":"#","p":["The interface should look like this with your D and G files being located here. Here you can manage your files. Whenever you want to download files from the Imjoy GUI just double click. Downloading files uses up the Ngrok bandwith data."]},{"i":"section-5","l":"‎"},{"l":"Starting RVC"},{"i":"section-6","l":"‎"},{"l":"1. Click the RVC link.","p":["It should take you to the GUI where you can then go to the top and click the 'Train' tab."]},{"l":"2. Setup.","p":["Run through the normal RVC setup with setting your model name, sample rate and such. If you are unable to see the 32k sample rate click on V1 then swap back to V2.","Only use V2 Don't use V1 or you will get an error.","In google drive create a folder named training and inside it make another folder named dataset then drag and drop your dataset in it. Then continue with normal RVC setup and training","Make sure you use WAV or Flac files inside. Do not zip the folder."]},{"l":"3. Syncing Graphs.","p":["For syncing graphs you need to train however many epochs you have set you save frequency then go into the file manager and find your model which should be in training/weights, in its name it should have a step count and epoch count like this: model_name_e(number)_s(number).","E means epochs and S means steps.","Once you know how many steps the model trained for stop training the model by stopping the cell.","Then start the same cell and open then navigate to assest/weights in google colab and delete all of your previous models.","Then navigate to the 32k.json file which is located in the V2 folder of /configs and edit log_interval to the amount of steps your model took, save it.","Now go to your /logs/ folder and do the same thing. Modify the log interval of the config.json with your step count.","Now delete Eval folder, tf-events file, G D_23333333 files and train log file in /logs/ folder."]},{"l":"4. Resuming Training.","p":["Do not process and feature extract again because those files are already in the /logs/ folder. Use the same model name, sample rate, batch size, pretrain, and save frequency to train the model again in the RVC GUI."]},{"i":"---","l":"‎ ‎"}],[{"l":"Dataset & Isolation","p":["Last update: August 18, 2025","In this massive guide it will be explained how to properly prepare a dataset to make a RVC model.","In the field of AI, it's the collection of data used to train an AI model. It contains examples of the inputs the model is expected to handle, along with the correct outputs.","In the context of RVC, it's an audio file that's given to RVC, containing the voice the model is going to replicate. It can be a speaking, singing voice drums, sound effects or noise.","The quality, variety& length of the dataset are the biggest determining factors for the final quality of the model. Let's explain Length and Variety.","For beginners we recommend sticking with a dataset length of 15 minutes of pure data not counting silence, or if you desire a natural sounding model go for 40+ minutes of dataset. Just remember quality over quantity.","Variety in your dataset is also important because without it RVC lacks the ability to generate diverse audio.","Some things to increase the generalization abilities of RVC and increase the diversity in your dataset include:","Removing repeated words. ( If you want you can be extreme you can do this and remove every single repeated word that's fine, but generally there is no need to do this. )","Include speech in many ranges and pitches.","Longer datasets.","Expressive speech.","A quality dataset is super important for RVC since without one RVC will struggle to make anything good or believable."]},{"i":"section","l":"‎"},{"l":"Clean vocals.","p":["Ensure there isn't much background noise, reverb, overlapping voices, music, distortion, or small silences. Some quiet natural background noise is fine and won't ruin your model since the original pretrains for RVC were made with a noisy dataset, so RVC knows how to deal with noise. You'll learn more on cleaning vocals in the Vocal Isolation & Cleaning section below.‎"]},{"l":"Audio quality.","p":["The higher the audio quality, the better. If possible have it in a lossless format like WAV or FLAC, not a lossy one like MP3. No converting a MP3 to a FLAC or WAV won't remove the compression.‎"]},{"l":"No harsh sibilance/popping.","p":["Additionally, don't include harsh sibilance (loud \"S\" & \"SH\" pronunciation) or popping sounds (loud \"P\" sound)","Robotic sibilances are due to your dataset being short or they are overfitted. You can fix this by making your dataset larger or by choosing an epoch where the sibilants aren't overfitted.","Harsh sibilances are due to your dataset having harsh sibilants. You can fix this by de-essing or making your dataset larger.‎"]},{"l":"No Audio Damage.","p":["The most inportant part of a clean dataset, if your audio is damaged RVC will struggle with it causing it to overall sound worse because RVC will create synthetic data and try to learn from it, so make sure your audio isn't damged.‎","In RVC, artifacting refers to an anomaly where the output voice sounds \"robotic\" & glitchy. This occurs after the inference or model training process."]},{"l":"Causes","p":["It usually occurs when the dataset/vocal sample meets any of these criteria:","• Audio is low-quality• Voice model was overfitted, undertrained or overtrained• There are overlapping voices• There is reverb• There is noise","As you noticed, most of the issues boil down to the audio sample not being properly clean. RVC is built for purely working with voices, not other sounds.","Remember that the cleaner your input audio is, the better the results."]},{"l":"Solutions"},{"l":"1. Use a lossless format:","p":["If possible, it's best if your audio is in a lossless format like WAV or FLAC, preserving its original quality.","Avoid using lossy ones like MP3 or OGG.‎"]},{"l":"2. If doing inference:","p":["Remove undesired noises with an vocal isolation software.","Lowering the search feature ratio can also minimize this issue.","If breathing sounds produce it, lower the Protection value.‎"]},{"l":"3. If training models:","p":["‎","‎‎‎","A higher value will deepen the extraction, and a lower one will soften it.","A vocal isolation app is a software designed to extract a person's vocals from an audio file, usually through the use of AI models.","Access the space here, you don't need an account to use this, but making one will get you more free time, and paying for HuggingFace PRO will give you the most ZeroGPU time.","Again, try to maintain the natural dynamic range if possible. If you must decrease the volume on a particular dialogue because its too loud for your ears and you still need to use the RX Dereverb module, consider normalizing between -2db or -3db. The use of the RX Dereverb will put it back to normal volume.","Aggresive","And the result of DeEcho-Dereverb at 0.5 aggression:","And this is speech:","Anvuew mel dereverb v2","Any use of the De-echo model with UVR will slightly damage your audio with a 17khz cutoff frequency on the spectrogram.","Apollo Universal by Lew (to enhance mp3 and other low quality files)","At the right you can select the output format. We recommend picking FLAC. Learn why here.‎","Audios with thumping, SFX sounds, ringing noises, and weird vocalizations or breaths that might affect the voice model should be removed from the dataset as it'll have those characteristics. You can try to lasso out the possible source of the noise, but keep the audio as natural as possible without damaging the frequencies.","Bands, or notch filters will increase depending on the complexity of the noise.","Before starting the process for separation, make sure that the audio has zero DC offset to prevent further issues that would interrupt the processing due to the bottom line noise. The waveform statistics is under the Window tab in RX11.","Chunk Size: 485100","Click Browse File& select your audio, or simply drag & drop. The audio will begin to upload.","Click Select input to select your audio/s. Or just drag the files to it.‎","Click Spererate! below. Wait a moment for the audio to process.‎","Click the wrench (\uD83D\uDD27) on the left & go to Download Center","De-crackle Only use this tool if you hear crackling noises in specific parts of your dialogue/speech. It's not a requirement to use it.","De-Echo / De-Reverb","De-hum Use De-humming to take care of low or high frequency hums. There are no consistent settings for this as each situation is different for your audio.","De-Noise","De-Noise (found in Restoration)","De-plosive Use Deplosives when there are audible thumps of air coming through the mouth. Again, -plosives are consonant sounds that needs attention. Specifically the P, T, K, and CH sounds. There are no right settings for this so adjust it until the roughness of the -plosives are gone.","De-Reverb","De-reverb & De-echo by Sucial v2","DeNoise by aufr33","Dialogue Isolate Use this tool when you have audible room echo that could be removed on specific parts of your speech. Do not use it for the entire dataset because it may be inaccurate and strip away details. Or sometimes it doesn't work. Keep the settings the same here including sensitivity.","Do not use any form of compression on speech as it'll squash the dynamics and introduce artificial tearing to your model.","Download the result located in the output folder.","Each audio is different, so you'll have to test the ideal value.","either: unwa big beta v5e OR 2024.10","either: unwa instrumental v1e plus OR 2024.10","Ensure to clean your dataset properly, this includes removing silences and distortions.","EQ This has already been covered in the section for removing the Removing DC Offset. Do this if you haven't done it already. It takes care of low-end noise, but if your dc offset is already at 0% then skip this module.","Execute the Gdrive Connection cell by pressing the play button . Grant all the permissions.‎‎‎","Execute the Installation cell by pressing the play button , optionally check use_drive& grant all the permissions for Batch Separation.‎‎‎","Extract Backing Vocals","Extraction","Filter Q is the range selector.","First access Eddy's UVR UI Google Colab.‎","First access the Colab space here.‎","First go to X-minus's website and click the \"Vocal Remover\" at the top right.","First, login.‎","Follow these steps: Open the menu for labeling.","For better results, have the audio in a lossless format( WAV or FLAC), & not MP3.","For example, this is noise. The orange area may look like breathing that's masked under the artifacts, but RVC will consider this noise as it's barely audible and you'll only hear static or dry air. The blue areas surrounding the audio are also noise. You can think of these as layers that needs to be removed when we use the Spectral Denoise module later.","For example, use the frequency selector tool, select the humming occurring below 100hz, then press learn and render.","For free users, you can't convert audios in batches or longer than 10 minutes. If that's your case, trim it into different pieces.","For RVC users, the best app is Ultimate Vocal Remover 5 (or UVR). It can be used either locally or through the cloud.","For YT-dlp, the command prompt is:","Gabox's voc_fv4","Getting the highest quality audio works best for Izotope and will result in a better model. Ideally, you want to preserve the dynamic range, the frequencies, and the fidelity/clarity. If you're working with low quality audio, RX cannot upscale or restore the missing details that were originally there. The end result will be audible as the voice model quality will be muddied with artifacts/tearing.","Go to their Eddy's UVR5 UI Latest Release& Follow the installation steps ( Precompiled versions are suggested).","Go to their Official Website& buy it.","Go to their official website& press Download UVR. If you want to use BS / Mel Roformer you are going to need to install this.","Here is the settings that were used in the previous guide, which is mostly fine if you can't recognize patterns in a noise profile. Artifact control, whitening, Release [ms], Smoothing, and Reduction could be adjusted based on particular datasets. Again, don't fix it if it aint broke.","Here's a small comparison with the changes after vocal separation. Without DC offset:","Here's what audio with mono reverb will look like on Izotope RX11:","If you don't want to use ClarityVX, the most common method is to use UVR Dereverb and De-echo separately as you have full control on what's needed for your audio. There is no predetermined settings as each audio is different. The rule of thumb is to use an aggression of 3.0 -5.0 and nothing more than that. If the audio does not have reverb or echo, do not use any of these models as it can muffle the audio.","If you need to remove multiple noises, follow this pipeline for the best results: Remove instrumental - Remove reverb - Extract main vocals - Remove noise","If you only rely on MVSEP, you're forced to use UVR-DeEcho-Dereverb as there is no standalone option for the dereverb model. The cutoff frequency for this separation model would be 17.5khz.","If you prefer to EQ in RX 11, then click on the EQ module and enable only the hp bell curve. Keep the frequency precision to 6 and the frequency at 30-32hz. Your DC offset will be at 0% if you've done it correctly.","If you want to remove noise manually to avoid ai artifacts you can use RX 11, which is mentioned in this guide.","If your GPU is compatible with CUDA, toggle GPU Conversion on for a faster process.","In Google Drive, make two folders, named input& output.‎‎‎","In Output encoding select FLAC. We recommend selecting FLAC from now on. Learn more here.‎","In Select output you can define the folder for the results.","In Separation type Select a Model based on the Best Models List.‎","INST Gabox V7","Instrumental","It will redirect you their GitHub page. Click the download link for your operating system. UVR is available both on Windows & Mac.","It'll finish once the logs say Mounted at /content/drive‎","Izotope's Spectral Denoise provides natural noise reduction and will preserve the quality of your audio as it minimizes artifacts. It analyzes the signals of noise that we select and modifies the frequency components. Essentially, UVR Denoise is not needed afterwards since the cleaning has been processed in spectral. The rest of the the cleanup is done through our manual denoising via RX11.","Logging in is not mandatory, but recommended for shorter waiting lists.","Login so you get more quota","MDX-Net","Meanwhile you can hear some of the low-end frequencies. If you were to resample from 40k to 32k, then most of the high-end and low-end frequencies are gone at the cost of less harsher sibilances and -plosives sounds like I mentioned before.","Mel denoiser v2","Mel Roformer De-Noise","Mel roformer karaoke","Mel-RoFormer by Gabox Fv7z","Mel-RoFormer Lead/Back (the only, invisible, available one)","Mel-Roformer-Denoise-Aufr33-Aggr","Mel-Roformer-Karaoke-Aufr33-Viperx","MelBand Karaoke","MelBand Roformer","MelBand Roformer | De-Reverb by anvuew","MelBand Roformer | INSTV7 by Gabox","MelBand Roformer | Vocals FV4 by Gabox","Mic proximity is also another factor to consider as you want the voice to be consistent since RVC does not handle audio frequency response well and will muffle the pronunciation of words. Keep this in mind for studio sessions and video game voice lines rips as it may have been bass boosted, compressed, or eq'd by default. Arguably, more variation of the voice will add to the vocal range of the model but we want to keep the accent consistent as well.","Model","Model fuzed gabox & aufr33/viperx (SDR: 9.85)","Modify the Aggression Setting value on the right.","Most of the extraction model are behind a pay wall.","Mouth De-clicking and De-click Only use mouth de-clicking when clicking noises are audible. It's a good practice to use mouth de-clicking on only the click and not the entire dialogue/audio. Adjust the sensitivity or frequency skew as needed, but do not go overboard as it can remove the finer details of our audio.","MVSEP is a website for isolating vocals, that works similarly as UVR.","Note: they actually are all MelBand Roformer models, but there isn't a proper list yet for them","Now after that it should look like this:","Now click the long Start Processing button.","Now go in the RVC folder and place all these files in datasets folder. Zip it up if needed.","Now in this case one pass of spectral denoise wasn't enough. Repeat the steps done in the first pass by selecting the areas with the blue noise.","Now that we have the file in RX, make sure to turn this slider to the right to show the Spectrogram only.","Now this is breathing. Keep in mind that RVC will consider \"soft breathing\" as a white noise if the breath sounds are too low (around -40db). Proper vocal breathing are grunts like \"huffs, \"hahhhhs, \"hoohhhhhs\". There cannot be harsh inhaling sounds. Breathe sounds by itself without a voice or tone behind it will also cause RVC to think it is noise. Without breathes, the model sound will robotic.","Now we go to export our audio.","Now we select the noise (click and hold shift after selecting an area to include multiple audios) then press learn in the Spectral Denoise module after we captured the entire noise profile. Unselect the audio and click render.","Once it's done it will look like this:","Once it's done open the public url, and it will be the same as using Eddy's UVR5 UI Local/HF-Space.","Once it's done uploading, select a Model by the List of the Best Models. Under that you can change Segment Size and Overlap, the defaults are fine.‎","Once logged in, go to the main page.","Once the audio is done uploading, click Separate","Once the installer finishes downloading, execute it & follow the instructions. Make sure to tick \uD83D\uDDF9 Create a desktop shortcut for an easier access to UVR.","Open the WAV or FLAC file.","Overlap: 8","Pay to be an HuggingFace PRO Member to get X5 times more quota","Playable audios will then appear in the output boxes below. To download the output, click the little download icon in the top right.","Plosives look like this. Do not select the whole speech and only the plosives as suggested in the red lines.","Process Method","Refer to the FAQ Regarding Normalization and Compression if the volume is peaking or \"introducing noises\", which isn't the case at all. Do note that RX Dereverb will raise the volume by 2db each time you use it.","Report your issue here.","Resampling down to 32k is also fine since it results in less harsher sibilance and -plosives for your model.","Restoration","Reverb is multiple sources of sound returning and echo is the delay. It is important to remove these from the dataset otherwise it'll cause your model to have artifacts as Hifigan cannot replicate the model's clarity.","Reverb Removal","Reverb removal by either: Sucial V2 (MelRoformer) OR Anvuew V2 (MelRoformer)","Run the audio through BVE. Modify the Aggression Setting if necessary.","Same thing for the Instrumental, if you wish to keep it.","Select the category of the model (Process Method)","Select the Process Method and Model depending on your use case and the List of Best Models.","Select your model of choice and run the Separation cell. You can look for the List of the Best Models.","Sensitivity will adjust the amount of hum that will be removed.","Separation Type","Sibilance's are the hissing sounds when a person speaks, and plosives are the sounds of air that are released through the mouth. They are both considered consonant sounds with RVC.","Signal to distortion ratio, the higher is technically better, but your ears are more trustworthy.","Spectrograms are graphical representations of frequencies which can determine whether your audio sample rate is 32k, 40k, or 48k.","Tap the Input Audio box & select your audio, or simply drag & drop.‎‎‎","Tap the three buttons of the Vocals audio and then Download.‎","The 2nd pass will remove the noise.","The brighter a color on the spectrogram, the louder it is and will be audible.","The current best Dereverb plugin is the Clarity VX DeReverb Pro module, another paid software that you can get for free. The aggressiveness can be tweaked or finetuned to your liking as it cuts into the audio and does not reconstruct the frequencies. Clarity VX cannot de-echo the audio so you need to use UVR De-echo or RX Dialogue Isolate.","The goal is to get an audio sample with clean and natural vocals, which is what RVC needs to give the most accurate & quality results.","The key principal behind preparing your dataset is doing the least audio processing as possible as you want to keep the overall robustness of the model. Excessively stacking vocal separator models such as UVR Inst Voc, Kim Vocals, Denoise, ensemble mode, and so forth can introduce noises to your dataset as it rips away frequencies from your audio. This harms the model fidelity and quality.","The output will be like this:","The red underline shows what you should be looking for.","The RX Dereverb module will remove reverb that's leftover from the audio and may help remove noises. With RX Dereverb, select the audio, press learn and render. Adjust the reduction if needed. Do not use a strong reduction as it may muffle the audio. You can always undo with Ctrl + Z.","The spectrogram in the example is slightly above a frequency of 15khz. Take the khz x 2 and it would equal to a 32k audio. If it were 20khz, then it'll be a 40k audio. 32k is the lowest sample rate you can train on in RVC if your khz is below 15.","Then click \"select a file\" and choose a audio file, or you can drag and drop a file. And when it's done it will look like this:","Then click the download button (\uD83D\uDCE5). The model will download, which will take a few minutes","Then Log in to your Google account.‎","Then run the Install cell.","Then run the Run UI cell. You can choose different tunnels in case one is down.","Then select the Seperator type you need and the model based on the Best Models List","There is a distinction between high-end frequency and low-end frequency in a spectrogram. The high-end frequency (48k-40khz), or the air region in the chart isn't audible for the human ears. But, it'll help to handle aliasing. Aliasing is the effect of new frequencies appearing in the sampled signal after reconstruction, that were not present in the original signal. In other words, it creates artifacts to your model.","There is a queue so make sure you make an account to skip most of it.","There may be leftover reverb or echo that weren't removed so that'll be addressed in the RX Dereverb module later in the Manual Denoising section. I will also troubleshoot the audio increasing in volume after dereverb and deecho in the FAQ Regarding Normalization and Compression section.","These are the best settings:","They can remove undesired noises, like background noise, reverb, echo, music, etc.","This can be done in audacity by going to the Effect tab > Volume and Compression > Normalize then check the box to remove the DC offset.","This determines the depth of the extraction. Only the VR method has it.","This helps to remove most of the clipping that is occurring throughout the audio around the -0db range. Do not touch the make-up gain as it will alter the natural dynamic range of the audio. You can press \"preview\" to see that it is working as indicated with \"clipped intervals repaired\". If it does nothing, then you can skip this part.","This is jarredou's Music Source Separation Training (MMST) (Colab Inference).","This is using Mel scaling, if you right click on the numbers list(20k and such). you can change the scaling. Mel is the best scaling in our case since it shows vocals better than Linear scaling would.","This is what the audio looks like before spectral denoising:","This module is designed to remove or reduce short impulse noises such as clicks, pops, and digital clipping artifacts from audio recordings. These noises can occur due to various reasons, including imperfections in the recording equipment, electrical interference, or flaws in the audio signal itself. The De-click module analyzes the audio waveform and identifies these short, transient noises, then applies processing algorithms to smooth out or remove them, restoring the audio to a cleaner state.","This module is specifically tailored to address mouth noise issues in vocal recordings. Mouth noises are typically caused by saliva, lip smacks, tongue clicks, or other oral sounds that can be distracting or unpleasant to listen to in vocal recordings. Mouth de-click module preserves the integrity and naturalness of the vocal performance while reducing these types of mouth noises.","This step is not mandatory, but recommended for better results.","TIP: To test models/options more efficiently, tick Sample Mode to only process 30 seconds of your sample.","To do these next steps you are going to need Spek and Audacity.","Try running the audio through MelBand Karaoke or BVE. Modify the Aggression Setting if necessary.","Turn off shaped dither with Ctrl + P> Quality since we are exporting with WAV 32-Bit or FLAC 24-bit anyways.","Unfold its dropdown & select the model that you need","Using the Separation Type of DeNoise by aufr33, you can modify the Aggressiveness. This determines the depth of the extraction.","Vocals","Vocals/Instrumental","Wait","We can open Audacity and run the the dataset through Auburn Renegate. After that convert your dataset to mono since RVC works on mono and not stereo. There are two ways of neatly removing the silences in your dataset called Audio Labeling and Truncate.","We run the de-click module only as a last resort when mouth de-click doesn't work. Using both modules together may strip away the \"k\" consonants of our model.","What if I didn't capture the entire profile? Some noises may, or may not stay in tact even after the second pass with Spectral Denoise. RX will only pick out each respective noise samples to the audio that has been selected so it's better that you capture it in full.","When it's done converting it will redirect you to a page where you can listen the results.","When should I normalize the whole dataset? It is optional to use this module as it can be argued that rvc will do the normalization for you with -2db. You can normalize the dataset when it has been thoroughly cleaned at the end, if you want.","Whenever you export the dataset, you can export it to WAV 32-bit. For Flac files, use 24-bit. We never use MP3 files since RVC heavily compresses those audio files.","While keeping in mind of doing the least processing, you want to keep the audio as natural as possible for RVC, or Hifigan. Hifigan is the generative adversarial network that makes up our discriminator and generator for cloning sounds and training stability. Having audio that has been damaged or reconstructed will affect the generalization of the graphs to fully replicate the clarity and accent of the voice.","Why does my audio sounds like it's clipping or distorted? Should I use the gain tool in RX? Audios will usually have crackling and clicking noises introduced if it's over 1.0+db. We don't want to use the gain tool to address this since it's a compressor. If it doesn't have any of those issues, then leave the dialogue alone to maintain its natural range.","With DC offset:","Wouldn't this take up too much time? If your dataset is too long for spectral denoising, I suggest splitting it into sections so it is more manageable. It won't mess up with the overall noise profile of the whole dataset if you do so. You're prone to make mistakes/misclicks when selecting audio, especially when the dataset is 20 minutes to an hour long.","You can find an extremely long and complex guide by the Audio Separation's Discord: here, but it's not suggested and might have some outdated info as it got hundrends of pages.","You can now click \"Vocals\" to download the vocals and \"Other\" to download the instrumentals.","You can use Cobalt, yt-dlp, or Loader.to. Overall, yt-dlp is the best for ripping. Preferably rip the audios in Opus format so the downloaded audio will be 48khz, which can be resampled down to 44.1k on Izotope and trained on 40k via RVC. The quality depends on what is uploaded on the server side so this might not always be the case.","You have exhausted the GPU runtime of Colab.","You only need to do one or two passes of spectral denoising. Doing further processing than that will compress, or degrade your audio. Be cautious of overdoing it with the noise profile (selecting breathing or speech by accident) as RX might take away details and important aspects of a person's voice.","You should have noticed that RX11 moves the audio closer to each other whenever you delete a space or speech. Use the Lasso or Brush tool to delete dialogue precisely, which should preserve the spacing in phonetics. Make sure it does not cause clipping on the waveform as it'll look like a sharp spike. You can also clean up the leftover residuals left behind by spectral denoising.","You will now have it like this:","You'll require great specs & GPU to run it effectively. Otherwise, use either the Eddy's UVR5 UI Google Colab or the HuggingFace Space.","yt-dlp -x ffmpeg -i audio.opus audio.wav or yt-dlp.exe --audio-format wav -x https://www.youtube.com/watch?v=5aYwU4nj5QA&t=2s","ZeroGPU HuggingFace Spaces have a max inference time duration, it’s the time it takes to do an Inference (use the model, not the time of your audio file itself), on default it’s around 1 minute which is what Ilaria RVC uses. You need to retry with a shorter audio, you could also split your audio.","ZeroGPU HuggingFace Spaces have a quota per account, if you aren’t signed in you will get less quota so it’s better to login for more quota. You could get the ‘Sign-up’ part even if you are logged in. The ZeroGPU Quota can’t be seen but it isn’t unlimited. You can either:"]},{"l":"Step 1: Find the Sample Rate","p":["‎","And finally, introduce these values:","Download and install Spek here.","Encoding: 32-Bit Float","Format: WAV","Go go to Effects -> Volume and Compression -> Loudness Normalization","Go go to Tracks -> Align Tracks -> Align End to End","Go to Effects -> Special -> Truncate Silence","If the frequencies don't reach the top of the spectrogram, see at which number peaks & multiply it by 2. Here it reached 20 kHz. Doubling it gives 40kHz. Therefore the ideal target sample rate would be 40k","In Audacity import your audio","LUFS are used over db because hifigan needs perceptual quality and db doesnt offer that.","On the upper right corner go to File and click Export Audio.‎‎‎","Open spek and just drag & drop audio into it.","The most common sample rates are 32, 40, 44.1, & 48. The higher the sample rate, the more information it stores, therefore the higher the quality.","This is a unit in that defines the total amount of samples(data) that can fit within 1 second of an audio. They are measured in kilohertz (kHz).","Use the following values:‎","Use these values:‎","While training in RVC, you'll have to set the target sample rate as your dataset's. This value affects the final quality."]}],[{"l":"Training","p":["Last update: May 5, 2025","In this guide it will be explained how to properly train a model from start to finish.","Properly training a model is just as important as having a great dataset.","It won't be explained how to prosess a dataset and how to acutally train a model since that is difference from fork to fork, please look at the guide for your fork to find this info.","\"Epoch\" is a unit of measuring the training cycles of an AI model.","In other words, the amount of times the model went over its dataset and learned from it."]},{"l":"How many epochs should I use for my dataset?","p":["There isn't a way to know the right amount previous to training. It depends on the length, quality and diversity of the dataset.","If you aim towards a quality model, it's not convenient to input a semi-arbitrary amount of epochs, as it makes it prone to underfitting/overtraining. (explained later)","So it's best to use TensorBoard. With it you can determine exactly for how long you should train. (explained later)"]},{"l":"Do more epochs equal a better model?","p":["No it doesn't, since using a disproportionate amount will overtrain the model, which will affect the quality of it.","In the field of AI, is when an AI model learns its dataset too well, to the point where it centers too much around it & starts replicating undesired data.","The model performs very well with data of the dataset, but poorly with new data, as it has lost its ability to replicate anything that deviates from it.","It happens when the model is trained for too long/is too complex. So to avoid this, RVC users use a tool called TensorBoard."]},{"l":"What is overtraining?","p":["‎","A batch size is the number of training examples used in one iteration before updaing the model's parameters. For 30+ minutes of data batch size 8 is recommended and for less than 30 minutes batch size 4 is recommended.","Be aware that if you're training with 2 GPUs, like in Kaggle's T4x2, the batch size has to be splitted, as each GPU runs the same batch size, for example if you want to train batch size 8, you have to put 4 in the program.","Bigger batch size:","Can beneficial in cases where your dataset is big and diverse.","Can lead to early overtraining or flat / 'stuck' graphs.","Can lead to instability / divergence or noisy graphs.","Finetune: Trained with a pretrain.","Generalization might be improved.‎","Generalization might be worsened.","Merge: Made by merging pretrains. (These are considered the worst)","More suitable when your dataset is small, less diverse or repetitive.","Overtraining also know as overfitting is where the model doesn't actually learn the underlying patterns of the data and memorizes them instead.","Pretrains are an integral part of making a model, they are basically models that have been trained with many different types of voices, genders, ages, languages, manor of speech and are much longer then normal models. The objective of pretrains is to reduce training time and increase the quality of your model. To make a model without a pretrain you would need several hours of data to make anything decent.","Promotes noisier, less stable gradients.","Promotes smoother, more stable gradients.","Scratch: Trained with no previous pretrain.","Smaller batch size:","Some signs of overfitting are when the sibilances are robotic, when the graphs in the Tensorboard are going up or when the model is unable to produce high end harmonics because it's learning your dataset to well and your dataset doesn't have these high end harmonics.","There are three types of pretrains:","This image is a bit extreme but it gives you a good idea. If you notice your model is poorly creating high end harmonics try using a model several epochs back."]},{"i":"section","l":"‎"},{"l":"How do i use Pretrains?","p":["Go into the training tab and check the 'Custom Pretrained' box and use the drop down to select the pretrain's D and G file.","If you dont see a pretrain in the dropdown that means you need to download a pretrain, go into the 'Downloads' tab then go to 'Download Pretrained Models' then use the dropdown to select your sample rate and what pretrain you would like to download, then finally click download.","If you want to upload pretrains manually go into your Applio folder then go to rvc\\models\\pretraineds\\pretraineds_custom and place your D and G files there.","Asssuming you have the pretrain you want to use go into your mainline folder then go to assets\\pretrained_v2 and place you D and G files there.","Then in the 'Train' tab near the train button you can input the location of your pretrain, replace the ending so it's the name of the pretrain you put in pretrained_v2."]},{"i":"section-1","l":"‎"},{"l":"Where do i find Pretrains?","p":["32k Download:","40k Download:","48k Download:","Base 32k Download:","batch size: 32 x 2","D Download","dataset: vctk + m4singer","embedder: SPIN","f0: rmvpe","Fine-Tuned 32k Download:","G Download","GuideVocalPretrain is a fine-tuned pretrain based on the original pretrain. This contains 58 hours of Korean speech with the goal being to improve Korean speech.","Here is a list of all publicly available pretrains:","KLM 4 is the final HiFi-GAN pretrain that is going to be made by SSS. This version of klm is like all of the others but it follows the original structure of training and contains noise in the dataset so it can handle it better. This was trained with 800 hours of data, with a large portion of it being in Korean.","KLM 4.1 is a fine-tuned based on KLM V7 pretrained and made with around 100 hours dataset (Korean vocal/speech, Japanese vocal/speech and English speech), so it will work better with those languages. Unlike typical pretrained models KLM is a pretrained model created to make vocal guides using short voice recordings from a studio, this means that even with short dataset high pitch information it is possible to implement high-pitched sounds but it is sensitive to noise so it is recommended to use it with high quality datasets","KLM 4.2 maintains the same highly extensive pitch range as before and was developed to be able to handle high-pitched vocal inference even without having the corresponding vocal data of the model you wish to generate. KLM 4.2 was trained with 146 hours of data which mostly contains Korean, Japanese and some English.","Nanashi V1.7 is a fine-tuned based on TITAN pretrained and made with 11 hours of Brazilian music, so it will work better with this language but it can work with other languages without any problems, like TITAN, it allows models to be trained with few epochs and handles the noise better.","Nezox is a fine-tuned pretrain based on the original pretrain. This pretrain contains 43 hours of Indonesian speech with the goal of the pretrain to make Indonesian speech better.","Ov2Super is a fine-tuned based on the original RVC V2 pretrained and made with 30 minutes dataset, works well for small datasets and English language, this pretrained was trained on a precisely chosen clean speech and singing dataset, with bright and emotional voices. Additionally, it allows models to train with very few epochs compared to regular pretrains.","Rigel is a fine-tuned pretrain based on Rigel Base. Rigel Base has 1921 of speech from most langauges, Rigel fine-tuned has 102 of high quality speech also from a ton of langauges. The goal of this pretrain is to be a better base then the original pretrain.","sample rate: 32k","SingerPetrain is a fine-tuned based on Ov2 Super pretrained and made with 14 hours dataset (English singers). It is most suitable for training singers but it works for everything, the vocal range dataset is c1 to db7 so it works well with bass, baritone, tenor, alto, mezzo-soprano, soprano voices.","SnowieV3 X RIN_E3 continues the training with Snowie dataset and then finetuned with additional data, so it will work better with English, Russian and Japanese language and also helps models of other languages to pronounce them well.","SnowieV3.1 is a fine-tuned based on Snowie base pretrained (not publicly available) and made with 58 hours dataset (Russian and Japanese), so it will work better with those languages and also helps models of other languages to pronounce them well.","This is a fine-tuned based on the original RVC V2 pretrained and made with 22 hours of dataset aimed towards e-girl, soft male/female and deep male/female voices.","This is a fine-tuned pretrain based off of the original pretrain which aims to improve anime-style speech. This was train with 11 hours of speech.","This is a fine-tuned pretrain based on the original pretrain that improves drum models.","This is a fine-tuned pretrain based on the original pretrains and was made with 10 hours of Italian speech. Itaila was made to improve Italian speech.","This is a fine-tuned pretrain based on the original pretrains and was made with 2 hours of robotic speech which aims to make robotic voices better.","This is a perfect recreation of the original pretrain of RVC, which means you can train both speech datasets and singing datasets while also enjoying the benefits of the Spin Embedder Model. Every language supported, english pronunciation is vastly improved compared to the original pretrain, potential improvements in other languages as well. This recreation also has higher singing range than the og pretrain, and better generation of both speech and singing.","This pretrain is made from scratch with a 140 hour dataset. It is suggested to use this with high quality datasets due to its sensitivity to noise.","TITAN is a fine-tuned based on the original RVC V2 pretrained, leveraging an 11.15-hours dataset sourced from Expresso. It gives cleaner results compared to the original pretrained, also handles the accent and noise better due to its robustness, being able to generate high quality results. Like Ov2 Super, it allows models to be trained with few epochs.","Training Info:","vocoder: hifigan","You can find all of the community made pretrains in the \"pretrain-models\" channel in AI HUB."]},{"i":"section-2","l":"‎"},{"l":"How do i make Pretrain?","p":["Creating a pretrain is pretty much the same as training a normal model but the dataset is bigger and longer.","There are two ways of making a pretrain the first being:","From scratch which means you don't use a pretrain when training this. To make a decent from scratch pretrain you are going to need at least 50 hours of low, mid and high quality speech with many different speakers. The second way being:","Finetuning which means you use a pretrain to train this pretrain. To make a good you are going to need at least 10 hours of high quality speech with many speakers.","The big pro of making a Finetune is that you can tailor it to anything, like you can tailor it to improve a certain language, improve accents, types of speech and more. It can even improve the graphs (like grads, g/total etc.) if trained properly."]},{"l":"Misc","p":["3060 (Ti)","3070 (Ti)","3080 (Ti)","3090 (Ti)","4060 (Ti) (8/16gb)","4070 (Ti)","4070 (Ti) (Super)","4080 (Super)","4090","A Tier:","A: There is no \"best pretrain\" it all depends on your needs and what you're ok with sacrificing to get those benefits.","A10, T4","A100 (80gb and 40gb)","A40","B Tier:","C Tier:","D Tier:","Each of these are different in fidelity and require their own pretrains to use.","H100","HiFi-GAN","In Applio you are given the choice between three vocoders:","L4","L40S","MRF HiFi-GAN","P 100","Q: What is the best pretrain?","RefineGAN","S Tier:","This section contains miscellaneous information about pretrains.","To make a pretrain you are going to need a pretty good GPU, because without one it will take a very long time to train. Here is a GPU tier list for training pretrains:","V 100"]},{"l":"HiFI-GAN","p":["The first vocoder choice is HiFi-GAN the original GAN used in RVC which is combatible with all version of RVC and forks. HiFI-GAN is pretty basic and has muddy high ends."]},{"l":"MRF HiFI-GAN","p":["The second choice is MRF HiFI-GAN, this is a modfied version of HiFi-GAN with MRF instead of MPD, new loss functions and non-simplified version of the resolution block.","Pros:","Higher fidelity","44.1k Training","Cons:","Only a slight upgrade from Hifi-GAN","Not many pretrains for it"]},{"l":"RefineGAN","p":["The third and final choice is RefineGAN, this is an entirely different GAN then HiFi. This GAN uses noise to fill in the gaps and has a different resolution block.","Pros:","Higher fidelity and quality","44.1k Training","TensorBoard is a tool that allows you to visualize & measure the training of an AI model, through graphs & metrics.","It's specially useful for determining when to stop training a voice model, since with it you can detect when the overtraining point begins.","Because of this, TB is the most convenient tool for RVC users for perfecting a voice model."]},{"i":"section-4","l":"‎"},{"l":"Installing & Opening"},{"i":"section-5","l":"‎","p":["Download this file & move it inside mainline RVC's folder. Ensure the file path doesn't contain spaces/special characters.","Training"]},{"i":"section-6","l":"‎","p":["Now execute it. It will open a console window & create some folders inside RVC.","If you get the Windows protected your PC issue, click More info& Run anyway.‎","Once it's done, your default browser should open with TensorBoard app.‎","If it doesn't, copy the address of the console at the bottom, and paste it in your browser. Said address will say \" https://localhost\" followed by some numbers.‎"]},{"i":"section-7","l":"‎"},{"l":"Usage Guide","p":["‎‎","‎","Activate Ignore outliers in chart scaling.","And the right one is to center the view.‎‎‎","As you can see in the image above there is an area with several low points, so in this scenario you would try several epochs in that area to find the best sounding epoch.","Click the gear () in the top left corner & turn on Reload data. You can always manually refresh with the refresh symbol (\uD83D\uDD04) in the top right.","d/total shows how well the discriminator is able to differentiate between real and generated audio.","Each graph has three buttons in the corner:","First ensure auto-refresh is on, so the graphs update constantly.","FM shows how well the generator is able to make synthetic data that has similar features to the dataset.","Go to the SCALARS tab.‎‎","grad_norm_d shows the magnitude of gradients during training. If the gradients are becoming too large (over 100 for fintuning) that can cause some training instabilities and if they are becoming small that can lead to slow learning.","grad_norm_g shows the magnitude of gradients during training. If the gradients are becoming too large (over 1,000 for fintuning) that can cause some training instabilities and if they are becoming small that can lead to slow learning.","If it reaches a low point, let it run for longer until it's very clear it's OT.","If the graph is decreasing that means the discriminator is becoming better at distinguishing between real and synthetic data which usually means that the generator is producing realistic audio.","If the graph is decreasing that shows that the generator is making audio with similar distribution of latent variables to real data.","If the graph is decreasing that shows that the generator is producing audio with similar spectral distribution to the dataset.","If the graph is increasing that indicates that the generator is able to make audio that has similar features to the dataset.","If you get the No dashboards are active issue, select SCALARS in the top right corner dropdown.","If you want you can just use the lowest avg g/total point.","If you're fintuning it's best if the gradients don't go above 1,000.","If you're fintuning it's best if the gradients don't go above 100.","In the search bar, type \" g/total\" then look for the avg graph. This will be the graph you'll monitor.‎‎‎‎","Is a mel spectrogram view of audio from your dataset.‎","Is a mel spectrogram view of audio that the generator created in attempt to make it match mel_org.‎","KL makes the generator create similar distribution of latest variables to real data. The KL loss ensures that the generator is not just memorizing real data but it's learning to capture the underlying patterns in the data.","Left one is for going fullscreen.","Middle one to disable Y axis, for a fuller view.","Now let the training go for some time.","Open TB & begin training in RVC.","Select your model in the Runs section below. The models you tick will show in the graphs. (untick /eval if you want)‎‎‎","Set Smoothing to 0.987.","The mel spectrogram loss compares both the real and synthetic mel spectrograms. This loss encourages the generator to produce audio that sounds similar to the dataset.","Then over your mouse over these low points and take note of the step counts. Since this is using the avg graphs you may not find the exact epoch connected to the step count so just choose the closest point.","Then zoom out & lower the smoothening. Then in the avg graph look for low points around where it started to overtrain.","There will be various low points, one after the other, so don't get too anxious if it's OT or not. You can always use a previous checkpoint either way.","To zoom in & out the graphs, press the ALT key + mouse wheel. Remember to center the view after moving around, and after the graph updates.","While looking through the Tensor Board you may come across slice/mel_gen and slice/mel_org.","you can think of this as clarity / fidelity.","You can think of this as how well it can replicate the speakers style.","you can think of this as how well the model can match timbral, spatial and temporal characteristics.","You'll detect OT(overtraining) when the graph hits the lowest point, then stay flat/ rising indefinitely.‎ Example of OT:"]}],[{"l":"Inference Settings","p":["Last update: August 14, 2025"]},{"i":"section","l":"‎","p":["When doing inference in RVC, you'll come across to quite a few options that you can tweak, that influence the conversion process.","Configuring them accordingly can improve the output quality by a lot, as well as reduce artifacting, so we highly recommend learning them.","There are some of them that are either obsolete or not important. So if a setting is not explained here, you can ignore it."]},{"i":"section-1","l":"‎"},{"i":"section-2","l":"‎"},{"l":"Also known as Pitch, it adjusts the tone of voice.","p":["Negative values lower the tone (e.g -2).","Positive ones raise it (e.g 5).","You can use decimals if necessary (e.g -4.3).","You'll usually have to modify this for the pitch to sound perfect. Modify it until it matches the tone of the model."]},{"i":"section-4","l":"‎"},{"l":"Also known as Index Rate, it determines the level of influence of model's .INDEX file:","p":["Higher values will apply more of the .INDEX's characteristics.","Lowering it can reduce artifacting.","Remember, if the dataset had other sounds like background noise, there will be noise in the .INDEX too."]},{"i":"section-5","l":"‎"},{"l":"They're the algorithms for converting the vocals.","p":["*FCPE*","*Mangio-Crepe / Crepe*","*RMVPE*","*Switft*","A Robust Model for Vocal Pitch Estimation in Polyphonic","As the majority of them are obsolete, we'll focus on the 3 best ones: RMVPE, Mangio-Crepe& FCPE.","Better with harmonic-rich voices / fuller voices","Better with soft, whspery or voices with feminine timbres","Check https://github.com/lars76/swift-f0/ for more info","Decent precision","Each one works in its own way, and has its pros & cons.","Fast","Fast Context-base Pitch Estimator","Fast, which is good for realtime","If you have really clean audio use this over RMVPE","Inference only. Allows you to set the maximum/minimum frequency, to reduce small distortions. Recommended for advanced users.","Lowering it too much might lead to voice cracks so it's recommended to not lower it below 64.","Mangio Crepe: It's crepe, but you can adjust its hop_length. It determines the time it takes the voice to hit a note. The lower the value, the more detailed results you'll get, but will take longer to process","Might be better with human softness","Might be more precise than RMVPE, but it's not as much tested yet for RVC","NOTE THAT MANGIO CREPE IS ONLY A PLACEBO: 160 hop is required to match with feature extraction, if you ran hop 40, that made f0 output 4x longer than needed and it had to be interpolated back required size. That's why it got removed from Applio, it would be better to use Crepe, or even better just use RMVPE.","Pretty new","RMVPE+:","Should be your go-to algorithm, due to its convenience","Some forks include RMVPE_GPU& RMVPE+. Same algorithm, but with a modification:","There's a Pitch Benchmark, but this is generally speaking and not only taken in the context of RVC.","They also work the same for training models.","Training only. Uses more GPU power, making you train faster.","Useful for Realtime. If you have poor performance in Realtime, use this over RMVPE","Usually sounds a little harsh","Very fast, less precise and prone to noise"]},{"i":"section-13","l":"‎"},{"l":"They're the Models used for learning speaker embedding.","p":["*Chinese-Hubert-Base*","*ContentVec*","*Custom*","*Japanese-Hubert-Base*","*Korean-Hubert-Base*","*Spin*","A HuBERT (Hidden-Unit BERT) model that was pre-trained specifically on a very large dataset (10,000 hours) of Chinese speech.","A HuBERT model pre-trained on a large dataset of Korean speech.","A HuBERT model pre-trained on a large-scale Japanese speech corpus (approximately 19,000 hours).","A newer, more advanced embedder based on the \"Speaker-invariant Clustering\" method.","An advanced model adapted from the HuBERT framework, specifically designed to be better at separating speech content from the speaker's unique vocal characteristics.","Because it is specialized for the Chinese language, it can be particularly effective at capturing the nuances of Mandarin-speaking voices.","Can also be used for other languages, but may show a performance advantage with Japanese source audio.","Each one works in its own way, and has its pros & cons.","Excels at separating the speaker's unique vocal timbre from the actual phonetic content (the words being spoken).","It can be downloaded at: https://huggingface.co/IAHispano/Applio/tree/main/Resources/embedders/spin.","Its architecture is well-suited for realtime models.","Its specialization in Japanese makes it adept at creating embeddings from Japanese speakers, as it's tuned to the phonetic and acoustic characteristics of the language.","Like the other language-specific models, it is optimized for capturing the vocal characteristics of Korean speakers.","Often considered a high-quality choice for getting a clean and accurate speaker identity.","The default and original one, used for the great majority of RVC models.","This could result in clearer and more accurate pronunciation compared to older models like ContentVec.","This helps to prevent the original speaker's pronunciation or words from \"bleeding\" into the final output.","This model analyzes an audio sample to extract a \"speaker embedding\", it's like a digital fingerprint of a voice's unique timbre and characteristics.","This option allows for the use of a user-provided, custom-trained embedder model.","Using a model pre-trained in the target language can sometimes yield more accurate speaker similarity.","While optimized for Chinese, it can still function as a general-purpose embedder for other languages."]},{"i":"section-20","l":"‎"},{"l":"Also known as Protection, they suppress breath sounds:","p":["Decrease the value to remove more breath sounds, as they cause some artifacting.","A value of 0.5 disables this feature.‎","Be careful, lowering it too much will make it voice sound \"inhumane\" & suppress part of the words."]},{"i":"section-21","l":"‎"},{"l":"Also known as Remix Mix Rate, controls the loudness of the output:","p":["The closer to 0, the more the output will match the loudness of the input audio.","The closer to 1, the more it will match the loudness of the dataset the model was trained on.","Basically, leave it at 0 if you want the audio to try to keep its original volume."]},{"i":"section-22","l":"‎"},{"l":"Gives a faster inference & more consistent output volume:","p":["In RVC sometimes there's an error where the volume of the output lowers in some parts.","To prevent this, Split Audio divides the audio & infers them one by one. Then unites them at the end.","Doing it this way is faster too."]},{"i":"section-23","l":"‎"}],[{"l":"Tg Develop's W Okada Fork","p":["Last update: September 6, 2025","W-Okada is a realtime voice changer that uses RVC for its conversion.","There are 3 versions of this realtime voice changer, the Offical Original W-Okada made by Wok, the Deiteris' Fork made by Deiteris, and the Tg-Develop's Fork made by Tg-Develop. Note that those 3 links are just for reference to the Source Code Github Repositories of both projects, you should instead follow the guide below.","This guide will be about the Wokada Tg-Develop's fork since it's a fork of the Deiteris' Fork, containing the performance improvements, has an improved Web User Interface, supports Spin Embedder Models, and has Audio Effects.","RVC does NOT mean realtime voice changer. RVC means Retrieval-based-Voice-Conversion."]},{"l":"Is The W-Okada Tg-Develop's Fork Safe?","p":["RVC Models are PyTorch Models, a Python library used for AI. PyTorch uses serialization via Pythons' Pickle Module, converting the model to a file. Since pickle can execute arbitrary code when loading a model, it could be theoretically used for malware, but Wokada Tg-Develop's Fork has a built-in feature to prevent code execution along the model. Also, HuggingFace has a Security Scanner which scans for any unsafe pickle exploits and uses also ClamAV for scanning dangerous files.","‎"]},{"l":"Pros & Cons"},{"l":"*Learn more*","p":["Currently stable","Good Performance","Has great support for Nvidia, AMD, Intel, Mac, Linux, Windows","Uses a Web User Interface, meaning it can be run on the Cloud","Uses FP16 Inference by default, and let's you choose to use FP32 for better quality/precision","Has Audio Effects","Lets you choose the Model Embedder Type, including ContentVec & Spin.","Uses a Web User Interface, having issues on some browsers, and bugs with renaming or deleting models on it","Doesn't have a very active development recently, it's more of a personal public fork with some Quality Of Life updates of the Wokada Deiteris' Fork, please don't have too much expectations and don't disturb the developer about it","Has Cut Off Issues Using an Extra superior to 2.7"]},{"i":"section","l":"‎"},{"l":"System & Hardware Requirements","p":["Windows 10 or Later","macOS 12 Monterey or later. With Apple Silicon or Intel CPU","Any Linux Distro","and","At least 6GB of RAM","At least 6GB of free disk storage"]},{"l":"For GPU-conversion","p":["TLDR: Make sure you have Nvidia RTX 20xx or AMD Radeon RX 5xxx or better. GTX 10xx or RX 580 will also work, but may run into issues with games and higher delay. If you have an iGPU (mostly AMD Radeon Graphics or Vega) use online hosted alternative instead.","Long answer:","Minimum:","An integrated graphics card: AMD Radeon Vega 7 (with AMD Ryzen 5 5600G) or later; with 2GB VRAM (in FP32 mode), ~ 1GB VRAM (in FP16 mode, if supported). But this is NOT recommended at all and we will most likely not recommend you to download the realtime voice changer with iGPUs.","A dedicated graphics card: Nvidia GeForce GTX 900 Series or later, or AMD Radeon RX 400 series or later, or Intel Arc A300 series or later.","Recommended:","A dedicated graphics card Nvidia GeForce RTX 20 Series or later, or AMD Radeon RX 5000 series or later, or Intel Arc A500 series or later."]},{"l":"For CPU-conversion","p":["TLDR: don't bother. You can't run games, discord usage might be the only thing that will work decently, but you might potentially damage your CPU. People with no GPU usually have old CPU's, so delay will be high too. Not worth it.","Minimum:","Intel Core i5-4690K or AMD FX-6300.","Recommended:","Intel Core i5-10400F or AMD Ryzen 5 1600X.","If you plan on playing games at the same, do not use CPU-conversion. With CPU, the delay will be massive and your PC will not run smoothly at all. If you have a higher-end CPU you can make it work, but those that have higher end CPUs most likely also have higher end GPUs, so you should be using your GPU if possible."]},{"l":"Virtual Audio Cable"},{"l":"A Virtual Audio Cable (VAC) is what you need to use the realtime voice changer on Discord & Games.","p":["A VAC (Virtual Audio Cable) makes a fake audio device, used to re-route the audio of different programs.","In Wokada Deiteris Fork context, it's used to get the output of Wokada Deiteris Fork as the input in other programs such as Discord.","Download this: VAC Lite (Virtual-Audio-Cable by Muzychenko).(Be sure to not use any toher vac like VB Audio Cable.)","Run setup64, not 64a, after extracting the zip to a new folder","After installing the VAC Lite, it changes your default audio system. Click Yes when it asks you to open the audio device settings (or press WIN+R, type \"mmsys.cpl\" if you closed it already), and change your Recording and Playback devices back to your usual devices. Same for communications device aswell (right click -> set as default communication device)","Download either: Blackhole Virtual Audio Cable or VB-Audio","For Debian / Ubuntu-based Systems (Ubuntu, Mint, Pop!_ OS), run in the terminal:","For Fedora / RHEL-based Systems (CentOS, Rocky Linux), run in the terminal:","For Arch / Arch-based Systems (Endeavour, Manjaro Linux), run in the terminal:"]},{"l":"Windows","p":["First, go to the latest release page: Latest Release (click here to go to Github)","On the release page, find the \"Assets\" section and download the files based on your GPU.","To check your GPU, open Task Manager (Ctrl+Shift+Esc), go to the \"Performance\" tab, and look at the GPU names. If you have an NVIDIA card, use the NVIDIA (CUDA) version."]},{"l":"Download NVIDIA on Windows","p":["Download all of the voice-changer-windows-amd64-cuda files. This will include multiple files, likely ending in .zip.001, .zip.002, etc.","Make sure all downloaded parts are in the same folder before extracting.","If you have a GTX 700 series card or older, the CUDA version may not work. Use the AMD/Intel (DML) version instead."]},{"l":"Download AMD, INTEL and CPU on Windows","p":["Download the single file named voice-changer-windows-amd64-dml.zip.","If you don't have a dedicated GPU or the dml version doesn't work, download voice-changer-windows-amd64-cpu.zip.","Some integrated graphics like Intel UHD may not be compatible. If it fails, you may need to use the CPU version or an online alternative."]},{"l":"Opening on Windows","p":["Make sure you have 7-Zip or WinRAR installed to extract the files.","To extract:","For the NVIDIA version: Right-click on the file ending in .zip.001(the first part of the archive) and choose to extract it. Your archiving program will automatically find the other parts and combine them.","For the AMD/Intel or CPU version: Right-click the single .zip file and extract it.","Open the newly created MMVCServerSIO folder and run the MMVCServerSIO.exe application.","If nothing opens, then open a browser and type in http://127.0.0.1:18888/. This is a local URL, it runs on the WebUI."]},{"l":"Mac","p":["First, go to the latest release page: Latest Release (click here to go to Github)","On the release page, find the \"Assets\" section and download the file corresponding to your Mac's processor."]},{"l":"Download for Apple Silicon Mac","p":["For Macs with an M1, M2, or newer Apple chip, download the file ending in macos-arm64-cpu.tar.gz."]},{"l":"Download for Intel Mac","p":["For older Macs with an Intel processor, download the file ending in macos-amd64-cpu.tar.gz."]},{"l":"Opening on Mac","p":["Double-click the downloaded .tar.gz file to unpack it. An MMVCServerSIO folder will appear.","Open the MMVCServerSIO folder and double-click the MMVCServerSIO application to run it.","You do not get a popup notification for this, so if it does not open or says \"Pytorch is damaged\", do the following:","Open the Terminal","Run the following command: xattr -dr com.apple.quarantine PUT IN THE PATH TO YOUR MMVCServerSIO FOLDER HERE For example, if you extracted the realtime voice changer to your desktop, the command may look as follows: xattr -dr com.apple.quarantine ~/Desktop/MMVCServerSIO","Now, open the extracted MMVCServerSIO folder and run MMVCServerSIO to run the realtime voice changer.","If nothing opens, then open a browser and type in http://127.0.0.1:18888/. This is a local URL, it runs on the WebUI."]},{"l":"Linux","p":["First, go to the latest release page: Latest Release (click here to go to Github)","On the release page, find the \"Assets\" section and download the file(s) corresponding to your hardware."]},{"l":"Download for NVIDIA on Linux","p":["Download all files that start with voice-changer-linux-amd64-cuda.tar.gz(e.g., .tar.gz.aa, .tar.gz.ab)."]},{"l":"Download for AMD on Linux","p":["Download all files that start with voice-changer-linux-amd64-rocm.tar.gz(e.g., .tar.gz.aa, .tar.gz.ab)."]},{"l":"Download for CPU on Linux","p":["Download the single file named voice-changer-linux-amd64-cpu.tar.gz."]},{"l":"Opening on Linux","p":["Make sure all downloaded parts for your version are in the same directory.","Open a Terminal and navigate to your downloads folder.","Use the cat command to combine and extract the multi-part archives. For the single-file CPU version, use tar.","A new folder named MMVCServerSIO will be created. Navigate into it:","Make the application executable:","Run the voice changer:","After the server finishes loading in your terminal, it will not open a window on its own. Open a web browser and go to http://127.0.0.1:18888/ to access the user interface."]},{"l":"Opening on Multi-PC Setups","p":["This is only for the people that have 2 PCs, and want to use 1 PC for Gaming, the other only for Wokada Tg-Develop's Fork.","Create a file named .env on the same folder where MMVCServerSIO.exe is located. Open it up with a notepad, copy paste the settings from the GitHub link.","After that, you create another file with the file extension ending .bat, open it up with a notepad, copy paste what is needed in there again from the GitHub link.","Now run the bat file. After it starts, you should be able to open the link. For example, if you specified HOST=192.168.0.1 and ALLOWED_ORIGINS='[https://192.168.0.1:18888]'), you should be able to open https://192.168.0.1:18888 in your browser and use the realtime voice changer UI from other machines in your local network."]},{"l":"Voice Models"},{"l":"Managing Models"},{"l":"Adding Models"},{"i":"#","p":["Click on + on the left sidebar Model Selector menu.","Only RVC models will work. If you have a gpt-sovits one or any other, they will not work.","Upload the Model File (.pth, .safetensors, .onnx), give it a Name (this is currently bugged, no matter what name you give it, it will always use the actual model's name), and Select between Hubert_Base/ContentVec & SPIN for the Embedder Type","Optionally Upload the Index File (.index), this controls the Trained Model Accent.","Optionally Upload a Thumbnail Image.","Optionally check \"Select model after upload\", to automatically select it after it gets uploaded."]},{"l":"Renaming Models","p":["Attempting to rename a model directly within the Web User Interface will cause the program to delete the model. This is a known bug. Use fix below to safely rename your models.","Navigate to your MMVCServerSIO folder.","Inside, open the model_dir folder. You will see several numbered folders, each corresponding to a model slot in the UI.","Open the folder for the slot number you want to rename.","Inside this folder, you will find a params.json configuration file. Open this file with a text editor like Notepad.","Look for the name: field in the file. Change the text in the quotes to your desired new model name.","Save the .json file. The name will be updated in the program UI."]},{"l":"Deleting Models","p":["If you wish to delete a model, you can simply click the red trashbin icon next to its name in the Model Selector sidebar list."]},{"l":"Merging Models (Merge Lab)","p":["The Merge Lab allows you to combine multiple RVC V2 voice models (.pth Weights only, not indexs too) into a single, new hybrid model. This is useful for creating unique voices.","Open Merge Lab: At the bottom left, click on the Merge Lab button.","Select Models to Merge:","In the \"Filter Settings\" section, you can use the Search Models bar to find specific models by name.","Select the desired Sample Rate and Embedder from the dropdown menus to filter the list of available models. Only models that share these same characteristics can be merged.","From the \"Available Models\" list, check the boxes next to the models you want to include in the merge.","Adjust Weights: Use the sliders next to each model's name to set its \"weight\" (RVC models are PyTorch files, the .pth is the weight containing the voice) or influence in the merged model. The numbers (from 0 to 100) represent the percentage of each voice in the mix.","Choose Merge Options:","Download merged model: This is checked by default and will download the final merged model file to your computer.","Save to merge slot: This option saves the merged model to a dedicated merge slot.","Save to empty slot (auto-select first available): This will automatically save the merged model into the next open model slot.","Finalize Merge: Once you have selected your models and set your desired save options, click the Merge button to create the new hybdrid model.","You can't merge indexs(in rvc context, the trained accent of the voice). Only the .pth actual voice file."]},{"l":"Audio Setup"},{"l":"Discord & Games","p":["On the realtime voice changer app wokada, you select:","Input: Your microphone","Output: Virtual Audio Cable","Monitor (if you wish to hear the realtime voice changer on your headphones aswell): Your headphones","On discord and games, you select:","Input: Virtual Audio Cable","Output: Your headphones"]},{"l":"Client and Server Setup","p":["Audio: CLIENT","Uses MME (normal audio processed through windows. You use this automatically with every application)","You can use the boxes echo, sup1, sup2 using this","Audio: SERVER","Use S.R. 48000","I recommend using [Windows WASAPI] on all prefixes for less delay, because this uses your audio devices (e.g. microphone) directly, before processing through windows.","Both Input and Output has to be the same (Windows WASAPI), you can't use MME for input and then Windows WASAPI for Output.","You can not use the in-built noise suppressions in this mode","ASIO > WASAPI > MME as a general thumbrule (this also affects delay)","Sometimes Client does not work, then use SERVER with prefix \"MME\" or \"Windows WASAPI\". You can not use the in-built noise suppression and echo fix if you use SERVER."]},{"l":"Settings Explained","p":["Start Server: Starts/Stops the realtime voice changer, be aware that you can't change every settings while it's running.","Passthrough: Sends your actual voice and not the realtime voice changer through the virtual audio cable. You want this to be OFF for the realtime voice changer to work."]},{"l":"Model Settings","p":["Pitch: This is the pitch. Going into negative will make it lower pitch, going higher will make it higher pitch. If you have a male voice using a female voice, aim for 10 - 14, this depends on your voice, try around those numbers until you find a sweet spot.","Formant Shift: Alters harmonic frequencies and changes the voice timbre without affecting the pitch","Index Ratio: This controls the accent of the voice model. In most cases, using Index on Realtime Voice Changer can add realism if you speak the language the model was trained in. If you have a heavy foreign accent, you may use this at a low rate. Beware, this increases CPU usage.","Save Settings: Save the previous Settings for the specific model you're using."]},{"l":"AI Settings","p":["Echo Cancellation: if you experience echo issues despite having sup2, In. Sens to the right and having lowered your windows system value, then this will help you as a last resort.","Noise Suppression 1: Noise suppression but weaker, not recommended to use this at all, because it barely has any impact whilst reportedly, making the voice inconsistent.","Noise Suppression 2: Noise suppression on your microphone.","Input Sensivity: microphone threshold, increasing this will cause less background noise to get picked up if it's a problem.","Chunk Size: Controls the delay (lower number means less delay, but depends on what your GPU is capable of).","Extra Processing Time: Controls voice model quality. 2.7s is the max, anything above can cause cutoff issues.","F0 det: Pitch extraction algorithm. Both RMVPE (for the best precision and robustness) and FCPE (for less precision & robustness but lower delay) are good options.","Processing Unit: Select your Processing Unit, which can be your GPU, CPU or M Chip."]},{"l":"Audio Settings","p":["Input Volume: This raises the microphone volume before it goes into the realtime voice changer (Recommended to leave it on the default or if needed, not to go too high, else it increases background noise and makes the voice sound worse).","Output Volume: Raising realtime voice changer volume on the output.","Monitor Volume: Increases volume of your headphones that you set on Monitor Device if you selected to hear yourself with the realtime voice changer."]},{"l":"Audio Effects","p":["Audio effects are powerful tools for shaping and enhancing your voice in real-time. They can be applied to either the input (your microphone) or the output (the final sound you and your audience hear)."]},{"l":"Input vs. Output Effects","p":["The same effects can be applied to both your input and output audio, but the application timing and purpose differ.","Input effects process your raw microphone audio before it's used by the voice changer. Applying effects here can alter how the AI processes your voice, which can be useful for creative sound design.","Output effects are applied to the audio after the voice-changing process. These are typically used to refine the final sound, add ambiance, or apply mastering touches."]},{"l":"Effect Providers: Simple vs. Pedalboard","p":["Two types of effect providers are available, each offering a different level of complexity and control.","Simple Provider: Offers basic, essential effects with straightforward controls. It's ideal for quick adjustments and users who are new to audio effects.","Pedalboard Provider: Provides a more extensive collection of high-quality effects with detailed parameters. This provider is geared towards users who want to fine-tune their sound with greater precision, much like a guitarist's physical pedalboard."]},{"l":"Simple Provider Effects","p":["Simple Gain: Basic volume adjustment.","Gain: Amount of volume boost or cut.","Simple Low Pass: Basic IIR low-pass filter.","Cutoff Frequency: Frequency above which signals are filtered out.","Simple High Pass: Basic IIR high-pass filter.","Cutoff Frequency: Frequency below which signals are filtered out.","Simple Delay: Basic delay effect.","Delay Time: Time delay for echoes.","Feedback: Amount of feedback for multiple echoes.","Wet Level: Level of delayed signal."]},{"l":"Pedalboard Provider Effects","p":["Attack: How quickly the compressor responds to loud signals.","Attack: How quickly the gate opens.","Bit Depth: Number of bits for quantization (lower = more distorted).","Bitcrush: Adds lo-fi digital reduction.","Chorus: Adds richness and width.","Clipping: Hard distortion by signal clipping.","Compressor: Controls dynamic range.","Convolution: Impulse response reverb/simulation.","Cutoff Frequency: Filter cutoff frequency.","Cutoff Frequency: Frequency above which signals are filtered out.","Cutoff Frequency: Frequency below which signals are filtered out.","Cutoff Frequency: Frequency where shelf begins.","Damping: Amount of high-frequency absorption in the room.","Delay Time: Time between the original sound and the echo.","Depth: Intensity of the pitch modulation.","Distortion: Adds harmonic saturation.","Drive: Amount of distortion applied to the signal.","Drive: Amount of filter overdrive.","Echo: Creates delayed repetitions.","Equalizer: Adjusts frequency response.","Feedback: How much of the echo is fed back to create repeats.","Frequency: Center frequency of the peak.","Gain: Adjusts overall volume level.","Gain: Amount of boost or cut.","Gain: Amount of volume boost or cut.","Gain: Boost or cut at the center frequency.","GSM Compressor: 2G cellular phone compression.","High (12.5kHz): Boost or cut high frequencies (air/brightness).","High Cut: Removes high frequencies from the echo.","High Pass Filter: Removes low frequencies below cutoff.","High Shelf Filter: Boosts/cuts high frequencies.","High-Mid (5kHz): Boost or cut high-mid frequencies (clarity).","Impulse Response: Type of acoustic space to simulate.","Invert: Flips signal polarity.","Ladder Filter: Moog-style multimode filter.","Level: Output volume of the effect.","Limiter: Prevents audio from exceeding threshold.","Low (80Hz): Boost or cut low frequencies (bass).","Low Pass Filter: Removes high frequencies above cutoff.","Low Shelf Filter: Boosts/cuts low frequencies.","Low-Mid (320Hz): Boost or cut low-mid frequencies (warmth).","Mid (1.25kHz): Boost or cut mid frequencies (presence).","Mix: Balance between dry and chorus signal.","Mix: Balance between dry and processed signal.","Mode: Filter mode (LPF=lowpass, HPF=highpass, BPF=bandpass, 12/24=slope).","MP3 Compressor: Adds MP3 compression artifacts.","Noise Gate: Eliminates unwanted background noise.","Peak Filter: Parametric EQ band.","Pre-delay: Delay before reverb starts, simulates room distance.","Q Factor: Sharpness of the transition.","Q Factor: Width of the frequency band (higher = narrower).","Rate: Speed of the chorus modulation.","Ratio: How aggressively the gate closes.","Ratio: How much the signal is compressed (4:1 means 4dB in = 1dB out).","Release: How quickly the compressor stops compressing.","Release: How quickly the gate closes.","Release: Time to return to normal after limiting.","Resonance: Emphasis at the cutoff frequency.","Resonance: Filter resonance (emphasis at cutoff).","Reverb: Adds room ambience and depth.","Room Size: Controls the size of the simulated room space.","Threshold: Level above which compression starts.","Threshold: Level at which clipping occurs.","Threshold: Level below which audio is gated (silenced).","Threshold: Maximum output level.","Tone: EQ balance from dark to bright.","Type: Character of the distortion algorithm.","VBR Quality: MP3 VBR quality (0=highest, 9=lowest).","Voice Count: Number of virtual voices in the chorus.","Wet Level: Volume level of the echo effect.","Wet/Dry Mix: Balance between original and reverb signal."]},{"l":"Advanced Settings","p":["UI Language: Select the Web User Interface Language, currently there is only English, German and Japanese.","Protocol: rest (Use SIO if you want less delay but if you encounter any issues with SIO switch back to rest. Rest has slightly more delay than SIO)","Crossfade Overlap: Controls how smoothly the AI stitches different processed parts \"chunks\" of your voice back together. 0.1 or 0.15 (0.1 for fastest voice, 0.15 for improved quality but increases delay by 50 ms)","SilenceFront: Reduce GPU usage when idle. This only reduces GPU resources when you're not talking or making sounds","Force FP32 mode: on (THIS IS OFF BY DEFAULT! Turning this on improves stability. Increases VRAM usage by 200 MB)","Disable JIT compilation: off for faster loading speed of the program, on for slightly better performance (10-15 ms) for Nvidia only.","Convert to ONNX: Reduces delay and slightly reduces gpu usage. Enabling this increases CPU usage by around 5-10%. Reduces the quality of the voice a bit. If you decide to enable this, pair it with rmvpe_onnx for even less delay","Protect: Reduces the occurrence of robotic sibilants and robotic breathing, but also reduces the effect of the index file. Lower values increase this protection, higher values decrease it. The default value is 0.5, which means that the protection is disabled, reduce this value to 0.33 to enable it","Interface - Switch To Classic UI: It should use the same Web User Interface as the Original Wokada and Deiteris Fork, it's not suggested as it won't have all the new features, and is currently broken.","Skip Pass through confirmation: It will skip the confirmation when you enable Passthrough, not suggested."]},{"l":"Finding my own settings for Chunk Size and Extra Processing Time","p":["First start with 500 ms, check what number your Perf (in the top right of the Performance Stats) is and go closer to that number but not lower.","Example: if your perf is 200, go down to 250 with your chunk. Chunk affects perf value, and Extra as well.","If your perf value is green, your selected chunk is stable. You can experiment and go down in chunk for less delay, or increase extra for more quality (would not recommend to go above 2.7s extra. Anything above uses more resource for no clear benefit).","If your perf value is yellow, your selected chunk is enough, but audio may be unstable if you run other processes at the same time. Operation in this range will also incur high GPU usage. Increasing Chunk size or reducing Extra is recommended.","If your perf value is red, the realtime voice changer is unstable. Increase Chunk Size or reduce Extra Processing Time."]},{"l":"Extras"},{"l":"Information","p":["This fork is a lot better for AMD GPU's compared to the original w-okada. On the original it requires converting models to onnx models which is annoying, requires more CPU and GPU resources, has a lot more delay and other little inconveniences/bugs.","This fork is better for NVIDIA users who normally use the prebuilt w-okada version, because this version uses GPU accelerated extra compared to the original which uses CPU.","For the RTX GPUs the delay performance differences are minimal, but quality performance is better. For older cards like GTX or MX, this fork performs better in all aspects."]},{"l":"Reduce more Delay (Windows Only)"},{"l":"Prerequisite: Match Sample Rates (for both WASAPI & ASIO)","p":["Afterwards, download and install the FlexASIO GUI: FlexASIO GUI Download","Assuming you completed the prerequisite step, you can now select the correct inputs and outputs in the voice changer as follows:","AUDIO: SERVER","Backend: Windows WASAPI","Buffer Size:✅ Set to 256","Ch.: For both input and output, it's best to leave them to \"default\", the numbers are for true asio devices which flex isnt.","Click SAVE TO DEFAULT FLEXASIO.TOML. Do not forget this step. You can close the GUI afterwards.","Download and run the installer from here: FlexASIO Download","Ensure both options for Exclusive Mode are activated.","First, you need the .NET Desktop runtime. Download and install it from here: .NET 6.x Desktop runtime","Go to the last tab, Advanced, and set the sample rate to 48000 Hz.","Having the input latency at 0.0 can make your microphone crackle. Using 0.1 often works fine. If you experience crackles, experiment with this value (e.g., 0.12, 0.15) until it stops. The lower you can go, the better. If you don't want to experiment, you can keep it at 0.2.","I would recommend using WASAPI first if you are a normal user, as ASIO is more complex to set up.","If you don't know how to open your sound devices, press WIN+R, type \" mmsys.cpl\", then hit enter.","In the Advanced tab, adjust the sample rate to match your microphone: 48000 Hz.","In the voice changer app:","Input Device: Select your Microphone.","Input: Your Virtual Audio Cable (e.g., Line 1 Output)","Input:[WINDOWS WASAPI] Your Microphone","Latency:✅ Set Input Latency: 0.2; ✅ Set Output Latency: 0.2","Like WASAPI, ASIO accesses your audio devices directly, bypassing multiple layers within the Windows audio subsystem that \"MME\" (the default driver) has to go through. It has a lower algorithmic delay and can reduce total delay by 50-80ms.","Monitor: You can use the WASAPI Windows, you could also use windows directsound but that might cause an issue if matching sample rates doesnt fix it.","Navigate to the Recording tab, right-click on your microphone, and select Properties.","Now, go to the Playback tab. Right-click on your virtual audio cable (e.g., Line 1) and go to Properties.","Output Device: Select your Virtual Audio Cable (e.g., Line 1).","Output: Your Headphones/Speakers","Output:[WINDOWS WASAPI] Your Virtual Audio Cable (e.g., Line 1)","Output:✅ Set: ;✅ AutoConvert","Run FlexASIO GUI. If it doesn't open, you missed installing the .NET runtime from the previous step. Copy the following settings:","S.R.: Match the sample rate you chose above, which should be 48000.","Select AUDIO: Server","Select S.R.: 48000","Select the input and output from ASIO. You can select \"ALL\" in the first column to filter for ASIO devices to make it easier.","The Tg-Develop's Fork works with ASIO, while some older versions of the original w-okada do not.","Then, on your game or Discord, you select:","This first step is mandatory for both methods. You must select the same sample rate for your microphone and the virtual audio cable before proceeding.","WASAPI accesses your audio devices directly, while the driver that you use by default (which is \"MME\") goes through multiple layers within the Windows audio subsystem, causing more delay. This will in total cut down 50-80ms delay.","With the sample rates matched, you can now proceed to configure either WASAPI or ASIO.","You cannot use the noise suppression ( sup1, sup2) or echo functions in SERVER mode.","You did not match the sample rate of your virtual audio cable to your microphone. Return to the prerequisite step and ensure both are set to 48000 Hz.","You did not match the sample rate of your virtual audio cable to your microphone. Return to the prerequisite step and ensure both are set to the same value (48000 Hz)."]},{"l":"Help"},{"l":"How to fix \"Failed to download or verify\"","p":["After you start the program for the first time and it finished downloading files, but you have slow/unstable internet connection it might say Failed to download or verify: ... followed by \"Press Enter to continue\" at the end, then the pretrain download failed. To fix it, you can either:","Retry with a better connection later.","Go to the \"pretrain\" folder in the MMVCServerSIO folder.","Delete everything inside it if there is anything.","Download the Zipped Version of the Pretrained folder","Extract the pretrain.zip, be sure the pretrain folder contains only the files, not a pretrain folder inside another pretrain folder with the files.","Then run MMVCServerSIO.exe again, this time it should work."]},{"l":"Crackle Fix","p":["Open Task Manager > Details","Right click audiodg.exe and set priority to High","Right click audiodg.exe again > set affinity > uncheck everything except CPU 2 (only ✅ CPU 2, turn off the rest)","With a program called ProcessLasso you can automate this to always be active, since Windows resets these sometimes. Or you can open up CMD/Powershell (or make a bat file) and type in:","powershell ForEach($PROCESS in GET-PROCESS audiodg) { $PROCESS.ProcessorAffinity=4; $PROCESS.PriorityClass='High' }"]},{"l":"Discord Crackle Fix","p":["Make sure to do the Crackle Fixes in this step before doing this to see if it fixes your issue","If the voice sounds fine in the app AND it sounds fine in games, but ONLY sounds weird on discord, then:","Turn off Echo Cancellation","Turn off Noise Suppression (sometimes causes issues, maybe not. Check for yourself)"]},{"l":"GPU Idling","p":["Sometimes your GPU will start idling after the program is in the background for a while and affect performance.","In the folder where w-okada is located there should be a .bat file called force_gpu_clocks.bat, run that and it should fix your gpu idling.","Once you no longer want your gpu clock speed to be forced anymore you can run reset_gpu_clocks.bat."]},{"l":"FAQ"},{"l":"Why does it run in a browser and not it's own window?","p":["Because it uses a Web User Interface (WebUI) coded in JavaScript & TypeScript, the majority of (Open Source) AI programs are designed to run on the browser (even tho usually using things like Gradio) since it can be used both on cloud and locally. The original wokada also ran on a WebUI, just that it made it's own window."]},{"l":"What browser should I use?","p":["It's better you try and test, some people had issues on Chrome, some others on Firefox, it might depend on the settings you use and also Java/Type Script having issues. The browser that usually is reported by most people to have issues is OperaGX, which is why we don't suggest it much."]},{"l":"Why are most YouTube (Video) Tutorials old? Is there going to be an updated one?","p":["YouTube Tutorials take way more time to make, and get outdated easily in this case, as AI progresses fast and continues to change in better, with more different settings and versions. Written guides are easier to update, since you don't have to remake an entire video. It's unknown if we will ever release a video since they easily get outdated, but if we will, it will be linked inside of this guide."]},{"l":"Do I need an extremely expensive mic for good quality?","p":["We had a conversation about this in https://discord.com/channels/1159260121998827560/1159290161683767298/1352325982689951765& https://discord.com/channels/1159260121998827560/1159290161683767298/1356265862704926907, RVC works by downsampling your audio voice to 16khz because f0 estimators only works at that sample rate, after that the model outputs the results using it's original sample rate (without any upscaling). So there won't be the need of having a super extremely expensive, a decent one should do the job."]},{"l":"Are there unique Voice Models?","p":["RVC Voice Models need to be trained on something, so the models themselves can't be unique, but you can use the Merge Lab to create a new unique merged model."]},{"i":"section-2","l":"‎"}],[{"l":"Vonovox","p":["Last update: September 6, 2025","Vonovox is a realtime voice changer that uses RVC for its conversion.","Vonovox was developed by dr87.","RVC does NOT mean realtime voice changer. RVC means Retrieval-based-Voice-Conversion."]},{"l":"Is Vonovox Safe?","p":["RVC Models are PyTorch Models, a Python library used for AI. PyTorch uses serialization via Pythons' Pickle Module, converting the model to a file. Since pickle can execute arbitrary code when loading a model, it could be theoretically used for malware, but Vonovox has a built-in feature to prevent code execution along the model. Also, HuggingFace has a Security Scanner which scans for any unsafe pickle exploits and uses also ClamAV for scanning dangerous files.","‎"]},{"l":"Pros & Cons"},{"l":"*Learn more*","p":["Has an active development","Good Performance, and has even more improvements","Currently stable","It doesn't use a Web User Interface, meaning that it is less prone to errors and opens on it's own window","Easily reduces delay on Windows via facilitating the WASAPI/ASIO Backend process","Lets you choose the embedder, including Spin and ContentVec","Adds the pretty new swift f0 pitch extraction method","Uses TF32 Inference by default, which is more precise than FP16, and has very very slightly less precision/quality but better performance compared to FP32","Fixed 2.7+ Extra Time Cut Off Issues","Extra Effects, such as \"Noise Gate\"","Has an Update Checker at startup","Not Open Source (at the moment, but the dev might be working on an Open Source version)","Supports only Nvidia GPUs on Windows","It doesn't use a Web User Interface, meaning that it can't be run on the Cloud","Many Effects are Premium (paid), such as \"Low Quality Mic\""]},{"i":"section","l":"‎"},{"l":"System & Hardware Requirements","p":["Windows 10 or Later","and","At least 6GB of RAM","At least 6GB of free disk storage"]},{"l":"For GPU-conversion","p":["TLDR: Make sure you have Nvidia RTX 20xx better. GTX 10xx or RX 900 will also work, but may run into issues with games and higher delay. If you have an iGPU (mostly AMD Radeon Graphics or Vega) use Wokada Deiteris Fork Cloud instead.","Long answer:","Minimum:","A dedicated graphics card: Nvidia GeForce GTX 900 Series or later.","Recommended:","A dedicated graphics card Nvidia GeForce RTX 20XX Series or later."]},{"l":"Virtual Audio Cable"},{"l":"A Virtual Audio Cable (VAC) is what you need to use the realtime voice changer on Discord & Games.","p":["A VAC (Virtual Audio Cable) makes a fake audio device, used to re-route the audio of different programs.","In AI Realtime Voice Changing context, it's used to get the output of AI Converted Voice Output as the input in other programs such as Discord.","Download this: VAC Lite (Virtual-Audio-Cable by Muzychenko)","Run setup64, not 64a, after extracting the zip to a new folder","After installing the Virtual Cable, it changes your default audio system. Click Yes when it asks you to open the audio device settings (or press WIN+R, type \"mmsys.cpl\" if you closed it already), and change your Recording and Playback devices back to your usual devices. Same for communications device aswell (right click -> set as default communication device)"]},{"l":"Windows","p":["Make sure you have a Nvidia and a good enough one to run Vonovox. You don't know what GPU you have? Open Task Manager > Performance tab and check for your GPU0 and GPU1 names."]},{"i":"#","p":["Use Online Hosted if you have an integrated GPU (AMD Radeon Graphics ; AMD Radeon Vega ; Intel UHD) and if you do not have a GPU at all"]},{"l":"Download NVIDIA on Windows","p":["Make sure you have the Microsoft Visual C++ Redistributable Package, if you don't already.","Go to Vonovox's Github Repository and download the Latest Stable Release Source Code.","You could also optionally get access to Beta/ Early Access versions via Becoming a Vonovox Supporter (and also gaining Premium Effects) or checking for any Free Versions in the Vonovox Official Discord Server. They may have bugs, fixes or settings/options not explained in the guide yet.","If you have a GTX 800 card or below you can't use Vonovox."]},{"l":"Opening on Windows","p":["First Make sure you have 7zip or WinRAR for extracting / unzipping.","After the download extract the zip file. Open the folders until you see an .bat file called setup.bat and run that.","Vonovox will start downloading everything it needs to run. Be patient as it can take up to 5 minutes to download everything it needs.","Once it's done downloading everything it will display Setup complete! in the command line. You can now go ahead and run start.bat."]},{"l":"Opening on Multi-GPU Systems","p":["This is ONLY For users with 2 GPUs in the same system, you must do the following:","Open NVIDIA Control Panel","Go to Manage 3D Settings > Program Settings Tab","Add python.exe from the Vonovox runtime folder (runtime\\python.exe)","Set both settings \"CUDA - GPUs\" and \"OpenGL rendering GPU\" to the GPU you want to use for conversion","This will hide the other GPU from being used by the application which is required"]},{"l":"Voice Models"},{"l":"Adding Models"},{"i":"#","p":["In Model Presrts, click on a slot and select the .pth file.","Only RVC models will work. If you have a gpt-sovits one or any other, they will not work.","Select your .pth file and click upload.","If the model has an Index file, you can optionally add it for the trained accent, but it may cause CPU spikes.","You can also choose the Embedder, which depends if the model is trained with ContentVec (used by default for most models) or Spin (newer and can help with realtime)."]},{"l":"Changing Models","p":["If you wish to use a different a model, you can overwrite the model you are currently using with a new model."]},{"l":"Audio Setup"},{"l":"Discord & Games","p":["In Vonovox select:","Input: Your microphone","Output: Virtual Cable or your headphones if you wish to hear the model first","On discord and games, you select:","Input: Virtual Cable","Output: Your headphones"]},{"l":"Settings"},{"l":"Current Model Settings:","p":["Embedder: Select between contentvec or spin trained models. Most current models are trained on contentvec. Make sure you read the model's description to find out what embedder it uses. Spin has kinda better breaths, more robust to noise, has some training related differences, but it's less used and newer.","Pitch: This is the pitch. Going into negative will make it lower pitch (masculine), going higher will make it higher pitch (feminine). If you have a male voice using a female voice, aim for 10 - 14, this depends on your voice, try around those numbers until you find a sweet spot.","Formant: Alters harmonic frequencies and changes the voice timbre without affecting the pitch (AKA Formant Shift)."]},{"l":"Audio Device Settings:","p":["Audio Backend: Use WASAPI unless you have an ASIO interface and know what you're doing (advanced users)","Exclusive Mode: WASAPI exclusive mode. It has much lower latency but the issue is if you don't lock your gpu clocks with something like msi afterburner, it will pop nonstop , because it needs something like a ~ 45-50ms gpu delay max to function (advanced users)","Sample Rate: Only 48000Hz is available. This is only the outgoing sample rate that matches your VAC line - It is compatible with 32000, 40000, or 48000 models","F0 det: Pitch extraction algorithm. Both RMVPE (for the best precision and robustness) and FCPE (for less precision & robustness but lower delay) are good options. There's a recently new Swift option which might be more precise than RMVPE but it's not as much tested for RVC yet.","Pitch Smoothing Factor: Pitch smoothing will dampen pitch changes. It still follows the exact curve of the f0 predictor allowing it to maintain 100% accuracy, just to a lower magnitude. This allows normal speaking voices to have better stability, since sometimes f0 can be over aggressive and cause pitch wobble on minor pitch fluctuations.","Output volume: Controls how loud the output volume is."]},{"l":"Noise Reduction:","p":["Noise reduction algorithms are not compatible with singing or whispering. Turn them off if you need to sing or whisper.","RNNoise Reduction: Greatly filters input background noise for very minimum latency. This can mitigate the chances of Vonovox trying to infer on noise.","VAD Noise Reduction: Completely mutes the output when speech is not detected. When speech is detected, it uses a 400ms release window. It is also much better at filtering breathe noises than RNNoise.","AP-BWE 48k Upscaler: This is an upscaler that extends the bandwidth of speech by adding missing frequency information up to 48k."]},{"l":"Voice Settings:","p":["Block Size: Critical setting. The optimal block size is the lowest you can get without audio being choppy. Listen to your output. This is GPU dependent, the more powerful the gpu, the lower the block size you can use. However the optimizations I made allow much smaller block sizes to work on lower end GPUs. At extremely low block sizes, quality may be reduced. This setting is similar to the Chunk in Wokada Deiteris Fork. Vonovox 0.30 Block size = Wokada Deiteris Fork 300ms Chunk. Use the GPU Delay to adjust it.","Extra Time: Gives the model more or less context to work with. Recommended 2.0 for best quality/latency ratio. The added latency of this setting is far less impactful than the block size. This setting is known as Extra in Wokada Deiteris Fork, and Vonovox fixed the certain cut off issues experienced in some models over the value 2.7.","Crossfade Duration: Controls how smoothly the AI stitches different processed parts \"chunks\" of your voice back together. 0.08-0.1 or 0.15 (0.08-0.1 for fastest voice, 0.15 for improved quality but increases delay by ~ 50 ms)"]},{"l":"Effects","p":["Most of the default values are already decent.","Processing from all effects, premium and free, are done directly in the pipeline as the output voice is being produced, making them extremely low latency. Many effects can greatly enhance voice quality if used properly, while some are just for fun.","Note: If you move sliders while in the middle of speaking, sound will have some minor popping. This is completely normal as you are applying effects in the middle of a block of audio being processed."]},{"l":"Basic Effects","p":["Those are the Free Effects.","Noise Gate: A simple noise gate so the application doesn't try to process low background noise that made it past RNNoise","EQ Band (1 2): Boosts or cuts specific frequency ranges of your voice to shape its overall tone.","Frequency (Hz): Selects the center of the frequency range you want to adjust.","Gain (dB): Sets how much to raise or lower the volume of the selected frequency.","Q: Adjusts the width of the frequency band. A low Q affects a wide range of frequencies, while a high Q is more narrow and precise."]},{"l":"Premium Effects","p":["Add Static: Overlays crackling static noise for a more distorted effect.","Attack (ms): How quickly the compressor reacts to loud sounds.","Chorus: Makes a single voice sound like multiple voices by adding slightly detuned and delayed copies.","Compressor: Evens out your voice's volume, preventing sudden loud peaks and making quieter sounds more audible.","Cutoff (Hz): The frequency above which sounds will be removed.","Cutoff (Hz): The frequency below which sounds will be removed.","Damping: Absorbs high frequencies in the echoes, making the reverb sound warmer or darker.","Delay (ms): Sets the time delay between your original voice and the copies.","Depth: Determines the intensity of the pitch variations.","Dry Level: Adjusts the volume of your original, unprocessed voice.","EQ Band (3 4): Boosts or cuts specific frequency ranges of your voice to shape its overall tone.","Feedback: Feeds some of the effected sound back into the input for a more intense, swirling effect.","Frequency (Hz): Selects the center of the frequency range you want to adjust.","Gain (dB): Sets how much to raise or lower the volume of the selected frequency.","High Pass Filter (12 dB/oct): Cuts low frequencies, which can remove low-end rumble and make a voice sound thinner.","Low Pass Filter (24 dB/oct): Cuts high frequencies, making the voice sound more muffled or distant.","Low Quality Mic: Simulates the sound of a bad microphone or a telephone call.","Mix: Blends the amount of the original (dry) voice with the chorused (wet) voice.","Q: Adjusts the width of the frequency band. A low Q affects a wide range of frequencies, while a high Q is more narrow and precise.","Rate (Hz): Controls the speed of the subtle pitch variations in the chorus effect.","Ratio: How much the volume is turned down after crossing the threshold.","Release (ms): How quickly the compressor stops after the sound is no longer loud.","Resonance: Adds a slight boost to frequencies right at the cutoff point for emphasis.","Reverb: Simulates the sound of being in a physical space by adding echoes and reflections.","Room Size: Controls the perceived size of the simulated room, from a small closet to a large hall.","Strength: Controls the overall intensity of the low-quality sound effect.","Telephone Effect: Narrows the frequency range to mimic the sound of a phone line.","Those are the Paid Effects, you can learn how to get them by clicking \"Manage License\" -> \"How To Purchase\".","Threshold (dB): The volume your voice must reach before the effect starts turning it down.","Wet Level: Adjusts the volume of the reverb effect itself."]},{"l":"Update","p":["To Update Vonovox, you can either:","Click the Update Check Symbol at the bottom right of the program.","Download the latest source code the next time a new version comes out, replace the files, run setup.bat and start.bat."]},{"l":"Extras"},{"l":"Realtime Sound File Inferencing","p":["You are able to load and play sound files, converted to your model's voice in realtime.","The sound file replaces your input mic while active. Whatever sound is coming from your loaded file is your \"new microphone\" while the sound is playing. That means it will infer and play the sound file as if it was your own voice in realtime. You can play speech, singing, or whatever you want. Just make sure the audio is clean, as the client still needs to inference it, no different than the real mic.","When a sound file is playing, it will zero out the input from your real mic, meaning you don't have to worry about overlapping your voice with playback. Mic will automatically unmute when sound is playing again. Also mute and unmute is handled properly when pausing and resuming the playback of audio files.","Seek timer and playback timer so you can go to specific times in your sound file.","‎","If you are playing singing files with high pitch, you must turn off all noise suppression options as suppression models are trained on speech, not high pitch singing.","Supports wav, mp3 and flac."]},{"l":"Known Issues / Bugs","p":["License caching is currently not working, meaning some licenses might not work."]},{"l":"FAQ"},{"l":"Do I need an extremely expensive mic for good quality?","p":["We had a conversation about this in https://discord.com/channels/1159260121998827560/1159290161683767298/1352325982689951765& https://discord.com/channels/1159260121998827560/1159290161683767298/1356265862704926907, RVC works by downsampling your audio voice to 16khz because f0 estimators only works at that sample rate, after that the model outputs the results using it's original sample rate (without any upscaling). So there won't be the need of having a super extremely expensive, a decent one should do the job."]},{"l":"How can I hear myself?","p":["Currently, Vonovox is missing the Monitor feature in Wokada Deiteris Fork, till it get's added, you have to use some workarounds:","Press WINDOWS+R, type \"mmsys.cpl\" and press Enter. Go to the Recording devices, find Line 1 and check it's Properties, go to the Listen tab and check \"Listen to this device\"."]},{"l":"What are the benefits of premium? Is it forever or monthly?","p":["You can get premium by a monthly subscription at dr87's Patreon, but the creator said he might make a lifetime version. The benefits are:","Early access.","Premium Effects and Features.","Supporter role and access to the Vonovox Official Discord Server."]},{"l":"Why are there Multiple EQ Bands Effects, which some are free and some others are paid?","p":["Having Multiple EQ bands provides the flexibility to precisely shape and refine the tone of your voice far more effectively than a single band ever could. It's made so you can adjust multiple parts of your voice range with each."]},{"i":"section-3","l":"‎"}],[{"l":"Deiteris' W Okada Fork","p":["Last update: September 6, 2025","W-Okada is a realtime voice changer that uses RVC for its conversion.","There are 3 versions of this realtime voice changer, the Offical Original W-Okada made by Wok, the Deiteris' Fork made by Deiteris, and the Tg-Develop's Fork made by Tg-Develop. Note that those 3 links are just for reference to the Source Code Github Repositories of both projects, you should instead follow the guide below.","This guide will be about the Wokada Deiteris' fork since it has better preformance and quality compared to the Original Wokada.","RVC does NOT mean realtime voice changer. RVC means Retrieval-based-Voice-Conversion."]},{"l":"Is The W-Okada Deiteris Fork Safe?","p":["RVC Models are PyTorch Models, a Python library used for AI. PyTorch uses serialization via Pythons' Pickle Module, converting the model to a file. Since pickle can execute arbitrary code when loading a model, it could be theoretically used for malware, but Wokada Deiteris Fork has a built-in feature to prevent code execution along the model. Also, HuggingFace has a Security Scanner which scans for any unsafe pickle exploits and uses also ClamAV for scanning dangerous files.","‎"]},{"l":"Pros & Cons"},{"l":"*Learn more*","p":["Currently stable","Good Performance","Has great support for Nvidia, AMD, Intel, Mac, Linux, Windows","Uses a Web User Interface, meaning it can be run on the Cloud","Uses FP16 Inference by default, and let's you choose to use FP32 for better quality/precision","Uses a Web User Interface, having issues on some browsers, and bugs with renaming or deleting models on it","Doesn't have an active development recently","Has Cut Off Issues Using an Extra superior to 2.7","Doesn't let you choose the embedder, using only RVC models trained on contentvec (the majority)"]},{"i":"section","l":"‎"},{"l":"System & Hardware Requirements","p":["Windows 10 or Later","macOS 12 Monterey or later. With Apple Silicon or Intel CPU","Any Linux Distro","and","At least 6GB of RAM","At least 6GB of free disk storage"]},{"l":"For GPU-conversion","p":["TLDR: Make sure you have Nvidia RTX 20xx or AMD Radeon RX 5xxx or better. GTX 10xx or RX 580 will also work, but may run into issues with games and higher delay. If you have an iGPU (mostly AMD Radeon Graphics or Vega) use online hosted alternative instead.","Long answer:","Minimum:","An integrated graphics card: AMD Radeon Vega 7 (with AMD Ryzen 5 5600G) or later; with 2GB VRAM (in FP32 mode), ~ 1GB VRAM (in FP16 mode, if supported). But this is NOT recommended at all and we will most likely not recommend you to download the realtime voice changer with iGPUs.","A dedicated graphics card: Nvidia GeForce GTX 900 Series or later, or AMD Radeon RX 400 series or later, or Intel Arc A300 series or later.","Recommended:","A dedicated graphics card Nvidia GeForce RTX 20 Series or later, or AMD Radeon RX 5000 series or later, or Intel Arc A500 series or later."]},{"l":"For CPU-conversion","p":["TLDR: don't bother. You can't run games, discord usage might be the only thing that will work decently, but you might potentially damage your CPU. People with no GPU usually have old CPU's, so delay will be high too. Not worth it.","Minimum:","Intel Core i5-4690K or AMD FX-6300.","Recommended:","Intel Core i5-10400F or AMD Ryzen 5 1600X.","If you plan on playing games at the same, do not use CPU-conversion. With CPU, the delay will be massive and your PC will not run smoothly at all. If you have a higher-end CPU you can make it work, but those that have higher end CPUs most likely also have higher end GPUs, so you should be using your GPU if possible."]},{"l":"Online Alternatives [Colab/Kaggle]"},{"l":"Kaggle","p":["It's free, but you will need a phone number verification.","Read the Tutorial HERE"]},{"l":"Lightning.AI","p":["It has free credits, but you will need a phone number verification.","Read the Tutorial HERE"]},{"l":"Google Colab","p":["You need the Google Colab Paid Tier to run this, as it uses a Web User Interface, else you could risk getting disconnected or getting banned off Colab.","Read the tutorial HERE"]},{"l":"Virtual Audio Cable"},{"l":"A Virtual Audio Cable (VAC) is what you need to use the realtime voice changer on Discord & Games.","p":["A VAC (Virtual Audio Cable) makes a fake audio device, used to re-route the audio of different programs.","In AI Realtime Voice Changing context, it's used to get the output of AI Converted Voice Output as the input in other programs such as Discord.","Download this: VAC Lite (Virtual-Audio-Cable by Muzychenko).(Be sure to not use any toher vac like VB Audio Cable.)","Run setup64, not 64a, after extracting the zip to a new folder","After installing the VAC Lite, it changes your default audio system. Click Yes when it asks you to open the audio device settings (or press WIN+R, type \"mmsys.cpl\" if you closed it already), and change your Recording and Playback devices back to your usual devices. Same for communications device aswell (right click -> set as default communication device)","Download either: Blackhole Virtual Audio Cable or VB-Audio","For Debian / Ubuntu-based Systems (Ubuntu, Mint, Pop!_ OS), run in the terminal:","For Fedora / RHEL-based Systems (CentOS, Rocky Linux), run in the terminal:","For Arch / Arch-based Systems (Endeavour, Manjaro Linux), run in the terminal:"]},{"l":"Windows","p":["Download based on your GPU. You don't know what GPU you have? Open Task Manager > Performance tab and check for your GPU0 and GPU1 names. Prioritize the Nvidia one if you have one, else use the other."]},{"i":"#","p":["Use Online Hosted if you have an integrated GPU (AMD Radeon Graphics ; AMD Radeon Vega ; Intel UHD) and if you do not have a GPU at all"]},{"l":"Download NVIDIA on Windows","p":["The latest version as of December 7th 2024 is: nvidia-b2332 (click here to download)","If you have a GTX 700 card or below, use AMD/Intel version instead."]},{"l":"Download NVIDIA RTX 5000-series on Windows","p":["NVIDIA RTX-5000 series, the newest release of GPU's, require a separate download. You do not need it if you have an older GPU, follow the normal Nvidia link in that case. nvidia-5000-Series (click here to download)","Download all 3 files, then extract the .zip file, it will automatically extract ALL 3 FILES into one. Then open the MMVCServerSIO folder and run MMVCServerSIO.exe(or called MMVCServerSIO if you don't have extensions activated)."]},{"l":"Download AMD, INTEL and CPU on Windows","p":["The latest version as of December 7th 2024 is: dml-b2332 (click here to download)","Intel UHD Graphics do NOT work at this point in time. Use Online Alternative."]},{"l":"Opening on Windows","p":["First Make sure you have 7zip or WinRAR for extracting / unzipping.","After the download, you extract the zip file. You open the folders until you see an exe application called MMVCServerSIO and run that.","If nothing opens, then open a browser and type in http://127.0.0.1:18888/. This is a local URL, it runs on the WebUI."]},{"l":"Mac"},{"l":"Download Mac Silicon","p":["The latest version as of December 7th 2024 is: arm-b2332 (click here to download)"]},{"l":"Download Mac Intel","p":["The latest version as of December 7th 2024 is: macos-amd-b2332 (click here to download)"]},{"l":"Opening on Mac","p":["Double click the voice-changer-macos-arm64-cpu.tar.gz file. The realtime voice changer will unpack and the MMVCServerSIO folder will appear.","Open the extracted MMVCServerSIO folder.","Double-click MMVCServerSIO to run the realtime voice changer.","You do not get a popup notification for this, so if it does not open or says \"Pytorch is damaged\", do the following:","Open the Terminal","Run the following command: xattr -dr com.apple.quarantine PUT IN THE PATH TO YOUR MMVCServerSIO FOLDER HERE For example, if you extracted the realtime voice changer to your desktop, the command may look as follows: xattr -dr com.apple.quarantine ~/Desktop/MMVCServerSIO","Now, open the extracted MMVCServerSIO folder and run MMVCServerSIO to run the realtime voice changer.","If nothing opens, then open a browser and type in http://127.0.0.1:18888/. This is a local URL, it runs on the WebUI."]},{"l":"Linux","p":["Installation of CUDA Toolkit or AMD HIP SDK is NOT REQUIRED. All other necessary libraries are bundled with the application."]},{"l":"Download on Nvidia on Linux","p":["you need to download both these files:","https://github.com/deiteris/voice-changer/releases/download/b2332/voice-changer-linux-amd64-cuda.tar.gz.aa","https://github.com/deiteris/voice-changer/releases/download/b2332/voice-changer-linux-amd64-cuda.tar.gz.ab"]},{"l":"Download on AMD on Linux","p":["you need to download all these files:","https://github.com/deiteris/voice-changer/releases/download/b2332/voice-changer-linux-amd64-rocm.tar.gz.aa","https://github.com/deiteris/voice-changer/releases/download/b2332/voice-changer-linux-amd64-rocm.tar.gz.ab","https://github.com/deiteris/voice-changer/releases/download/b2332/voice-changer-linux-amd64-rocm.tar.gz.ac"]},{"l":"Download on CPU on Linux","p":["you need only this file:","https://github.com/deiteris/voice-changer/releases/download/b2332/voice-changer-linux-amd64-cpu.tar.gz"]},{"l":"Opening on Linux","p":["I'm not sure about the capabilities of UI tar archive extractors, but you can extract these archive parts with the following command that will merge them and extract: cat voice-changer-linux-amd64-cuda.tar.gz.* | tar xzf -, change cuda to rocm or cpu depending on your PC GPU.","After you extract the files using the command above, a new folder called MMVCServerSIO will appear.","Open a Terminal and navigate into that folder:","You may need to make the application executable. Run this command just in case:","Now, run the realtime voice changer:","After the server finishes loading in your terminal, it will not open a window on its own. Open a web browser and go to http://127.0.0.1:18888/ to access the user interface."]},{"l":"Opening on Multi-PC Setups","p":["This is only for the people that have 2 PCs, and want to use 1 PC for Gaming, the other only for Wokada Deiteris Fork.","Create a file named .env on the same folder where MMVCServerSIO.exe is located. Open it up with a notepad, copy paste the settings from the GitHub link.","After that, you create another file with the file extension ending .bat, open it up with a notepad, copy paste what is needed in there again from the GitHub link.","Now run the bat file. After it starts, you should be able to open the link. For example, if you specified HOST=192.168.0.1 and ALLOWED_ORIGINS='[https://192.168.0.1:18888]'), you should be able to open https://192.168.0.1:18888 in your browser and use the realtime voice changer UI from other machines in your local network."]},{"l":"Voice Models"},{"l":"Managing Models"},{"l":"Adding Models"},{"i":"#","p":["Click on Edit on the small blue square located around the the top left side","Pick any slot you want, click upload","Only RVC models will work. If you have a gpt-sovits one or any other, they will not work.","Select Type: RVC, then select file on the Model slot and upload your .pth file.","No need for an Index file, but you can upload it. This controls the accent of the voice model."]},{"l":"Renaming Models","p":["Attempting to rename a model directly within the Web User Interface will cause the program to crash. This is a known bug. Use one of the two methods below to safely rename your models.","Method 1: Re-uploading the Model","This is the simplest method.","Find the model's .pth file on your computer.","Rename the file to your desired new name.","In the voice changer UI, click Edit, select the slot of the model you want to rename, and click upload.","Re-upload the renamed .pth file to the same slot. This will overwrite the old model and update its name.","Method 2: Editing the Configuration File","This method doesn't require re-uploading.","Navigate to your MMVCServerSIO folder.","Inside, open the model_dir folder. You will see several numbered folders, each corresponding to a model slot in the UI.","Open the folder for the slot number you want to rename.","Inside this folder, you will find a params.json configuration file. Open this file with a text editor like Notepad.","Look for the name: field in the file. Change the text in the quotes to your desired new model name.","Save the .json file. The name will be updated in the voice changer UI."]},{"l":"Deleting Models","p":["If you wish to delete a model, you can simply overwrite the slot with a new model by following the steps in the Adding Models section. If you want to completely empty a slot, navigate to the MMVCServerSIO/model_dir folder, open the folder of the slot number you want to delete, and delete all the files inside it."]},{"l":"Merging Models (Merge Lab)","p":["The Merge Lab allows you to combine multiple RVC V2 voice models (.pth Weights only, not indexs too) into a single, new hybrid model. This is useful for creating unique voices.","Open Merge Lab: Scroll down in the user interface and click on the Merge Lab button.","Select Model Type: From the Type dropdown menu, choose the type of models you wish to merge. Only models that share the same sample rate and type (e.g., \"pyTorchRVCv2, 32000Hz, 768\" which are all RVC v2 models with the 32kHz Sample Rate, or \"pyTorchRVC, 40000Hz, 256\" which are all RVC v1 models with the 40kHz Sample Rate) will be shown and can be merged together.","Adjust Weights: Use the sliders next to each model's name to set its \"weight\" (RVC models are PyTorch files, the .pth is the weight containing the voice) or influence in the merged model. The numbers (from 0 to 100) represent the percentage of each voice in the mix.","Merge and Download: Once you have set the desired proportions, click the Merge button. Your browser will automatically download the new, merged.pth model file, which you can rename to whatever you want.","The merged model is not automatically added to your model list. You must upload it to an empty slot yourself by following the steps in the Adding Models section.","You can't merge indexs(in rvc context, the trained accent of the voice). Only the .pth actual voice file."]},{"l":"Audio Setup"},{"l":"Discord & Games","p":["On the realtime voice changer app wokada, you select:","Input: Your microphone","Output: Virtual Audio Cable","Monitor (if you wish to hear the realtime voice changer on your headphones aswell): Your headphones","On discord and games, you select:","Input: Virtual Audio Cable","Output: Your headphones"]},{"l":"Client and Server Setup","p":["Audio: CLIENT","Uses MME (normal audio processed through windows. You use this automatically with every application)","You can use the boxes echo, sup1, sup2 using this","Audio: SERVER","Use S.R. 48000","I recommend using [Windows WASAPI] on all prefixes for less delay, because this uses your audio devices (e.g. microphone) directly, before processing through windows.","Both Input and Output has to be the same (Windows WASAPI), you can't use MME for input and then Windows WASAPI for Output.","You can not use the in-built noise suppressions in this mode","ASIO > WASAPI > MME as a general thumbrule (this also affects delay)","Sometimes Client does not work, then use SERVER with prefix \"MME\" or \"Windows WASAPI\". You can not use the in-built noise suppression and echo fix if you use SERVER."]},{"l":"Settings Explained","p":["PASSTHRU button: Sends your actual voice and not the realtime voice changer through the virtual audio cable. You want this to be GLOWING GREEN or GREY (grey for dark mode users) for the realtime voice changer to work.","F0 det: Pitch extraction algorithm. Both RMVPE (for the best precision and robustness) and FCPE (for less precision & robustness but lower delay) are good options.","Chunk: Controls the delay (lower number means less delay, but please check out the recommended settings for what your GPU is capable of).","Extra: Controls voice model quality. 2.7s is the max, anything above can cause cutoff issues."]},{"l":"VOL:","p":["in: This raises the microphone volume before it goes into the realtime voice changer (Recommended to leave it on the default or if needed, not to go too high, else it increases background noise and makes the voice sound worse).","OUT: Raising realtime voice changer volume on the output.","MON: Increases volume of your headphones that you set on \"mon\" if you selected to hear yourself with the realtime voice changer.","Pitch: This is the pitch. Going into negative will make it lower pitch, going higher will make it higher pitch. If you have a male voice using a female voice, aim for 10 - 14, this depends on your voice, try around those numbers until you find a sweet spot.","Formant Shift: Alters harmonic frequencies and changes the voice timbre without affecting the pitch","Index: This controls the accent of the voice model. In most cases, using Index on Realtime Voice Changer can add realism if you speak the language the model was trained in. If you have a heavy foreign accent, you may use this at a low rate. Beware, this increases CPU usage","In. Sens: microphone threshold, increasing this will cause less background noise to get picked up if it's a problem","Sup2: Noise suppression on your microphone.","Sup1: Noise suppression but weaker, not recommended to use this at all, because it barely has any impact whilst reportedly, making the voice inconsistent","Echo: if you experience echo issues despite having sup2, In. Sens to the right and having lowered your windows system value, then this will help you as a last resort"]},{"l":"Settings"},{"l":"Advanced Settings","p":["Protocol: rest (Use SIO if you want less delay but if you encounter any issues with SIO switch back to rest. Rest has slightly more delay than SIO)","Crossfade length: Controls how smoothly the AI stitches different processed parts \"chunks\" of your voice back together. 0.1 or 0.15 (0.1 for fastest voice, 0.15 for improved quality but increases delay by 50 ms)","SilenceFront: Reduce GPU usage when idle. This only reduces GPU resources when you're not talking or making sounds","Force FP32 mode: on (THIS IS OFF BY DEFAULT! Turning this on improves stability. Increases VRAM usage by 200 MB)","Disable JIT compilation: off for faster loading speed of the program, on for slightly better performance (10-15 ms) for Nvidia only.","Convert to ONNX: Reduces delay and slightly reduces gpu usage. Enabling this increases CPU usage by around 5-10%. Reduces the quality of the voice a bit. If you decide to enable this, pair it with rmvpe_onnx for even less delay","Protect: Reduces the occurrence of robotic sibilants and robotic breathing, but also reduces the effect of the index file. Lower values increase this protection, higher values decrease it. The default value is 0.5, which means that the protection is disabled, reduce this value to 0.33 to enable it"]},{"l":"Finding my own settings for Chunk","p":["First start with 500 ms, check what number your perf is and go closer to that number but not lower.","Example: if your perf is 200, go down to 250 with your chunk. Chunk affects perf value, and Extra as well.","If your perf value is green, your selected chunk is stable. You can experiment and go down in chunk for less delay, or increase extra for more quality (would not recommend to go above 2.7s extra. Anything above uses more resource for no clear benefit).","If your perf value is yellow, your selected chunk is enough, but audio may be unstable if you run other processes at the same time. Operation in this range will also incur high GPU usage. Increasing Chunk size or reducing Extra is recommended.","If your perf value is red, the realtime voice changer is unstable. Increase chunk size or reduce Extra."]},{"l":"Known working settings for Chunk and Extra","p":["100 - 120 + 2.7s extra","110 - 130 ms chunk + 2.7s extra","128 ms + 2.7s extra","128ms + 2.7s extra","140 - 180 ms chunk + 2.7s extra","140 - 200ms + 2.0s extra","200 ms chunk + 2.0s extra","250 ms chunk + 1.0s extra","256 ms + 2.7s extra","30 - 60 ms chunk + 2.7s extra","50 - 80 ms chunk + 2.7s extra","50 - 90 ms chunk + 2.7s extra","500 ms chunk + 0.6s extra","5xxx cards","5xxx XT cards","60 - 80 ms + 2.7s extra","60 - 90 ms chunk + 2.7s extra","600 ms + 0.6s extra","6xxx cards","6xxx XT cards","70 - 100 ms + 2.7s extra","700 ms + 1.0s extra","7xxx cards","7xxx XT cards","80 - 120 ms + 2.7s extra","AMD Radeon RX Vega 10 (with Ryzen 7 3700U)","AMD Radeon RX Vega 8 (with Ryzen 3 3200G)","AMD Radeon(TM) Graphics (with Ryzen 7 5800H)","bugged 256 ms + 2.7s extra","fcpe ; for chunk check the perf number and add 50 to it ; 1.0s extra","fcpe ; for chunk check the perf number and add 50 to it ; 2.7s extra","fcpe + 230ms + 2.7s extra","GTX 10xx-series","GTX 16xx-series","GTX 900-series","If you are playing a video game with the realtime voice changer, you will have to increase the chunk higher than what you usually can handle. This is because the game runs on GPU and the realtime voice changer aswell. The game will always take higher priority by default, so the listed settings are safe options that should run with most games. If you run into issues, you will need to lower quality and limit your FPS, or increase chunk. It is best to first tweak your game's settings first","It is recommended to go up to Finding my own settings after you are comfortable with the program","Mac M1","Mac M1 Air","Mac M2","Mac M2 Air","MX 330","perf number + 100 ms chunk","perf number + 40 ms chunk","perf number + 50 ms chunk","perf number + 60 ms chunk","perf number + 80 ms chunk","rmvpe_onnx + 260 ms + 0.6s extra","rmvpe_onnx + 650ms + 1.0s extra","RTX xx50 (e.g. 3050)","RTX xx60 (e.g. 3060)","RTX xx60 Ti (e.g. 3060 Ti)","RTX xx70 (e.g. 3070)","RTX xx70 Ti (e.g. 3070 Ti)","RTX xx80 (e.g. 3080)","RTX xx80 Ti (e.g.3080 Ti)","RTX xx90 (e.g. 3090)","RX 560","RX 570","RX 580","RX 6600M","Ryzen 7 5800x"]},{"l":"Extras"},{"l":"Information","p":["This fork is a lot better for AMD GPU's compared to the original w-okada. On the original it requires converting models to onnx models which is annoying, requires more CPU and GPU resources, has a lot more delay and other little inconveniences/bugs.","Example: AMD RX 6650 XT lowest latency is 298 ms chunk on original w-okada. On this fork lowest latency is around 60 - 80 ms chunk","Deiteris' fork is better for NVIDIA users who normally use the prebuilt w-okada version, because this version uses GPU accelerated extra compared to the original which uses CPU.","For the RTX GPUs the delay performance differences are minimal, but quality performance is better. For older cards like GTX or MX, this fork performs better in all aspects.","Example: NVIDIA RTX 3070 on prebuilt w-okada reaches 170 - 213 ms chunk latency. On manually set up environment of w-okada reaches 42 ms chunk latency. On this fork it can reach 30 - 38 ms chunk latency, depending on the extra set. Keep in mind these are settings tested to the max, without a video game or intense operations running in the background"]},{"l":"Reduce more Delay (Windows Only)"},{"l":"Prerequisite: Match Sample Rates (for both WASAPI & ASIO)","p":["Afterwards, download and install the FlexASIO GUI: FlexASIO GUI Download","Assuming you completed the prerequisite step, you can now select the correct inputs and outputs in the voice changer as follows:","AUDIO: SERVER","Backend: Windows WASAPI","Buffer Size:✅ Set to 256","Ch.: For both input and output, it's best to leave them to \"default\", the numbers are for true asio devices which flex isnt.","Click SAVE TO DEFAULT FLEXASIO.TOML. Do not forget this step. You can close the GUI afterwards.","Download and run the installer from here: FlexASIO Download","Ensure both options for Exclusive Mode are activated.","First, you need the .NET Desktop runtime. Download and install it from here: .NET 6.x Desktop runtime","Go to the last tab, Advanced, and set the sample rate to 48000 Hz.","Having the input latency at 0.0 can make your microphone crackle. Using 0.1 often works fine. If you experience crackles, experiment with this value (e.g., 0.12, 0.15) until it stops. The lower you can go, the better. If you don't want to experiment, you can keep it at 0.2.","I would recommend using WASAPI first if you are a normal user, as ASIO is more complex to set up.","If you don't know how to open your sound devices, press WIN+R, type \" mmsys.cpl\", then hit enter.","In the Advanced tab, adjust the sample rate to match your microphone: 48000 Hz.","In the voice changer app:","Input Device: Select your Microphone.","Input: Your Virtual Audio Cable (e.g., Line 1 Output)","Input:[WINDOWS WASAPI] Your Microphone","Latency:✅ Set Input Latency: 0.2; ✅ Set Output Latency: 0.2","Like WASAPI, ASIO accesses your audio devices directly, bypassing multiple layers within the Windows audio subsystem that \"MME\" (the default driver) has to go through. It has a lower algorithmic delay and can reduce total delay by 50-80ms.","Monitor: You can use the WASAPI Windows, you could also use windows directsound but that might cause an issue if matching sample rates doesnt fix it.","Navigate to the Recording tab, right-click on your microphone, and select Properties.","Now, go to the Playback tab. Right-click on your virtual audio cable (e.g., Line 1) and go to Properties.","Output Device: Select your Virtual Audio Cable (e.g., Line 1).","Output: Your Headphones/Speakers","Output:[WINDOWS WASAPI] Your Virtual Audio Cable (e.g., Line 1)","Output:✅ Set: ;✅ AutoConvert","Run FlexASIO GUI. If it doesn't open, you missed installing the .NET runtime from the previous step. Copy the following settings:","S.R.: Match the sample rate you chose above, which should be 48000.","Select AUDIO: Server","Select S.R.: 48000","Select the input and output from ASIO. You can select \"ALL\" in the first column to filter for ASIO devices to make it easier.","The Deiteris Fork works with ASIO, while some older versions of the original w-okada do not.","Then, on your game or Discord, you select:","This first step is mandatory for both methods. You must select the same sample rate for your microphone and the virtual audio cable before proceeding.","WASAPI accesses your audio devices directly, while the driver that you use by default (which is \"MME\") goes through multiple layers within the Windows audio subsystem, causing more delay. This will in total cut down 50-80ms delay.","With the sample rates matched, you can now proceed to configure either WASAPI or ASIO.","You cannot use the noise suppression ( sup1, sup2) or echo functions in SERVER mode.","You did not match the sample rate of your virtual audio cable to your microphone. Return to the prerequisite step and ensure both are set to 48000 Hz.","You did not match the sample rate of your virtual audio cable to your microphone. Return to the prerequisite step and ensure both are set to the same value (48000 Hz)."]},{"l":"Help"},{"l":"How to fix \"Failed to download or verify\"","p":["After you start the program for the first time and it finished downloading files, but you have slow/unstable internet connection it might say Failed to download or verify: ... followed by \"Press Enter to continue\" at the end, then the pretrain download failed. To fix it, you can either:","Retry with a better connection later.","Go to the \"pretrain\" folder in the MMVCServerSIO folder.","Delete everything inside it if there is anything.","Download the Zipped Version of the Pretrained folder","Extract the pretrain.zip, be sure the pretrain folder contains only the files, not a pretrain folder inside another pretrain folder with the files.","Then run MMVCServerSIO.exe again, this time it should work."]},{"l":"Crackle Fix","p":["Open Task Manager > Details","Right click audiodg.exe and set priority to High","Right click audiodg.exe again > set affinity > uncheck everything except CPU 2 (only ✅ CPU 2, turn off the rest)","With a program called ProcessLasso you can automate this to always be active, since Windows resets these sometimes. Or you can open up CMD/Powershell (or make a bat file) and type in:","powershell ForEach($PROCESS in GET-PROCESS audiodg) { $PROCESS.ProcessorAffinity=4; $PROCESS.PriorityClass='High' }"]},{"l":"Discord Crackle Fix","p":["Make sure to do the Crackle Fixes in this step before doing this to see if it fixes your issue","If the voice sounds fine in the app AND it sounds fine in games, but ONLY sounds weird on discord, then:","Turn off Echo Cancellation","Turn off Noise Suppression (sometimes causes issues, maybe not. Check for yourself)"]},{"l":"GPU Idling","p":["Sometimes your GPU will start idling after the program is in the background for a while and affect performance.","In the folder where w-okada is located there should be a .bat file called force_gpu_clocks.bat, run that and it should fix your gpu idling.","Once you no longer want your gpu clock speed to be forced anymore you can run reset_gpu_clocks.bat."]},{"l":"FAQ"},{"l":"Why does it run in a browser and not it's own window?","p":["Because it uses a Web User Interface (WebUI) coded in JavaScript & TypeScript, the majority of (Open Source) AI programs are designed to run on the browser (even tho usually using things like Gradio) since it can be used both on cloud and locally. The original wokada also ran on a WebUI, just that it made it's own window."]},{"l":"What browser should I use?","p":["It's better you try and test, some people had issues on Chrome, some others on Firefox, it might depend on the settings you use and also Java/Type Script having issues. The browser that usually is reported by most people to have issues is OperaGX, which is why we don't suggest it much."]},{"l":"Why are most YouTube (Video) Tutorials old? Is there going to be an updated one?","p":["YouTube Tutorials take way more time to make, and get outdated easily in this case, as AI progresses fast and continues to change in better, with more different settings and versions. Written guides are easier to update, since you don't have to remake an entire video. It's unknown if we will ever release a video since they easily get outdated, but if we will, it will be linked inside of this guide."]},{"l":"Do I need an extremely expensive mic for good quality?","p":["We had a conversation about this in https://discord.com/channels/1159260121998827560/1159290161683767298/1352325982689951765& https://discord.com/channels/1159260121998827560/1159290161683767298/1356265862704926907, RVC works by downsampling your audio voice to 16khz because f0 estimators only works at that sample rate, after that the model outputs the results using it's original sample rate (without any upscaling). So there won't be the need of having a super extremely expensive, a decent one should do the job."]},{"l":"Are there unique Voice Models?","p":["RVC Voice Models need to be trained on something, so the models themselves can't be unique, but you can use the Merge Lab to create a new unique merged model."]},{"l":"Is there a way to use Spin embedder rvc voice models?","p":["Wokada Deiteris Fork doesn't support models trained with the Spin embedder (which are very few), but there is a Pull Request for that https://github.com/deiteris/voice-changer/pull/213, which is just the Tg-Develop's Fork."]},{"i":"section-3","l":"‎"}],[{"l":"Tg Develop's W Okada Fork Kaggle","p":["Last update: September 6, 2025"]},{"i":"section","l":"‎","p":["This is a cloud-based alternative to run Wokada Tg-Develop's Fork, Realtime Voice Changer for calls/games, only for people who don't have a good PC GPU, via the Kaggle Service.","Check the Kaggle Glossary for more info on Free Tier, Limits, Verification, Pricing and other things."]},{"l":"Virtual Audio Cable"},{"l":"A Virtual Audio Cable (VAC) is what you need to use the realtime voice changer on Discord & Games.","p":["A VAC (Virtual Audio Cable) makes a fake audio device, used to re-route the audio of different programs.","In AI Realtime Voice Changing context, it's used to get the output of AI Converted Voice Output as the input in other programs such as Discord.","Download this: VAC Lite (Virtual-Audio-Cable by Muzychenko).(Be sure to not use any toher vac like VB Audio Cable.)","Run setup64, not 64a, after extracting the zip to a new folder","After installing the VAC Lite, it changes your default audio system. Click Yes when it asks you to open the audio device settings (or press WIN+R, type \"mmsys.cpl\" if you closed it already), and change your Recording and Playback devices back to your usual devices. Same for communications device aswell (right click -> set as default communication device)","Download either: Blackhole Virtual Audio Cable or VB-Audio","For Debian / Ubuntu-based Systems (Ubuntu, Mint, Pop!_ OS), run in the terminal:","For Fedora / RHEL-based Systems (CentOS, Rocky Linux), run in the terminal:","For Arch / Arch-based Systems (Endeavour, Manjaro Linux), run in the terminal:"]},{"i":"section-1","l":"‎"},{"l":"Create an Account"},{"i":"section-2","l":"‎"},{"l":"1. Set up account.","p":["Start by making an account here.‎","‎","Verify your acount with a phone number so you can turn on the \"internet\" option."]},{"i":"section-3","l":"‎"},{"i":"create-an-account-1","l":"Create an Account"},{"i":"section-4","l":"‎"},{"i":"1-set-up-account-1","l":"1. Set up account.","p":["Start by making an account here.‎","‎","Verify your acount with a phone number so you can turn on the \"internet\" option."]},{"i":"section-5","l":"‎"},{"l":"Notebook Creation & Setup"},{"i":"section-6","l":"‎"},{"l":"2. Clone Notebook","p":["Go to Kaggle and click \"Create\" then \"New Notebook\" at the top left.","‎","Under your session's name click \"File\" then \"Import Notebook\".","On the new window that appeared on the right click \"Link\" then type in the box this link https://github.com/tg-develop/voice-changer/blob/master-custom/Kaggle_RealtimeVoiceChanger.ipynb.","Click \"Import\" on the bottom right once you've done this.","When it's done importing it will display this text window.","Under \"Session options\" in the sidebar turn on \"internet\". Make sure persistance is on for both files and varibles.","Turn on T4 X2 GPUs in accelerator.","‎ g: (Optional) Turn on headless mode so you can run so you can run the GPU on all sessions and save your progress. Go to the top right and click \"Save version\" then open the advanced dropdown.","Your runtime will continue draining when you're not running any cells with this option on."]},{"i":"section-7","l":"‎"},{"l":"Installation"},{"i":"section-8","l":"‎"},{"l":"3. Installation Cells","p":["Starting from the top run the first cells, with the first being:","a2. When it's done, which may take a couple of minutes, it will output Done! Proceed with the next steps.","Run the third cell which is:"]},{"i":"section-9","l":"‎"},{"l":"Ngrok & Sever Setup"},{"i":"section-10","l":"‎"},{"l":"4. Ngrok Setup","p":["Scroll down to the last cell and you should see a section where you put your ngrok token. If you dont have a ngrok acount sign up here. a2. Once you have an acount you can authenticate your ngrok tunnel agent here: https://dashboard.ngrok.com/get-started/your-authtoken‎","put the Ngrok token in the last cell like so:","Once the Ngrok token is there run the cell and let it download what it needs then you can click on the ngork link and start using W-Okada.","There is a monthly limit rate with Ngrok so dont be supprised if training is suddenly interrupted.","(Optional) Directly under where you put your ngrok token there is region selection. You can change it to any of these servers to get less latency:","us -> United States (Ohio)","ap -> Asia/Pacific (Singapore)","au -> Australia (Sydney)","eu -> Europe (Frankfurt)","in -> India (Mumbai)","jp -> Japan (Tokyo)","sa -> South America (Sao Paulo)","From here it's pretty much the same as using local W-Okada."]},{"l":"Usage","p":["Now that you have the web interface running via Kaggle, the rest of the process is identical to using a local installation.","For mostly all subsequent steps, including audio routing, application settings, and model usage, please continue by following the Local PC guide."]},{"i":"section-11","l":"‎"}],[{"l":"Deiteris' W Okada Fork Kaggle","p":["Last update: September 6, 2025"]},{"i":"section","l":"‎","p":["This is a cloud-based alternative to run Wokada Deiteris Fork, Realtime Voice Changer for calls/games, only for people who don't have a good PC GPU, via the Kaggle Service.","Check the Kaggle Glossary for more info on Free Tier, Limits, Verification, Pricing and other things."]},{"l":"Virtual Audio Cable"},{"l":"A Virtual Audio Cable (VAC) is what you need to use the realtime voice changer on Discord & Games.","p":["A VAC (Virtual Audio Cable) makes a fake audio device, used to re-route the audio of different programs.","In AI Realtime Voice Changing context, it's used to get the output of AI Converted Voice Output as the input in other programs such as Discord.","Download this: VAC Lite (Virtual-Audio-Cable by Muzychenko).(Be sure to not use any toher vac like VB Audio Cable.)","Run setup64, not 64a, after extracting the zip to a new folder","After installing the VAC Lite, it changes your default audio system. Click Yes when it asks you to open the audio device settings (or press WIN+R, type \"mmsys.cpl\" if you closed it already), and change your Recording and Playback devices back to your usual devices. Same for communications device aswell (right click -> set as default communication device)","Download either: Blackhole Virtual Audio Cable or VB-Audio","For Debian / Ubuntu-based Systems (Ubuntu, Mint, Pop!_ OS), run in the terminal:","For Fedora / RHEL-based Systems (CentOS, Rocky Linux), run in the terminal:","For Arch / Arch-based Systems (Endeavour, Manjaro Linux), run in the terminal:"]},{"i":"section-1","l":"‎"},{"l":"Create an Account"},{"i":"section-2","l":"‎"},{"l":"1. Set up account.","p":["Start by making an account here.‎","‎","Verify your acount with a phone number so you can turn on the \"internet\" option."]},{"i":"section-3","l":"‎"},{"l":"Clone and Notebook Setup"},{"i":"section-4","l":"‎"},{"l":"2. Clone Notebook","p":["Go to the realtime voice changer notebook and click \"Copy and Edit\"","Under \"Session options\" in the sidebar turn on \"internet\". Make sure persistance is on for both files and varibles.","Turn on T4 X2 GPUs in accelerator.","d: (Optional) Turn on headless mode so you can run so you can run the GPU on all sessions and save your progress. Go to the top right and click \"Save version\" then open the advanced dropdown.","‎","Your runtime will continue draining when you're not running any cells with this option on."]},{"i":"section-5","l":"‎"},{"l":"Installation"},{"i":"section-6","l":"‎"},{"l":"3. Installation Cells","p":["Starting from the top run the first cells, with the first being:","a2. When it's done, which may take a couple of minutes, it will output Done! Proceed with the next steps.","Run the third cell which is:"]},{"i":"section-7","l":"‎"},{"l":"Ngrok & Sever Setup"},{"i":"section-8","l":"‎"},{"l":"4. Ngrok Setup","p":["Scroll down to the last cell and you should see a section where you put your ngrok token. If you dont have a ngrok acount sign up here. a2. Once you have an acount you can authenticate your ngrok tunnel agent here: https://dashboard.ngrok.com/get-started/your-authtoken‎","put the Ngrok token in the last cell like so:","Once the Ngrok token is there run the cell and let it download what it needs then you can click on the ngork link and start using W-Okada.","There is a monthly limit rate with Ngrok so dont be supprised if training is suddenly interrupted.","(Optional) Directly under where you put your ngrok token there is region selection. You can change it to any of these servers to get less latency:","us -> United States (Ohio)","ap -> Asia/Pacific (Singapore)","au -> Australia (Sydney)","eu -> Europe (Frankfurt)","in -> India (Mumbai)","jp -> Japan (Tokyo)","sa -> South America (Sao Paulo)","From here it's pretty much the same as using local W-Okada."]},{"l":"Usage","p":["Now that you have the web interface running via Kaggle, the rest of the process is identical to using a local installation.","For mostly all subsequent steps, including audio routing, application settings, and model usage, please continue by following the Local PC guide."]},{"i":"section-9","l":"‎"}],[{"l":"Deiteris' W Okada Fork Lightning AI","p":["Last update: September 6, 2025","This is a cloud-based alternative to run Wokada Deiteris Fork, Realtime Voice Changer for calls/games, only for people who don't have a good PC GPU, via the Lightning.AI Service.","Check the Lightning.AI Glossary for more info on Free Tier, Limits, Verification, Pricing and other things.","This doesn't affect when the Wokada Deiteris Fork is running, this is affected only if no cell is running or the site is closed, which will shutdown the studio session. So you don't have to check every 10 minutes the site after you're sure the server cell is running and you're using the Web User Interface after starting it."]},{"l":"Virtual Audio Cable"},{"l":"A Virtual Audio Cable (VAC) is what you need to use the realtime voice changer on Discord & Games.","p":["A VAC (Virtual Audio Cable) makes a fake audio device, used to re-route the audio of different programs.","In AI Realtime Voice Changing context, it's used to get the output of AI Converted Voice Output as the input in other programs such as Discord.","Download this: VAC Lite (Virtual-Audio-Cable by Muzychenko).(Be sure to not use any other VAC like VB Audio Cable.)","Run setup64(not 64a) after extracting the zip to a new folder.","After installing the VAC Lite, it may change your default audio system. Click Yes when it asks to open audio device settings (or press WIN+R and type \"mmsys.cpl\" if you closed it). Change your Recording and Playback devices back to your usual devices. Do the same for the default communication device.","Download either: Blackhole Virtual Audio Cable or VB-Audio","For Debian / Ubuntu-based Systems (Ubuntu, Mint, Pop!_ OS), run in the terminal:","For Fedora / RHEL-based Systems (CentOS, Rocky Linux), run in the terminal:","For Arch / Arch-based Systems (Endeavour, Manjaro Linux), run in the terminal:"]},{"l":"Create an Account"},{"l":"1. Set up account.","p":["First make an account with Lightning Ai","Make sure you verify yourself with a phone number. Once you've done that you will get an email that looks like this:","You will need to wait 2-3 business days to become fully verified","Once you are verified Lightning Ai will send you a email that conatins this:"]},{"l":"Studio Setup & Installation"},{"l":"2. Access the Notebook","p":["After creating your Lightning.AI account, open the Wokada-Deiteris-Fork Notebook and Clone it."]},{"l":"3. Activate/Switch GPU","p":["If you aren't on a GPU environment by default, you must switch to a GPU environment. This is crucial for performance.","On the right-hand lateral menu, click on Studio Environment(the processor icon).","Click Switch To GPU, select an available GPU, and wait for the environment to restart.","75 hours monthly of T4 16gb","31 hours monthly of L4 24gb","15 hours monthly of L40 48gb"]},{"l":"4. Clone Repository and Install Dependencies","p":["Run the first code cell. This will download the latest version of the realtime voice changer and install necessary dependencies.","This step may take a few minutes to complete. It will print \"Installed!\" when finished."]},{"l":"5. Set Server Configuration","p":["Run the second code cell to apply the server configuration.","It will print \"Server successfully configurated!\" upon completion."]},{"l":"Tunnels & Server Setup"},{"l":"6. Launch the Server via Tunnels","p":["(Optional) To potentially reduce latency, select a geographical Region from the list of parameter options next to it, that is closest to you.","After configuring your chosen tunnel, run the cell. The first time you run it, it will download the necessary voice models, which might take a minute or two.","Click on Port Viewer and then click Add a new port.","Click the + at the bottom of the right tab, click on Web Apps and install Port Viewer.","Click the URL. A new page will ask for a password.","Click your Port in Port Viewer, you can also click Open to open it in an external tab.","Cloudflare (Easy, No Account Needed)","Copy the password from the notebook output and paste it into the password prompt in your browser to access the voice changer.","Enter 18888 as the Port Number and optionally give it a name (e.g., \"Voice Changer\").","Go to the Horizon Dashboard and sign up. On the second step of the setup, you will see a command like hrzn login YOUR_ID. Copy that YOUR_ID part.","Go to the Ngrok Dashboard to get your free authtoken.","Horizon (Fast, Requires Account & ID)","How it works: Horizon is another tunneling service that requires a free account and a personal ID for authentication.","How it works: LocalTunnel is another free service that doesn't require an account. For security, it generates a unique URL that is protected by a password.","How it works: Ngrok is a popular service that creates secure tunnels. It requires a free account and an authentication token. It has a 1GB Bandwidth Free Monthly Limit https://ngrok.com/docs/pricing-limits/free-plan-limits/.","How it works: This is a built-in Lightning.AI feature. It's the most straightforward method as it doesn't require any external accounts or tokens.","How it works: This option uses Cloudflare's free trycloudflare service. It's very easy to use as it requires no accounts or tokens.","In the notebook cell, paste this ID into the Token field.","In the notebook cell, paste your token into the Token field, replacing 'Ngrok | Horizon TOKEN'.","In the right-hand sidebar of the Lightning.AI interface, click the Web Apps tab.","LocalTunnel (No Account, Password Protected)","Navigate to the third code cell, titled \"Start Server using Tunnels\". This cell boots up the Wokada Deiteris Fork application inside your Lightning.AI Studio.","Ngrok (Fast, Popular & Reliable)","Once the setup is complete, the output will display a message: \"--------- SERVER READY! ---------\", followed by your public URL. Click this link to open the Wokada Deiteris Fork interface and start using the voice changer.","Port Viewer (Recommended & Default method)","Run the cell.","Run the cell. The first time you use it, the output may ask you to authorize the connection by clicking a link ( https://hrzn.run/dashboard/settings/cli-token-requests/...). Click this link and approve the request in your Horizon dashboard.","Run the cell. The public Ngrok URL (ending in ngrok.io) will be printed in the output once the server is ready. Click on it to access the UI.","Run the cell. The script will automatically download the necessary tools. After a few moments, a public URL (ending in trycloudflare.com) will be printed in the output. Click it to open the interface.","Run the code cell. Wait for the output to show that the server is listening.","Select \"Cloudflare\" from the Tunnel code.","Select \"LocalTunnel\" from the Tunnel code.","Select \"Port Viewer\" from the Tunnel code.","Select a Tunnel: A tunnel securely exposes the application running in your private cloud environment to the public internet. The notebook gives you five different services to do this. Choose one from the Tunnel code menu in the code cell.","Steps:","The output will display two key pieces of information: the public URL (ending in loca.lt) and a Local Tunnel Password below it.","The public Horizon URL (ending in hrzn.run) will then be printed in the output. Click it to access the UI.","The server runs in the foreground. If you stop the cell or close the Lightning.AI site, the server will shut down. Keep the cell running to use the program.","This final code cell is the most important one—it starts the voice changer's server and uses a \"tunneling\" service to create a secure, public web address (URL) for you to access it from your own computer.","You can optionally go back to the Jupyter session in the right-hand sidebar of the Lightning.AI interface, to check if any error appears in the code output."]},{"l":"Usage","p":["Now that you have the web interface running via Lightning.AI, the rest of the process is identical to using a local installation.","For all subsequent steps, including audio routing, application settings, and model usage, please continue by following the Local PC guide."]},{"l":"Maintenance"},{"l":"Deleting Everything","p":["If you need to update the Wokada Deiteris Fork or start fresh, you can run the final cell in the notebook, \"Delete everything\". This will remove all downloaded files and configurations from your persistent storage, allowing for a clean installation."]},{"i":"section","l":"‎"}],[{"l":"Tg Develop's W Okada Fork Colab","p":["Last update: September 6, 2025","This is a cloud-based alternative to run Wokada Tg-Develop's Fork, Realtime Voice Changer for calls/games, only for people who don't have a good PC GPU, via the Google Colab Service.","Check the Google Colab Glossary for more info on Free Tier, Limits, Verification, Pricing and other things.","You need the Google Colab Paid Tier to run this, as it uses a Web User Interface, else you could risk getting disconnected or banned off Colab."]},{"l":"Virtual Audio Cable"},{"l":"A Virtual Audio Cable (VAC) is what you need to use the realtime voice changer on Discord & Games.","p":["A VAC (Virtual Audio Cable) makes a fake audio device, used to re-route the audio of different programs.","In AI Realtime Voice Changing context, it's used to get the output of AI Converted Voice Output as the input in other programs such as Discord.","Download this: VAC Lite (Virtual-Audio-Cable by Muzychenko).(Be sure to not use any toher vac like VB Audio Cable.)","Run setup64, not 64a, after extracting the zip to a new folder","After installing the VAC Lite, it changes your default audio system. Click Yes when it asks you to open the audio device settings (or press WIN+R, type \"mmsys.cpl\" if you closed it already), and change your Recording and Playback devices back to your usual devices. Same for communications device aswell (right click -> set as default communication device)","Download either: Blackhole Virtual Audio Cable or VB-Audio","For Debian / Ubuntu-based Systems (Ubuntu, Mint, Pop!_ OS), run in the terminal:","For Fedora / RHEL-based Systems (CentOS, Rocky Linux), run in the terminal:","For Arch / Arch-based Systems (Endeavour, Manjaro Linux), run in the terminal:"]},{"i":"section-1","l":"‎"},{"l":"Installation"},{"i":"section-2","l":"‎"},{"l":"1. Installation Cells","p":["Click here to access the colab. Then starting from the top run the first cell:","a2. When it's done, which may take a couple of minutes, it will output Done! Proceed with the next steps.","Run the second cell:"]},{"i":"section-3","l":"‎"},{"l":"Ngrok & Sever Setup"},{"i":"section-4","l":"‎"},{"l":"2. Ngrok Setup","p":["Scroll down to the last cell and you should see a section where you put your ngrok token. If you dont have a ngrok acount sign up here. a2. Once you have an acount you can authenticate your ngrok tunnel agent here: https://dashboard.ngrok.com/get-started/your-authtoken‎","When you have your ngrok token place it in the text box that says TOKEN_HERE","Once the Ngrok token is there run the cell and let it download what it needs then you can click on the ngork link and start using W-Okada.","There is a monthly limit rate with Ngrok so dont be supprised if training is suddenly interrupted.","(Optional) Directly under where you put your ngrok token there is region selection. You can change it to any of these servers to get less latency:","us -> United States (Ohio)","ap -> Asia/Pacific (Singapore)","au -> Australia (Sydney)","eu -> Europe (Frankfurt)","in -> India (Mumbai)","jp -> Japan (Tokyo)","sa -> South America (Sao Paulo)","From here it's pretty much the same as using local W-Okada."]},{"l":"Usage","p":["Now that you have the web interface running via Kaggle, the rest of the process is identical to using a local installation.","For mostly all subsequent steps, including audio routing, application settings, and model usage, please continue by following the Local PC guide."]},{"i":"section-5","l":"‎"}],[{"l":"Deiteris' W Okada Fork Colab","p":["Last update: September 6, 2025","This is a cloud-based alternative to run Wokada Deiteris Fork, Realtime Voice Changer for calls/games, only for people who don't have a good PC GPU, via the Google Colab Service.","Check the Google Colab Glossary for more info on Free Tier, Limits, Verification, Pricing and other things.","You need the Google Colab Paid Tier to run this, as it uses a Web User Interface, else you could risk getting disconnected or banned off Colab."]},{"l":"Virtual Audio Cable"},{"l":"A Virtual Audio Cable (VAC) is what you need to use the realtime voice changer on Discord & Games.","p":["A VAC (Virtual Audio Cable) makes a fake audio device, used to re-route the audio of different programs.","In AI Realtime Voice Changing context, it's used to get the output of AI Converted Voice Output as the input in other programs such as Discord.","Download this: VAC Lite (Virtual-Audio-Cable by Muzychenko).(Be sure to not use any toher vac like VB Audio Cable.)","Run setup64, not 64a, after extracting the zip to a new folder","After installing the VAC Lite, it changes your default audio system. Click Yes when it asks you to open the audio device settings (or press WIN+R, type \"mmsys.cpl\" if you closed it already), and change your Recording and Playback devices back to your usual devices. Same for communications device aswell (right click -> set as default communication device)","Download either: Blackhole Virtual Audio Cable or VB-Audio","For Debian / Ubuntu-based Systems (Ubuntu, Mint, Pop!_ OS), run in the terminal:","For Fedora / RHEL-based Systems (CentOS, Rocky Linux), run in the terminal:","For Arch / Arch-based Systems (Endeavour, Manjaro Linux), run in the terminal:"]},{"i":"section-1","l":"‎"},{"l":"Installation"},{"i":"section-2","l":"‎"},{"l":"1. Installation Cells","p":["Click here to access the colab. Then starting from the top run the first cell:","a2. When it's done, which may take a couple of minutes, it will output Done! Proceed with the next steps.","Run the second cell:"]},{"i":"section-3","l":"‎"},{"l":"Ngrok & Sever Setup"},{"i":"section-4","l":"‎"},{"l":"2. Ngrok Setup","p":["Scroll down to the last cell and you should see a section where you put your ngrok token. If you dont have a ngrok acount sign up here. a2. Once you have an acount you can authenticate your ngrok tunnel agent here: https://dashboard.ngrok.com/get-started/your-authtoken‎","When you have your ngrok token place it in the text box that says TOKEN_HERE","Once the Ngrok token is there run the cell and let it download what it needs then you can click on the ngork link and start using W-Okada.","There is a monthly limit rate with Ngrok so dont be supprised if training is suddenly interrupted.","(Optional) Directly under where you put your ngrok token there is region selection. You can change it to any of these servers to get less latency:","us -> United States (Ohio)","ap -> Asia/Pacific (Singapore)","au -> Australia (Sydney)","eu -> Europe (Frankfurt)","in -> India (Mumbai)","jp -> Japan (Tokyo)","sa -> South America (Sao Paulo)","From here it's pretty much the same as using local W-Okada."]},{"l":"Usage","p":["Now that you have the web interface running via Kaggle, the rest of the process is identical to using a local installation.","For mostly all subsequent steps, including audio routing, application settings, and model usage, please continue by following the Local PC guide."]},{"i":"section-5","l":"‎"}],[{"l":"Voyages","p":["Last update: August 11, 2025","https://voyages.weights.com is a comprehensive, free-to-use AI cloud platform for creating, collecting, and exploring AI-generated art. It provides users with the tools to transform their ideas into images, create custom styles, and manage their visual creations.","The platform is designed for a broad audience, from beginners to experienced artists, offering a user-friendly web interface that removes the need for powerful local hardware.","Users can either utilize a wide variety of official and community-created styles or design their own.","Originally known as Weights.com with a focus on RVC models, the platform has expanded into a broader creative suite with the introduction of Voyages for image generation."]},{"l":"Pros & Cons","p":["The core function of Voyages is to generate images from text prompts. The platform offers several options to customize the output to your liking."]},{"l":"*Learn more*","p":["All-in-one web cloud platform for AI art generation.","No requirement for a high-end PC.","Mobile-friendly, with dedicated apps for iOS and Android.","An intuitive interface that is suitable for users of all skill levels.","A large library of public styles to choose from.","A free tier is available for users to get started.","An active community for sharing and collaborating on creations.","The free plan may have limitations on usage and features.","Generation times can vary depending on server load.","Advanced controls might be less granular compared to specialized, local software."]},{"l":"Instructions:","p":["1. Select a Creation Type","2. Describe Your Image","3. Choose a Style","4. Select an Aspect Ratio","5. Set the Number of Images","6. Generate and Download","Choose Image to create a new image from a text prompt.","Click on the Aspect Ratio button to choose the dimensions of your image.","Click on the Number of Images button to select how many images you want to generate.","Click on the Style button to open the style browser.","Click the Create button to start the image generation process.","In addition to using pre-made styles, Voyages allows you to train your own custom styles, giving you full creative control.","In the text box, enter a detailed description of the image you want to create. Be as specific as possible to get the best results.","Navigate to the main creation interface. You will be presented with several options.","Options include Square, Landscape, and Portrait.","You can also choose Edit to modify an existing image or Video to create a video from a prompt or image (premium feature).","You can choose from Official Styles curated by Weights, or browse through Community Styles created by other users. You also have access to your own saved styles under Mine.","You can generate 1 image for free, while generating 2 or 4 images at once is a premium feature.","Your generated images will appear in your creations feed, where you can view and download them."]},{"i":"instructions-1","l":"Instructions:","p":["1. Navigate to the Create a Style tab from the main menu. This will take you to the \"Create Style\" page.","2. Fill in the Model Details:","3. Upload your Training Images:","4. Start the Training Process:","For optimal results, upload between 5 and 30 high-quality, square images that represent the aesthetic you want to achieve.","Limits and features are subject to change. Always check the Voyages.weights.com Official Pricing Page for the most current information.","Model Name: Give your style a unique and descriptive name.","NSFW: Toggle this switch if the style is intended to create adult content.","Once your details are filled in and images are uploaded, click Train and Publish.","Private Model: Enable this if you want the style to be accessible only to you.","Subscriptions increase the number of Style Images, Images, IImage Edits, Queued Images, Queued Trainings, Style Trainings and In-App Style Trainings.","This section is for the dataset—the collection of images the AI will learn from.","Trigger (Optional): Add a specific word that will activate this style. Using a trigger word can lead to more accurate and consistent image generations.","Voyages.weights.com operates on a freemium model. It offers a Free Plan with core features, alongside paid Basic and Pro plans for users who need more and better resources.","You can drag and drop your image files directly into the upload area.","Your new style will be processed and will become available in the Mine tab of the style browser once the training is complete."]}],[{"l":"Illusion Diffusion","p":["Last update: August 13, 2025"]},{"i":"section","l":"‎"},{"l":"1. Upload","p":["Go to the Illusion Diffusion Hugging Face Space.","Click on Input Illusion.","Upload the image you wish to blend.‎"]},{"l":"2. Illusion Strength","p":["Modify the \"Illusion Strength\" slider.","Higher values will make the picture more visible.","Lower ones will hide it more.‎"]},{"l":"3. Prompt","p":["Describe the characteristics of the output.","These can be environments (medieval castle - flowery meadow), traits (high quality - A specific image style), etc.‎"]},{"l":"4. Negative Prompt (optional)","p":["Specify what the AI should NOT do when creating the image (e.g., low quality, specific styles to avoid, etc.).‎"]},{"l":"5. Run","p":["Press Run to begin processing.","Be patient; there might be a queue.","If you encounter an error, try clicking again until it works."]}],[{"l":"Realtime TTS","p":["Last update: July 28, 2025"]},{"i":"section","l":"‎","p":["A VAC (Virtual Audio Cable) makes a fake audio device, used to re-route the audio of different programs.","After installing a VAC you need to:","After installing the VAC Lite, it changes your default audio system. Click Yes when it asks you to open the audio device settings (or press WIN+R, type \"mmsys.cpl\" if you closed it already), and change your Recording and Playback devices back to your usual devices. Same for communications device aswell (right click -> set as default communication device)","After that:","Audio Setup in Games:","Choose any TTS in our TTS Tools.","Download either: Blackhole Virtual Audio Cable or VB-Audio","Download this: VAC Lite (Virtual-Audio-Cable by Muzychenko).(Be sure to not use any toher vac like VB Audio Cable.)","For Debian / Ubuntu-based Systems (Ubuntu, Mint, Pop!_ OS), run in the terminal:","For Fedora / RHEL-based Systems (CentOS, Rocky Linux), run in the terminal:","Hold down the option key on your keyboard then click the sound icon in the upper right corner of the menu bar and then a dropdown menu will appear which allows you to select the VAC as the output device.","In Discord go to Settings then Voice & Video and select your VAC for your input.","In Wokada Deiteris Fork context, it's used to get the output of Wokada Deiteris Fork as the input in other programs such as Discord.","It's the same thing for game too, go into its audio settings and set its input to the VAC.","Press Command + Spacebar and type Sound and set the default audio device back to your headphones and microphone , make sure you do this for default and communication default.","Press WIN+CTRL+V and select the VAC as the output device.","Press WIN+R and type mmsys.cpl and change the default audio devices back to your headphones and microphone, make sure you do this for default and communication default.","Run setup64, not 64a, after extracting the zip to a new folder","This is a guide on how to use most of the TTS' listed in the TTS Tools section in realtime."]},{"i":"---","l":"‎ ‎"}],[{"l":"TTS Tools","p":["Last update: Dec 12, 2024"]},{"l":"Introduction","p":["TTS is an abbreviation of Text To Speech, an AI that converts any given text into vocal speech.","The ones listed here offer a decent variety of features & options, such as model training, fine-tuning, 0 shot training, or being mixed with RVC.","Here's an index of the best TTS tools out there:‎"]},{"l":"ElevenLabs/11Labs","p":["ElevenLabs is a freemium service that offers TTS, training TTS models & translating videos from different languages.","‎"]},{"l":"Fish Speech","p":["Fish speech is a 0shot multilingual TTS model created by Fish Audio.","This is one of the best 0shot TTS as of now, it rarely hallucinates.","It can be used either locally or on the cloud.","Offical github repo","Offical site","HuggingFace Space"]},{"l":"F5 TTS","p":["F5 is the best 0shot TTS model.","F5 gives fairly high quality outputs that rarely hallucinate.","But it is limited with issue like: Reading to fast = you are using a reference audio that is more the six seconds long or 100 characters. Hallucinates on low voices.","It can be used either locally or on the cloud.","Offical github repo","HuggingFace Space"]},{"i":"section","l":"‎"},{"l":"Dia TTS","p":["Dia generates high quality English only dialogue from a transcript.","Dia can create nonverbal sounds like laughter, coughing, clearing throat, etc.","It can be used either locally or on the cloud.","Offical github repo","HuggingFace Space"]},{"i":"section-1","l":"‎"},{"l":"Edge TTS","p":["\uD83D\uDCD2 Google Colab","\uD83E\uDD17 Hugging Face","Applio Colab","Download the browser.","Ilaria RVC","In the TTS input the text you want & click Generate. Stop recording when the voice is done.","It can only be used online via their API, through their web browser, a HF/Colab space or mixed with RVC.","Local Applio‎","Open Microsoft Edge & drag the .html to it.","Open your Notepad & paste the following code:","Rename it to “Microsoft Edge TTS.html”","Save it as “Microsoft Edge TTS.txt”","These being mixed with RVC means it generates the speech & runs the output through RVC, applying the voice model.","This is Microsoft Edge TTS, which is good quality, multilingual & works great on long sentences.","Use Audacity to record the audio. Set the recording mode to loopback to record the internal audio (Realtek driver might be needed).","You can then select Voice Options in the toolbar & change the speed to a faster/slower speech."]},{"i":"section-3","l":"‎"},{"i":"section-4","l":"‎"},{"l":"XTTS2","p":["Built on \uD83D\uDC22 Tortoise TTS & developed by Coqui AI, which has been discontinued unfortunately.","Has important model changes that make cross-language 0 Shot voice cloning& multilingual speech generation super easy.","You need less training data. Just least a 2 minute audio.","Can use it either online or locally:","Official XTTS 2 Guide","XTTS 2 UI Fork","Inference 0 Shot Training UI Colab(Run it & click the Public Link)","Training & Inference UI Colab","Inference 0 Shot Training HF Space"]},{"i":"section-5","l":"‎"},{"l":"Zonos","p":["0 Shot TTS with great emotion controls","Can be used with English, French, German, Chinese and Japanese","Can be used locally or online","Official Github Repo","Offical Playground","HuggingFace Space"]},{"i":"section-6","l":"‎"},{"l":"Kokoro-TTS","p":["CLI TTS","Only has premade voices","Voice bleeding for English, British English French, Itailian, Japanese and Chinese","Not the best emotion control","Official Github Repo","Offical HuggingFace Space"]},{"i":"section-7","l":"‎"},{"l":"OpenVoice","p":["Has Versatile Instant Voice Cloning (aka 0 Shot Training)","Contains cross-lingual & flexible voice style control","Available both locally & online:","Official GitHub repo","Inference GUI Colab","Official Demo Part 1 Colab","Official Demo Part 2 Colab","Official HF Space"]},{"i":"section-8","l":"‎"},{"l":"Piper","p":["Fast TTS","Great multilingual support","Works for almost all languages","Decent quality","Official Github Repo","Github Repo with Colabs(For training and inference)","Several HuggingFace Spaces for each Language"]},{"i":"section-9","l":"‎"},{"l":"MeloTTS","p":["MeloTTS is a high-quality multilingual TTS library, made by MyShell.ai","Includes almost real-time inference.","It can be used both locally and online:","Official GitHub Repo","UI Colab","NO UI Colab","HF Space"]},{"i":"section-10","l":"‎"},{"l":"GPT-SoVITS","p":["GPT-SoVITS has cross language inference, but there could be some noises.","It's very good with Chinese, but also with English.","Most parts are in japanese & not deeply tested. Expect some instability.","Can be used both locally & online:","Official GitHub Repo","Colab Space(with fine-tuning, inference & UI)"]},{"i":"section-11","l":"‎"}],[{"l":"GPT-SoVITS","p":["Last update: Mar 8, 2024"]},{"i":"section","l":"‎","p":["See original guide","GPT-SoVITS is an open-source repository focused on TTS & cross-language inference, with a Colab port coming soon. Credits to RVC-Boss.","Currently it only supports Chinese, English & Japanese. More languages are coming soon.","You'll require great specs & a NVIDIA GPU with >=6G VRAM to run it smoothly. Otherwise, use the Colab.","This guide is a translation of the original one with a few tweaks, made by Delik. [Discord: @delik - Wechat: Dellikk ]‎"]},{"i":"section-1","l":"‎"},{"i":"section-2","l":"‎"},{"l":"1. Download prezip","p":["Download the prezip of the latest version here."]},{"i":"section-3","l":"‎"},{"l":"2. Extract","p":["Unzip the folder. It's advisable to use 7-ZIP to do so."]},{"i":"section-4","l":"‎"},{"l":"3. Launch","p":["Open the folder & run go-web.bat to open WebUI."]},{"i":"section-5","l":"‎"},{"i":"section-6","l":"‎"},{"l":"1. Prepare dataset","p":["The dataset should be between 1 - 30 minutes. But you must prioritize quality over quantity.","For the best results, ensure the audio is properly cleaned, free of undesired noises & distortions.","GPT-So-VITS is made for TTS only, so it's also best to remove any singing/muffly voice parts."]},{"i":"section-7","l":"‎"},{"l":"2. Audio Slicer","p":["Copy the path file of your dataset & paste it in the Audio slicer input bar.","Create a new folder somewhere. Copy its path folder & paste in Audio slicer output. This is where the outputs are getting stored.","Adjust the parameters if needed.","Finally, click Start Audio Slicer to complete this step."]},{"i":"section-8","l":"‎"},{"l":"3. ASR","p":["The Input folder path should be the same as Audio slicer output. Jst copy the path & paste it inside the bar.","If the dataset is in English/Japanese, use Faster-Whisper large v3.","If it's in Chinese, use 达摩ASR.","Then click Start batch ASR.","If you run GPT-SoVITS for the first time, you might need to wait for a few minutes for it to download the ASR model you select.","Finally, locate the .list file & copy the path. It will be in output/asr_opt, if you didn't change the folder for Output folder path."]},{"i":"section-9","l":"‎"},{"l":"4. Text Labelling (optional)","p":["Paste the .list file path into .list annotation file path.","Tick Open labelling WebUI to open Text Labelling WebUI. A new tab will open.","Listen to each clip & edit the text if it's not transcribed properly.","The functions are self-explanatory. Use next index& previous index to check the next/previous page.","If you make changes, remember to save file& submit text."]},{"i":"section-10","l":"‎"},{"l":"5. Formatting","p":["Click 1-GPT-SOVITS-TTS& 1A-Dataset formatting to enter the training page.","Input the name of your model in Experiment/model name, & the .list file path to Text labelling file.","Scroll down to the end & start One-click formatting to begin formatting."]},{"i":"section-11","l":"‎"},{"l":"6. SoVITS Training","p":["Scroll up then click 1B-Fine-tuned training.‎‎"]},{"i":"section-12","l":"‎","p":["2| Use 1 if the GPU has 6GB VRAM.","8","<= 0.4","4","After that, click Start SoVITS training"]},{"i":"section-13","l":"‎"},{"l":"7. GPT Training","p":["2 (1 if your gpu has 6G vram)","10","5","disabled (explained later)","‎","After that, click Start GPT training"]},{"i":"section-14","l":"‎"},{"l":"DPO training (optional)","p":["DPO training greatly improves the performance (not audio quality) & stability of the model.","It can infer more text at once without slicing & it's less prone to errors (like repeating/skipping words) when inferring.","A GPU with 12G VRAM or more.","A very high quality dataset (you need to do text labelling) to enable this.","Using a batch size of 1. Keep the other settings same as above.","Otherwise, this will worsen the model."]},{"i":"section-15","l":"‎","p":["Go to the 1C-inference tab.","Press refreshing model paths& select your models from the dropdowns respectively.","Tick Open TTS inference WEBUI.","Upload a clip for reference audio ( must be 3-10 seconds). Then fill-in the Text for reference audio, which is what does the character say in the audio. Choose the language on the right.","The reference audio is very important as it determines the speed & emotion of the output. Try different ones to polish your output.","You can reopen the text proofreading tool to download the reference audio, and copy & paste the text for reference audio.","Hover above the \"duration\" to adjust the length of the reference audio, & hover above \"it\" to delete the current reference audio.","No reference text mode exists, but it's not advisable to use it. It affects the quality a lot.‎","Fill the Inference text& set the Inference language, then click Start inference.","If the text is too long choose the options in How to slice the sentence.","If you did not get your desired output, you can infer it again or change reference audio and/or adjust GPT parameters."]},{"i":"section-16","l":"‎"}],[{"l":"Model Maker Role","p":["Last update: October 20, 2024","To upload your Voice Models in the AI HUB Discord Server's #voice-models forum channel, you need to pass a Quality Control (QC) check to be sure that you post good voice models."]},{"i":"section","l":"‎","p":["Model's .PTH file.","Model's .INDEX file.","General information about the model.","General information about its training process.","A Hugging Face or weights.com account.","At least 1 raw audio sample of the model WITH NO MUSIC."]},{"l":"It lacks the correct files.","p":["The .ZIP file must contain both the correct.INDEX& .PTH file.","The correct .index is the one named added_.","The added index contains the voice's accent and speech manor.","The correct .pth is the one that has your model's name, for example: TylerSwift_e60_s120.pth","The .pth contains the actual model and pitch data."]},{"i":"section-2","l":"‎"},{"l":"Model is low quality.","p":["A bad model:","Sounds scratchy/screechy.","Has a muffled sound.","Sounds inaccurate to the source.","Is incapable of hitting certain notes.","Has slurred speech.","Is unable of pronouncing words correctly in its intended language.","Has artifacting.","Has noise."]},{"i":"section-3","l":"‎"},{"l":"An outdated extraction method was used.","p":["Only Mangio-Crepe& RMVPE are allowed. Learn about them here","Harvest, Dio, Crepe-Tiny, PM, etc. are obsolete."]},{"i":"section-4","l":"‎"},{"l":"The audio demo contains instrumental.","p":["Don't include ANY music in the audio demo, even if it's not copyrighted. This is due to:","Concerns over copyright.","In many cases, the music can \"hide\" the flaws of the voice model, making it harder to judge its quality."]},{"l":"The audio demo is altered.","p":["Don't add reverb, equalize, or alter the demo in any way, as it won't be a faithful representation of the model. It must be the raw, unmodified output from the inference.","Trimming silences at the beginning/end of the audio demo is allowed."]},{"i":"section-5","l":"‎"},{"l":"Is a robotic or non-human voice.","p":["Robotic, sound effect and drum models will also be rejected, because with these types of voices it is difficult to determine if you know how to clean a dataset properly.","However once you get model maker you will be able to post robotic, sound effect or drum models."]},{"i":"section-6","l":"‎"},{"i":"section-7","l":"‎"},{"l":"Step 3: Prepare the submission.","p":["Once your model is ready, head over to the AI HUB's #model-maker-role channel.","Click the Submit Model button.","‎‎","Its name.","The technology used for its training.","The extraction method you used.","Total epochs amount.","Its download link from Hugging Face or Weights.","An audio sample of it talking/singing.","Optional. Add more context about the model if you want.","You can attach more samples when you repost the model to #voice-models."]},{"i":"section-8","l":"‎"},{"l":"Step 4: Send submission.","p":["Once you are done filling the information it will send your model to get QCed","‎‎","Now, wait for a QC(quality checker) to verify your model. You'll be notified once it has been reviewed.","If you made a mistake in your submission or you want to change something you can cancel your submission by clicking on the cancel button that is attatched to the message you get when you send a submission.","‎","If your model gets approved, the bot will notify you with a message like this:","You can then repost the model (& future models) to the #voice-models forum."]},{"i":"section-9","l":"‎"}],[{"l":"Glossary","p":["‎","\"free studios\" are considerated the free CPU studios, which have a 4 hour limit session, after that they will become paid CPU studios using your credits unless you restart them. Unlike GPU Studios which directly use credits.","A cloud-based is a cloud-based platform by Google to run Jupyter Notebooks.","A cloud-based platform designed for developing and running AI applications in persistent environments called \"Studios.\", and one of the options is via (but not limited to) running Jupyter Notebooks.","A optimizer is an algorithm used to minimize the loss function during the training of neural networks. It helps adjust the model's weights and biases.","A technology developed by NVIDIA, that uses the power of graphics cards to perform calculations that require great processing power.","An interactive, web-based document that lets you combine runnable code, explanatory text, and media (like images and charts) into a single file.","Any software or application that's stored, managed, and available through the provider's virtual servers, and is accessed through a web browser.","Audio formats that compress(lose) the original quality. They're built to be space-efficient.","Audio formats that don't compress(lose) the original quality.","Basically the speed at which RVC/UVR will work will depend on how good your GPU is.","Basically, higher bit depths represent more accurately the loudness of an audio.","Both formats give the same results & don't have an audible difference. Converting a lossy audio to a lossless one won't restore the lost quality.","Common lossy formats are MP3, OGG, OPUS, M4A, etc.","CPUs:","CUDA is downloaded automatically as a part of the NVIDIA driver.","Default (CPU)(4 cores, 16GB RAM): unlimited(first 4 hours are free and you'd need to restart, or keep paying, and if you pay you'd have 45 hours monthly)","Different from making a dataset & doing the long training process, based on lots of criteria such as epochs.","Doesn't do any kind of compression. It's purely the original data.","Doing inference on an AI model without explicitly training on it.","Done by users with a powerful GPU.","Example: G_70.pth and D_70.pth","Example: Tyler_e60_s120.pth","FLAC:","For basic audio editing, we recommend Audacity.","For example, in RVC is when a voice model is used to transform an audio, to make it sound like the model.","For example, in TTS you do inference by cloning a voice with an audio, a data it hasn't seen before.","For professional mixing, FL Studio.","For this, using the GPU is more convenient as it's faster. Though normally you can still use CPU, which takes longer.","Free tier users get free 15 monthly credits that can be used on CPUs or GPUs. Be sure to monitor your usage and stop the Studio when not in use.","Further improving an AI model, training it with a another dataset.","G and D:","Google Colaboratory, also known as Google Colab or just Colab, is a cloud-based platform by Google to run Jupyter Notebooks.","GPUs (Powerful, Recommended):","Gradio is an open-source Python packag that makes it easy for developers to create user-friendly web interfaces for machine learning models and other applications, such as RVC.","Higher bitrate equals a higher quality.","If you have trouble verifying your phone number, Contact Kaggle.","In AI training, is used for quick parallel independent computations, which increases the speed substantially.","In Free tier you can change GPU/CPUs at any time, but if you used all free 15 credits monthly only on a specific computing, you'd have:","In RVC, these are files of a model that generate over the course of training, that can be very useful.","In some cases you can do it on GPU, some in CPU.","In the context of AI, it's using an AI model to complete any task.","In the field of AI, is the process where an AI model is fed with its dataset & learns from it.","In the field of digital audio, it defines the dynamic range of each sample.","It allows Web User Interfaces on the Free tier, avoiding any encryption or ban risk unlike Google Colab or Kaggle.","It deploys the program on a Local URL, which is the one running locally on the machine, and a Public Share Link, which is a tunnel that exposes the Local URL. The Public Share Link is used, for example, in Google Colabs, powered by their Share API. Sometimes, the Share API goes down, you can check its status.","It doesn't Web User Interfaces in the Free Tier, meaning it needs encrypted Jupyter Notebooks) and has ban risk like Kaggle.","It doesn't Web User Interfaces, meaning it needs encrypted Jupyter Notebooks) and has ban risk like Google Colab.","It features persistent storage, meaning your files are saved between sessions.","It is a shorter way to say optimizer.","It refers to a computer's specifications. Hardware like GPU, CPU, RAM, etc.","It requires a Google Account.","It requires a phone number verification to access the internet (which is needed to run most Jupyter Notebooks).","It requires a phone number verification to full access.","It stands for Digital Audio Workstation, and it's any software used for making and mixing music.","It stands for Graphics Processing Unit. It's designed to rapidly manipulate and alter memory to accelerate creation of images.","It's a copy of a main GitHub project. It aims to make a different version of the project with improvements, changes & new features.","It's especially useful for AI tools, such as RVC and UVR, which greatly optimizes the process.","It's faster but with less quality, and you won't be able to save the model.","It's focused on data science and machine learning.","It's free tier offers max 4 hours a day of T4 GPU, but it's random. Once you exhaust it, you'll have to wait 12 - 24 hours or get a paid tier.","It's more accurate to describe it as an uncompressed format","It's organized into individual cells. Code cells can be run one by one to perform tasks, while text cells (using Markdown) are used for documentation and instructions.","It's recommended over WAV since it's space-efficient.","It's used in Google Colabs to expose the Local URL so that users on Cloud can access the program.","Its algorithm compresses the data without losing quality.","Jupyter Notebook files are saved with the .ipynb file extension.","L4(24GB VRAM): 31 hours","L40S(48GB VRAM): 15 hours","Large (CPU): (8 cores, 32GB RAM): 29 hours","Last update: August 9, 2025","Learn how to bypass their limitations here.","Localtunnel is a tunnel made to expose a local url (like http://localhost:3000).","Named G_ and D_, followed by the step number & .pth.","Of course. Here is a standalone section about Jupyter Notebooks, written in the same glossary style.","Running locally is a process that involves running apps in your own machine, using its resources.","So by getting rid of some data (in this case, quality), they achieve a smaller file size.","Studios Auto Sleep (stop running) after 10 minutes of inactivity (such as closing the site or not running anything in the background) in the Free tier.","T4(16GB VRAM): 75 hours","The amount of data processed per certain unit of time, usually in kilobits per second (KBPS).","The free tier provides a limited quota of 30 free GPU hours per week of either T4x2 (2 T4 GPUs at once) or P100, which you can swap at any time, using either will consume the quota, which resets weekly.","The main ones are WAV & FLAC:","The opposite of cloud-based.","The opposite of local running.","The performance of the hardware of a computer directly correlates to the performance of all its software.","The rate at which they're saved is determined by the save frequency value (or save rate or similar names). For newbies, it's recommended use a value of 15.","Therefore it has a much bigger file size.","These allow you to resume training, if G and D's numbers match.","These are actual models.","They are divided by two types:","They're organized with this format: modelname_epoch_step.pth","They're recommended for RVC, as the more quality an audio has, the more accurate results they'll offer.","This cell-based format is perfect for creating step-by-step guides and running AI applications, which is why it's the standard interface for platforms like Google Colab, Kaggle, and Lightning.AI.","This determines the difference between the quietest & loudest sound.","Used by users with a weak GPU, which can't do local running.","Users inactive for a 6 month period and do not have an active paid subscription will be scheduled for deletion. Users will be notified 30 days before the scheduled deletion, with several reminders sent during this period.","Vocal lines that contribute to the sound of the lead vocals in a song.","WAV:","Weights:","Which Nvidia driver you use might affect performance, Studio Drivers can help AI, but this is mostly for other types of AIs such as Stable Diffusion, rather than RVC.","X-Large (CPU): (16 cores, 64GB RAM): 15 hours","You can check the pricing for more info.","You can think of it as video resolution (240, 480, 1080, etc.)."]}],[{"l":"Contributions","p":["Last update: July 28, 2025"]},{"l":"Contributions","p":["We'll appreciate any feedback, big or small. If you wish to contact anyone who worked on this you can find them in AI Hub.","Leave suggestions in the #suggestions channel.","To report issues use #ai-help-forum. You can also send issues to our GitHub Repository.","If you would like to add to this doc please make a pull request in the GitHub Repository."]},{"i":"section","l":"‎"}],[{"l":"License"},{"l":"License CC BY-NC-SA 4.0","p":["This work is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License."]},{"l":"You are free to:","p":["Share: Copy and redistribute the material in any medium or format.","Adapt: Remix, transform, and build upon the material.","The licensor cannot revoke these freedoms as long as you follow the license terms."]},{"l":"Under the following terms:","p":["Attribution: You must give appropriate credit, provide a link to the license, and indicate if changes were made. You may do so in any reasonable manner, but not in any way that suggests the licensor endorses you or your use.","NonCommercial: You may not use the material for commercial purposes.","ShareAlike: If you remix, transform, or build upon the material, you must distribute your contributions under the same license as the original."]},{"l":"You are not required to comply with the license for elements of the material in the public domain or where your use is permitted by an applicable exception or limitation."},{"l":"No warranties are given. The license may not give you all the permissions necessary for your intended use. For example, other rights such as publicity, privacy, or moral rights may limit how you use the material.","p":["Read the full text of the license."]}]]